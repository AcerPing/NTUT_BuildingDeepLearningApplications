{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":65534,"databundleVersionId":7325705,"sourceType":"competition"}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport socket\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-30T03:59:53.963100Z","iopub.execute_input":"2023-12-30T03:59:53.964257Z","iopub.status.idle":"2023-12-30T03:59:54.142060Z","shell.execute_reply.started":"2023-12-30T03:59:53.964198Z","shell.execute_reply":"2023-12-30T03:59:54.141376Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<H2>Install Stable Baseline3 version >= 2.0.0a5</H2>","metadata":{}},{"cell_type":"markdown","source":"<H3>Note some SB3 versions are not compatible with Gymnasium interface.</H3>","metadata":{}},{"cell_type":"code","source":"!pip install \"stable-baselines3[extra] >= 2.0.0a5\"","metadata":{"execution":{"iopub.status.busy":"2023-12-30T03:59:54.143912Z","iopub.execute_input":"2023-12-30T03:59:54.144760Z","iopub.status.idle":"2023-12-30T04:00:24.488141Z","shell.execute_reply.started":"2023-12-30T03:59:54.144725Z","shell.execute_reply":"2023-12-30T04:00:24.486854Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: stable-baselines3[extra]>=2.0.0a5 in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (0.29.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (1.24.3)\nRequirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (2.0.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (2.2.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (2.0.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (3.7.4)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (4.8.1.78)\nCollecting pygame (from stable-baselines3[extra]>=2.0.0a5)\n  Obtaining dependency information for pygame from https://files.pythonhosted.org/packages/c8/c7/0d77e0e327bf09c12f445f92f5bad0b447375d7b836c5bac5255ead8436f/pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: tensorboard>=2.9.1 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (2.13.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (5.9.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (4.66.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (13.5.2)\nCollecting shimmy[atari]~=1.1.0 (from stable-baselines3[extra]>=2.0.0a5)\n  Obtaining dependency information for shimmy[atari]~=1.1.0 from https://files.pythonhosted.org/packages/d5/fb/083e36bbcf325f6304bbeb2278b102c4ac8e87eb1ca771780f64decbb2f1/Shimmy-1.1.0-py3-none-any.whl.metadata\n  Downloading Shimmy-1.1.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (10.1.0)\nCollecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra]>=2.0.0a5)\n  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (2.31.0)\nCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5)\n  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5) (4.5.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5) (0.0.4)\nCollecting ale-py~=0.8.1 (from shimmy[atari]~=1.1.0->stable-baselines3[extra]>=2.0.0a5)\n  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (3.4.4)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (68.1.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (3.0.1)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (0.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (3.1.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]>=2.0.0a5) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]>=2.0.0a5) (2023.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5) (2.16.1)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0->stable-baselines3[extra]>=2.0.0a5) (5.13.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (4.9)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.3.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5) (0.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (3.2.2)\nDownloading pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading Shimmy-1.1.0-py3-none-any.whl (37 kB)\nBuilding wheels for collected packages: AutoROM.accept-rom-license\n  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=41ad348a0553a4dce8ee12e1ed9128f94da6d7194846b72258b6c1f249a746a1\n  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\nSuccessfully built AutoROM.accept-rom-license\nInstalling collected packages: pygame, ale-py, shimmy, AutoROM.accept-rom-license, autorom\n  Attempting uninstall: shimmy\n    Found existing installation: Shimmy 1.3.0\n    Uninstalling Shimmy-1.3.0:\n      Successfully uninstalled Shimmy-1.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires shimmy>=1.2.1, but you have shimmy 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 pygame-2.5.2 shimmy-1.1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<H2>Run the Java Tetris Server (v0.6) using subprocess</H2>","metadata":{}},{"cell_type":"markdown","source":"<p>The latest server can be found on GitHub (JavaTetris4RL).</p>\n<href>https://github.com/aiotlab-teaching/JavaTetris4RL</href>","metadata":{}},{"cell_type":"code","source":"# Download v0.5 server from AIoTLab website\n!wget http://www.aiotlab.org/teaching/oop/tetris/TetrisTCPserver_v0.6.jar","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:24.489709Z","iopub.execute_input":"2023-12-30T04:00:24.490103Z","iopub.status.idle":"2023-12-30T04:00:34.777659Z","shell.execute_reply.started":"2023-12-30T04:00:24.490059Z","shell.execute_reply":"2023-12-30T04:00:34.776714Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2023-12-30 04:00:25--  http://www.aiotlab.org/teaching/oop/tetris/TetrisTCPserver_v0.6.jar\nResolving www.aiotlab.org (www.aiotlab.org)... 52.219.164.49, 52.219.40.237, 52.219.164.131, ...\nConnecting to www.aiotlab.org (www.aiotlab.org)|52.219.164.49|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3879189 (3.7M) [binary/octet-stream]\nSaving to: 'TetrisTCPserver_v0.6.jar'\n\nTetrisTCPserver_v0. 100%[===================>]   3.70M   776KB/s    in 8.7s    \n\n2023-12-30 04:00:34 (436 KB/s) - 'TetrisTCPserver_v0.6.jar' saved [3879189/3879189]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import subprocess\nsubprocess.Popen([\"java\",\"-jar\",\"TetrisTCPserver_v0.6.jar\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:34.778986Z","iopub.execute_input":"2023-12-30T04:00:34.779310Z","iopub.status.idle":"2023-12-30T04:00:34.790987Z","shell.execute_reply.started":"2023-12-30T04:00:34.779281Z","shell.execute_reply":"2023-12-30T04:00:34.790044Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<Popen: returncode: None args: ['java', '-jar', 'TetrisTCPserver_v0.6.jar']>"},"metadata":{}}]},{"cell_type":"markdown","source":"<H2>Create our own Tetris Test environment by inheriting gym.Env class</H2>","metadata":{}},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:34.793539Z","iopub.execute_input":"2023-12-30T04:00:34.793804Z","iopub.status.idle":"2023-12-30T04:00:35.574991Z","shell.execute_reply.started":"2023-12-30T04:00:34.793780Z","shell.execute_reply":"2023-12-30T04:00:35.574272Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Tetris TCP server is listening at 10612\n","output_type":"stream"}]},{"cell_type":"code","source":"class TetrisEnv(gym.Env):\n    \n    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 20}\n    \n    '''\n        The supported actions are\n        0: move -1\n        1: move 1\n        2: rotate 0 // counter-clockwise\n        3: rotate 1 // clockwise\n        4: drop down\n    '''\n    N_DISCRETE_ACTIONS = 5\n    \n    IMG_HEIGHT = 200\n    IMG_WIDTH = 100\n    IMG_CHANNELS = 3\n    \n\n    def __init__(self, host_ip=\"127.0.0.1\", host_port=10612):\n        super().__init__()\n        \n        self.action_space = spaces.Discrete(self.N_DISCRETE_ACTIONS)\n        # Example for using image as input (channel-first; channel-last also works):\n        self.observation_space = spaces.Box(low=0, high=255,\n                                            shape=(self.IMG_HEIGHT, self.IMG_WIDTH, self.IMG_CHANNELS), dtype=np.uint8)\n        self.server_ip = host_ip\n        self.server_port = host_port\n            \n        self.client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.client_sock.connect((self.server_ip, self.server_port))\n\n    def step(self, action):\n        if action == 0:\n            self.client_sock.sendall(b\"move -1\\n\")\n        elif action == 1:\n            self.client_sock.sendall(b\"move 1\\n\")\n        elif action == 2:\n            self.client_sock.sendall(b\"rotate 0\\n\")\n        elif action == 3:\n            self.client_sock.sendall(b\"rotate 1\\n\")\n        elif action == 4:\n            self.client_sock.sendall(b\"drop\\n\")\n            \n        terminated, lines, height, holes, observation = self.get_tetris_server_response(self.client_sock)\n        self.observation = observation\n        \n        reward = 0\n        if action == 4: # Drop reward\n            reward += 5\n            \n        # Negative height reward\n        if height > self.height:\n            reward -= (height - self.height)*5\n        \n        # Positive hole reduction reward\n        if holes < self.holes:\n            reward += (self.holes - holes)*10\n        \n        if lines > self.lines_removed:\n            reward = reward + (lines - self.lines_removed)*1000\n            self.lines_removed = lines\n        \n        if self.lifetime > 400:\n            reward += 1\n        if self.lifetime > 600:\n            reward += 2\n        if self.lifetime > 800:\n            reward += 2\n        if self.lifetime > 1000:\n            reward += 5\n        if reward > 1000:\n            reward += 1\n        if reward > 2000:\n            reward += 2\n        if reward > 3000:\n            reward += 3\n        \n        \n        self.holes = holes\n        self.height = height\n        self.lifetime += 1\n        truncated = False\n        info = {'removed_lines':self.lines_removed, 'lifetime':self.lifetime}\n        return (observation, reward, terminated, truncated, info)\n\n    def reset(self, seed=None, options=None):\n        self.client_sock.sendall(b\"start\\n\")\n        terminated, lines, height, holes, observation = self.get_tetris_server_response(self.client_sock)\n        self.observation = observation\n        self.reward = 0\n        self.lines_removed = 0\n        self.holes = 0\n        self.height = 0\n        self.lifetime = 0\n        info = {}\n        return observation, info\n\n    def render(self):\n        ''''''\n        #if self.render_mode == \"console\":\n        #    print('Total reward ' + str(self.reward))\n        '''\n        if self.render_mode == \"human\":\n            cv2.imshow(\"Image\", self.observation)\n            cv2.waitKey(0)\n            cv2.destroyAllWindows()\n        '''\n\n    def close(self):\n        self.client_sock.close()\n        \n    def get_tetris_server_response(self, sock):\n        is_game_over = (sock.recv(1) == b'\\x01')\n        removed_lines = int.from_bytes(sock.recv(4), 'big')\n        height = int.from_bytes(sock.recv(4), 'big')\n        holes = int.from_bytes(sock.recv(4), 'big')\n        img_size = int.from_bytes(sock.recv(4), 'big')\n        img_png = sock.recv(img_size)\n\n        nparr = np.frombuffer(img_png, np.uint8)\n        np_image = cv2.imdecode(nparr, -1)\n\n        return is_game_over, removed_lines, height, holes, np_image","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:35.576146Z","iopub.execute_input":"2023-12-30T04:00:35.576421Z","iopub.status.idle":"2023-12-30T04:00:35.596419Z","shell.execute_reply.started":"2023-12-30T04:00:35.576397Z","shell.execute_reply":"2023-12-30T04:00:35.595519Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<H2>Use SB3 env_checker to check our environment</H2>","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.env_checker import check_env\n\nenv = TetrisEnv()\n# It will check your custom environment and output additional warnings if needed\n# No response may be caused by mismatched action state definition and implementation\ncheck_env(env)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:35.597488Z","iopub.execute_input":"2023-12-30T04:00:35.598058Z","iopub.status.idle":"2023-12-30T04:00:51.330848Z","shell.execute_reply.started":"2023-12-30T04:00:35.598025Z","shell.execute_reply":"2023-12-30T04:00:51.330030Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"Client has joined the game\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<H2>Randomly test the environment</H2>","metadata":{}},{"cell_type":"code","source":"obs, info = env.reset()\nn_steps = 20\nfor _ in range(n_steps):\n    # Random action\n    action = env.action_space.sample()\n    obs, reward, terminated, truncated, info = env.step(action)\n    \n    env.render() # We render nothing now\n    \n    if terminated:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:51.331958Z","iopub.execute_input":"2023-12-30T04:00:51.332505Z","iopub.status.idle":"2023-12-30T04:00:52.197679Z","shell.execute_reply.started":"2023-12-30T04:00:51.332478Z","shell.execute_reply":"2023-12-30T04:00:52.196859Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Show the final screen\n%matplotlib inline \nplt.imshow(obs)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:52.198754Z","iopub.execute_input":"2023-12-30T04:00:52.199026Z","iopub.status.idle":"2023-12-30T04:00:52.466852Z","shell.execute_reply.started":"2023-12-30T04:00:52.199001Z","shell.execute_reply":"2023-12-30T04:00:52.465989Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7d2167b32b90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"from stable_baselines3 import PPO, A2C, DQN\nfrom stable_baselines3.common.env_util import make_vec_env","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:52.467877Z","iopub.execute_input":"2023-12-30T04:00:52.468182Z","iopub.status.idle":"2023-12-30T04:00:52.472299Z","shell.execute_reply.started":"2023-12-30T04:00:52.468156Z","shell.execute_reply":"2023-12-30T04:00:52.471440Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<H2>Create an environment with 30 client threads</H2>","metadata":{}},{"cell_type":"code","source":"# Let's try A2C by creating 30 environments\nvec_env = make_vec_env(TetrisEnv, n_envs=30)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:52.473605Z","iopub.execute_input":"2023-12-30T04:00:52.474420Z","iopub.status.idle":"2023-12-30T04:00:52.541508Z","shell.execute_reply.started":"2023-12-30T04:00:52.474384Z","shell.execute_reply":"2023-12-30T04:00:52.540649Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Client has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<H2>We choose A2C with CNN policy, and train 2,000,000 steps</H2>","metadata":{}},{"cell_type":"code","source":"# Train the agent\nmodel = A2C(\"CnnPolicy\", vec_env, verbose=1).learn(2000000)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T04:00:52.542564Z","iopub.execute_input":"2023-12-30T04:00:52.542829Z","iopub.status.idle":"2023-12-30T05:36:12.140505Z","shell.execute_reply.started":"2023-12-30T04:00:52.542806Z","shell.execute_reply":"2023-12-30T05:36:12.139605Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Using cuda device\nWrapping the env in a VecTransposeImage.\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 76.7     |\n|    ep_rew_mean        | -30.6    |\n| time/                 |          |\n|    fps                | 315      |\n|    iterations         | 100      |\n|    time_elapsed       | 47       |\n|    total_timesteps    | 15000    |\n| train/                |          |\n|    entropy_loss       | -1.13    |\n|    explained_variance | 0.229    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 99       |\n|    policy_loss        | -0.716   |\n|    value_loss         | 8.88     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 95.7     |\n|    ep_rew_mean        | -83.2    |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 200      |\n|    time_elapsed       | 88       |\n|    total_timesteps    | 30000    |\n| train/                |          |\n|    entropy_loss       | -0.0152  |\n|    explained_variance | 0.876    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 199      |\n|    policy_loss        | -0.0762  |\n|    value_loss         | 24.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 18.4     |\n|    ep_rew_mean        | -48.3    |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 300      |\n|    time_elapsed       | 136      |\n|    total_timesteps    | 45000    |\n| train/                |          |\n|    entropy_loss       | -0.632   |\n|    explained_variance | 0.612    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 299      |\n|    policy_loss        | 1.2      |\n|    value_loss         | 23.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 72.7     |\n|    ep_rew_mean        | -23.3    |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 400      |\n|    time_elapsed       | 181      |\n|    total_timesteps    | 60000    |\n| train/                |          |\n|    entropy_loss       | -0.848   |\n|    explained_variance | 0.14     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 399      |\n|    policy_loss        | -0.505   |\n|    value_loss         | 17.8     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 93.7     |\n|    ep_rew_mean        | -56.8    |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 500      |\n|    time_elapsed       | 223      |\n|    total_timesteps    | 75000    |\n| train/                |          |\n|    entropy_loss       | -0.677   |\n|    explained_variance | 0.626    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 499      |\n|    policy_loss        | 1.12     |\n|    value_loss         | 9.82     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 70.2     |\n|    ep_rew_mean        | -19.6    |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 600      |\n|    time_elapsed       | 266      |\n|    total_timesteps    | 90000    |\n| train/                |          |\n|    entropy_loss       | -0.918   |\n|    explained_variance | 0.256    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 599      |\n|    policy_loss        | -0.331   |\n|    value_loss         | 17.3     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 79.2     |\n|    ep_rew_mean        | -15.8    |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 700      |\n|    time_elapsed       | 308      |\n|    total_timesteps    | 105000   |\n| train/                |          |\n|    entropy_loss       | -0.75    |\n|    explained_variance | 0.422    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 699      |\n|    policy_loss        | -0.0371  |\n|    value_loss         | 8.94     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 65.8     |\n|    ep_rew_mean        | -10.8    |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 800      |\n|    time_elapsed       | 351      |\n|    total_timesteps    | 120000   |\n| train/                |          |\n|    entropy_loss       | -0.874   |\n|    explained_variance | 0.765    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 799      |\n|    policy_loss        | -0.263   |\n|    value_loss         | 5.41     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 73.6     |\n|    ep_rew_mean        | -8.75    |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 900      |\n|    time_elapsed       | 394      |\n|    total_timesteps    | 135000   |\n| train/                |          |\n|    entropy_loss       | -0.372   |\n|    explained_variance | 0.476    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 899      |\n|    policy_loss        | 0.0917   |\n|    value_loss         | 9.32     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 70.8     |\n|    ep_rew_mean        | -1.25    |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 1000     |\n|    time_elapsed       | 438      |\n|    total_timesteps    | 150000   |\n| train/                |          |\n|    entropy_loss       | -0.287   |\n|    explained_variance | 0.816    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 999      |\n|    policy_loss        | -0.168   |\n|    value_loss         | 6.1      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 80.1     |\n|    ep_rew_mean        | 46.4     |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 1100     |\n|    time_elapsed       | 480      |\n|    total_timesteps    | 165000   |\n| train/                |          |\n|    entropy_loss       | -0.297   |\n|    explained_variance | 0.849    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1099     |\n|    policy_loss        | -0.0582  |\n|    value_loss         | 8.61     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 82.4     |\n|    ep_rew_mean        | 27.8     |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 1200     |\n|    time_elapsed       | 523      |\n|    total_timesteps    | 180000   |\n| train/                |          |\n|    entropy_loss       | -0.378   |\n|    explained_variance | 0.862    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1199     |\n|    policy_loss        | -0.383   |\n|    value_loss         | 9.82     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 84.2     |\n|    ep_rew_mean        | 31.7     |\n| time/                 |          |\n|    fps                | 344      |\n|    iterations         | 1300     |\n|    time_elapsed       | 565      |\n|    total_timesteps    | 195000   |\n| train/                |          |\n|    entropy_loss       | -0.271   |\n|    explained_variance | 0.871    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1299     |\n|    policy_loss        | -0.453   |\n|    value_loss         | 9.21     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 86.6     |\n|    ep_rew_mean        | 66       |\n| time/                 |          |\n|    fps                | 345      |\n|    iterations         | 1400     |\n|    time_elapsed       | 608      |\n|    total_timesteps    | 210000   |\n| train/                |          |\n|    entropy_loss       | -0.264   |\n|    explained_variance | 0.0226   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1399     |\n|    policy_loss        | -0.222   |\n|    value_loss         | 1.36e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 81.7     |\n|    ep_rew_mean        | 87.7     |\n| time/                 |          |\n|    fps                | 345      |\n|    iterations         | 1500     |\n|    time_elapsed       | 651      |\n|    total_timesteps    | 225000   |\n| train/                |          |\n|    entropy_loss       | -0.189   |\n|    explained_variance | 0.00826  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1499     |\n|    policy_loss        | 5.73     |\n|    value_loss         | 2.63e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 87.2     |\n|    ep_rew_mean        | 39.9     |\n| time/                 |          |\n|    fps                | 346      |\n|    iterations         | 1600     |\n|    time_elapsed       | 693      |\n|    total_timesteps    | 240000   |\n| train/                |          |\n|    entropy_loss       | -0.159   |\n|    explained_variance | 0.863    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1599     |\n|    policy_loss        | -0.558   |\n|    value_loss         | 11.6     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 83.8     |\n|    ep_rew_mean        | 102      |\n| time/                 |          |\n|    fps                | 346      |\n|    iterations         | 1700     |\n|    time_elapsed       | 735      |\n|    total_timesteps    | 255000   |\n| train/                |          |\n|    entropy_loss       | -0.159   |\n|    explained_variance | 0.96     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1699     |\n|    policy_loss        | 0.108    |\n|    value_loss         | 4.28     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 86.1     |\n|    ep_rew_mean        | 94.6     |\n| time/                 |          |\n|    fps                | 346      |\n|    iterations         | 1800     |\n|    time_elapsed       | 778      |\n|    total_timesteps    | 270000   |\n| train/                |          |\n|    entropy_loss       | -0.156   |\n|    explained_variance | 0.921    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1799     |\n|    policy_loss        | -0.14    |\n|    value_loss         | 4.23     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 82.9     |\n|    ep_rew_mean        | 104      |\n| time/                 |          |\n|    fps                | 347      |\n|    iterations         | 1900     |\n|    time_elapsed       | 821      |\n|    total_timesteps    | 285000   |\n| train/                |          |\n|    entropy_loss       | -0.184   |\n|    explained_variance | 0.929    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1899     |\n|    policy_loss        | -0.306   |\n|    value_loss         | 6.38     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 95.5     |\n|    ep_rew_mean        | 166      |\n| time/                 |          |\n|    fps                | 347      |\n|    iterations         | 2000     |\n|    time_elapsed       | 863      |\n|    total_timesteps    | 300000   |\n| train/                |          |\n|    entropy_loss       | -0.185   |\n|    explained_variance | 0.942    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1999     |\n|    policy_loss        | 0.109    |\n|    value_loss         | 5.02     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 94       |\n|    ep_rew_mean        | 193      |\n| time/                 |          |\n|    fps                | 347      |\n|    iterations         | 2100     |\n|    time_elapsed       | 905      |\n|    total_timesteps    | 315000   |\n| train/                |          |\n|    entropy_loss       | -0.125   |\n|    explained_variance | 0.931    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2099     |\n|    policy_loss        | 0.00615  |\n|    value_loss         | 6.24     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 91.1     |\n|    ep_rew_mean        | 153      |\n| time/                 |          |\n|    fps                | 347      |\n|    iterations         | 2200     |\n|    time_elapsed       | 949      |\n|    total_timesteps    | 330000   |\n| train/                |          |\n|    entropy_loss       | -0.133   |\n|    explained_variance | 0.924    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2199     |\n|    policy_loss        | 0.15     |\n|    value_loss         | 3.84     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 92.3     |\n|    ep_rew_mean        | 197      |\n| time/                 |          |\n|    fps                | 347      |\n|    iterations         | 2300     |\n|    time_elapsed       | 992      |\n|    total_timesteps    | 345000   |\n| train/                |          |\n|    entropy_loss       | -0.0834  |\n|    explained_variance | 0.968    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2299     |\n|    policy_loss        | -0.128   |\n|    value_loss         | 6.43     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 97.9     |\n|    ep_rew_mean        | 353      |\n| time/                 |          |\n|    fps                | 348      |\n|    iterations         | 2400     |\n|    time_elapsed       | 1034     |\n|    total_timesteps    | 360000   |\n| train/                |          |\n|    entropy_loss       | -0.0782  |\n|    explained_variance | 0.955    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2399     |\n|    policy_loss        | -0.0989  |\n|    value_loss         | 9.69     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 318      |\n| time/                 |          |\n|    fps                | 348      |\n|    iterations         | 2500     |\n|    time_elapsed       | 1075     |\n|    total_timesteps    | 375000   |\n| train/                |          |\n|    entropy_loss       | -0.0716  |\n|    explained_variance | 0.936    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2499     |\n|    policy_loss        | 0.0438   |\n|    value_loss         | 4.96     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 96.2     |\n|    ep_rew_mean        | 330      |\n| time/                 |          |\n|    fps                | 348      |\n|    iterations         | 2600     |\n|    time_elapsed       | 1117     |\n|    total_timesteps    | 390000   |\n| train/                |          |\n|    entropy_loss       | -0.12    |\n|    explained_variance | 0.044    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2599     |\n|    policy_loss        | -0.0623  |\n|    value_loss         | 1.38e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 96.7     |\n|    ep_rew_mean        | 215      |\n| time/                 |          |\n|    fps                | 349      |\n|    iterations         | 2700     |\n|    time_elapsed       | 1159     |\n|    total_timesteps    | 405000   |\n| train/                |          |\n|    entropy_loss       | -0.127   |\n|    explained_variance | 0.94     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2699     |\n|    policy_loss        | 0.0424   |\n|    value_loss         | 4.36     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 88.3     |\n|    ep_rew_mean        | 247      |\n| time/                 |          |\n|    fps                | 349      |\n|    iterations         | 2800     |\n|    time_elapsed       | 1202     |\n|    total_timesteps    | 420000   |\n| train/                |          |\n|    entropy_loss       | -0.101   |\n|    explained_variance | 0.973    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2799     |\n|    policy_loss        | -0.264   |\n|    value_loss         | 8.13     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 97.2     |\n|    ep_rew_mean        | 420      |\n| time/                 |          |\n|    fps                | 349      |\n|    iterations         | 2900     |\n|    time_elapsed       | 1245     |\n|    total_timesteps    | 435000   |\n| train/                |          |\n|    entropy_loss       | -0.0649  |\n|    explained_variance | 0.979    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2899     |\n|    policy_loss        | -0.00473 |\n|    value_loss         | 2.94     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 98       |\n|    ep_rew_mean        | 429      |\n| time/                 |          |\n|    fps                | 349      |\n|    iterations         | 3000     |\n|    time_elapsed       | 1287     |\n|    total_timesteps    | 450000   |\n| train/                |          |\n|    entropy_loss       | -0.0945  |\n|    explained_variance | 0.0566   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2999     |\n|    policy_loss        | -0.0434  |\n|    value_loss         | 6.86e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 98.5     |\n|    ep_rew_mean        | 295      |\n| time/                 |          |\n|    fps                | 349      |\n|    iterations         | 3100     |\n|    time_elapsed       | 1329     |\n|    total_timesteps    | 465000   |\n| train/                |          |\n|    entropy_loss       | -0.109   |\n|    explained_variance | -0.0179  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3099     |\n|    policy_loss        | 0.352    |\n|    value_loss         | 1.71e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 357      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 3200     |\n|    time_elapsed       | 1371     |\n|    total_timesteps    | 480000   |\n| train/                |          |\n|    entropy_loss       | -0.0876  |\n|    explained_variance | 0.942    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3199     |\n|    policy_loss        | -0.108   |\n|    value_loss         | 9.4      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 412      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 3300     |\n|    time_elapsed       | 1412     |\n|    total_timesteps    | 495000   |\n| train/                |          |\n|    entropy_loss       | -0.0733  |\n|    explained_variance | 0.964    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3299     |\n|    policy_loss        | 0.00643  |\n|    value_loss         | 8.16     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 453      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 3400     |\n|    time_elapsed       | 1454     |\n|    total_timesteps    | 510000   |\n| train/                |          |\n|    entropy_loss       | -0.1     |\n|    explained_variance | 0.0396   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3399     |\n|    policy_loss        | 5.87     |\n|    value_loss         | 2.59e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 422      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 3500     |\n|    time_elapsed       | 1496     |\n|    total_timesteps    | 525000   |\n| train/                |          |\n|    entropy_loss       | -0.0638  |\n|    explained_variance | 0.968    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3499     |\n|    policy_loss        | -0.0059  |\n|    value_loss         | 5.66     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 514      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 3600     |\n|    time_elapsed       | 1538     |\n|    total_timesteps    | 540000   |\n| train/                |          |\n|    entropy_loss       | -0.0996  |\n|    explained_variance | 0.98     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3599     |\n|    policy_loss        | -0.152   |\n|    value_loss         | 3.65     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 533      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 3700     |\n|    time_elapsed       | 1579     |\n|    total_timesteps    | 555000   |\n| train/                |          |\n|    entropy_loss       | -0.0449  |\n|    explained_variance | 0.972    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3699     |\n|    policy_loss        | -0.106   |\n|    value_loss         | 10.4     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 340      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 3800     |\n|    time_elapsed       | 1621     |\n|    total_timesteps    | 570000   |\n| train/                |          |\n|    entropy_loss       | -0.108   |\n|    explained_variance | 0.965    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3799     |\n|    policy_loss        | -0.0698  |\n|    value_loss         | 5.81     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 410      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 3900     |\n|    time_elapsed       | 1663     |\n|    total_timesteps    | 585000   |\n| train/                |          |\n|    entropy_loss       | -0.103   |\n|    explained_variance | 0.0232   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3899     |\n|    policy_loss        | 0.302    |\n|    value_loss         | 2.89e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 368      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 4000     |\n|    time_elapsed       | 1705     |\n|    total_timesteps    | 600000   |\n| train/                |          |\n|    entropy_loss       | -0.0722  |\n|    explained_variance | 0.972    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3999     |\n|    policy_loss        | -0.0798  |\n|    value_loss         | 5.07     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 481      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 4100     |\n|    time_elapsed       | 1747     |\n|    total_timesteps    | 615000   |\n| train/                |          |\n|    entropy_loss       | -0.104   |\n|    explained_variance | 0.975    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4099     |\n|    policy_loss        | -0.126   |\n|    value_loss         | 4.43     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 410      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 4200     |\n|    time_elapsed       | 1788     |\n|    total_timesteps    | 630000   |\n| train/                |          |\n|    entropy_loss       | -0.0822  |\n|    explained_variance | 0.956    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4199     |\n|    policy_loss        | -0.103   |\n|    value_loss         | 3.88     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 540      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 4300     |\n|    time_elapsed       | 1830     |\n|    total_timesteps    | 645000   |\n| train/                |          |\n|    entropy_loss       | -0.0733  |\n|    explained_variance | 0.972    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4299     |\n|    policy_loss        | -0.11    |\n|    value_loss         | 10.9     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 348      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 4400     |\n|    time_elapsed       | 1872     |\n|    total_timesteps    | 660000   |\n| train/                |          |\n|    entropy_loss       | -0.136   |\n|    explained_variance | 0.0221   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4399     |\n|    policy_loss        | -0.184   |\n|    value_loss         | 1.38e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 487      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 4500     |\n|    time_elapsed       | 1914     |\n|    total_timesteps    | 675000   |\n| train/                |          |\n|    entropy_loss       | -0.0595  |\n|    explained_variance | 0.0285   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4499     |\n|    policy_loss        | -0.018   |\n|    value_loss         | 1.35e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 429      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 4600     |\n|    time_elapsed       | 1956     |\n|    total_timesteps    | 690000   |\n| train/                |          |\n|    entropy_loss       | -0.0931  |\n|    explained_variance | 0.0273   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4599     |\n|    policy_loss        | -0.163   |\n|    value_loss         | 1.97e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 483      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 4700     |\n|    time_elapsed       | 1997     |\n|    total_timesteps    | 705000   |\n| train/                |          |\n|    entropy_loss       | -0.0794  |\n|    explained_variance | 0.0259   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4699     |\n|    policy_loss        | -0.0418  |\n|    value_loss         | 5.33e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 569      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 4800     |\n|    time_elapsed       | 2039     |\n|    total_timesteps    | 720000   |\n| train/                |          |\n|    entropy_loss       | -0.0572  |\n|    explained_variance | 0.00486  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4799     |\n|    policy_loss        | 0.00671  |\n|    value_loss         | 3.43e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 627      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 4900     |\n|    time_elapsed       | 2081     |\n|    total_timesteps    | 735000   |\n| train/                |          |\n|    entropy_loss       | -0.0714  |\n|    explained_variance | 0.958    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4899     |\n|    policy_loss        | -0.0653  |\n|    value_loss         | 20.5     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 452      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5000     |\n|    time_elapsed       | 2123     |\n|    total_timesteps    | 750000   |\n| train/                |          |\n|    entropy_loss       | -0.132   |\n|    explained_variance | 0.00613  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4999     |\n|    policy_loss        | 0.468    |\n|    value_loss         | 2.12e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 513      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5100     |\n|    time_elapsed       | 2165     |\n|    total_timesteps    | 765000   |\n| train/                |          |\n|    entropy_loss       | -0.124   |\n|    explained_variance | 0.964    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5099     |\n|    policy_loss        | -0.0644  |\n|    value_loss         | 6.33     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 402      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5200     |\n|    time_elapsed       | 2207     |\n|    total_timesteps    | 780000   |\n| train/                |          |\n|    entropy_loss       | -0.0676  |\n|    explained_variance | 0.957    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5199     |\n|    policy_loss        | -0.066   |\n|    value_loss         | 5.59     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 631      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5300     |\n|    time_elapsed       | 2249     |\n|    total_timesteps    | 795000   |\n| train/                |          |\n|    entropy_loss       | -0.0466  |\n|    explained_variance | 0.0902   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5299     |\n|    policy_loss        | -0.459   |\n|    value_loss         | 3.51e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 318      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5400     |\n|    time_elapsed       | 2292     |\n|    total_timesteps    | 810000   |\n| train/                |          |\n|    entropy_loss       | -0.0672  |\n|    explained_variance | 0.977    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5399     |\n|    policy_loss        | -0.063   |\n|    value_loss         | 7.34     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 661      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5500     |\n|    time_elapsed       | 2334     |\n|    total_timesteps    | 825000   |\n| train/                |          |\n|    entropy_loss       | -0.108   |\n|    explained_variance | 0.961    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5499     |\n|    policy_loss        | -0.185   |\n|    value_loss         | 24.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 112      |\n|    ep_rew_mean        | 534      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5600     |\n|    time_elapsed       | 2376     |\n|    total_timesteps    | 840000   |\n| train/                |          |\n|    entropy_loss       | -0.105   |\n|    explained_variance | 0.92     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5599     |\n|    policy_loss        | -0.234   |\n|    value_loss         | 29.6     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 418      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5700     |\n|    time_elapsed       | 2417     |\n|    total_timesteps    | 855000   |\n| train/                |          |\n|    entropy_loss       | -0.0612  |\n|    explained_variance | 0.979    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5699     |\n|    policy_loss        | -0.0588  |\n|    value_loss         | 5.45     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 107      |\n|    ep_rew_mean        | 671      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5800     |\n|    time_elapsed       | 2459     |\n|    total_timesteps    | 870000   |\n| train/                |          |\n|    entropy_loss       | -0.0281  |\n|    explained_variance | 0.0584   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5799     |\n|    policy_loss        | -0.236   |\n|    value_loss         | 7.81e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 547      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 5900     |\n|    time_elapsed       | 2502     |\n|    total_timesteps    | 885000   |\n| train/                |          |\n|    entropy_loss       | -0.0506  |\n|    explained_variance | -0.0236  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5899     |\n|    policy_loss        | -0.19    |\n|    value_loss         | 1.57e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 412      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 6000     |\n|    time_elapsed       | 2543     |\n|    total_timesteps    | 900000   |\n| train/                |          |\n|    entropy_loss       | -0.0844  |\n|    explained_variance | 0.942    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5999     |\n|    policy_loss        | -0.0153  |\n|    value_loss         | 11.8     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 111      |\n|    ep_rew_mean        | 483      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 6100     |\n|    time_elapsed       | 2585     |\n|    total_timesteps    | 915000   |\n| train/                |          |\n|    entropy_loss       | -0.135   |\n|    explained_variance | 0.937    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6099     |\n|    policy_loss        | 0.568    |\n|    value_loss         | 16.6     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 633      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 6200     |\n|    time_elapsed       | 2627     |\n|    total_timesteps    | 930000   |\n| train/                |          |\n|    entropy_loss       | -0.0633  |\n|    explained_variance | 0.953    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6199     |\n|    policy_loss        | -0.0283  |\n|    value_loss         | 16       |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 111      |\n|    ep_rew_mean        | 746      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 6300     |\n|    time_elapsed       | 2669     |\n|    total_timesteps    | 945000   |\n| train/                |          |\n|    entropy_loss       | -0.0374  |\n|    explained_variance | -0.0623  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6299     |\n|    policy_loss        | -0.0778  |\n|    value_loss         | 3.42e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 409      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 6400     |\n|    time_elapsed       | 2711     |\n|    total_timesteps    | 960000   |\n| train/                |          |\n|    entropy_loss       | -0.0642  |\n|    explained_variance | -0.0453  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6399     |\n|    policy_loss        | 5.49     |\n|    value_loss         | 4.06e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 548      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 6500     |\n|    time_elapsed       | 2753     |\n|    total_timesteps    | 975000   |\n| train/                |          |\n|    entropy_loss       | -0.0443  |\n|    explained_variance | 0.064    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6499     |\n|    policy_loss        | -0.19    |\n|    value_loss         | 2.82e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 520      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 6600     |\n|    time_elapsed       | 2795     |\n|    total_timesteps    | 990000   |\n| train/                |          |\n|    entropy_loss       | -0.0558  |\n|    explained_variance | 0.928    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6599     |\n|    policy_loss        | -0.134   |\n|    value_loss         | 54.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 467      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 6700     |\n|    time_elapsed       | 2838     |\n|    total_timesteps    | 1005000  |\n| train/                |          |\n|    entropy_loss       | -0.0766  |\n|    explained_variance | -0.0105  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6699     |\n|    policy_loss        | -0.176   |\n|    value_loss         | 2.02e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 803      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 6800     |\n|    time_elapsed       | 2880     |\n|    total_timesteps    | 1020000  |\n| train/                |          |\n|    entropy_loss       | -0.0432  |\n|    explained_variance | 0.235    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6799     |\n|    policy_loss        | -0.0596  |\n|    value_loss         | 6.87e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 568      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 6900     |\n|    time_elapsed       | 2922     |\n|    total_timesteps    | 1035000  |\n| train/                |          |\n|    entropy_loss       | -0.0263  |\n|    explained_variance | 0.118    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6899     |\n|    policy_loss        | -0.0921  |\n|    value_loss         | 2.61e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 579      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7000     |\n|    time_elapsed       | 2965     |\n|    total_timesteps    | 1050000  |\n| train/                |          |\n|    entropy_loss       | -0.031   |\n|    explained_variance | 0.182    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6999     |\n|    policy_loss        | -0.178   |\n|    value_loss         | 6.59e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 762      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7100     |\n|    time_elapsed       | 3007     |\n|    total_timesteps    | 1065000  |\n| train/                |          |\n|    entropy_loss       | -0.0531  |\n|    explained_variance | 0.953    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7099     |\n|    policy_loss        | -0.539   |\n|    value_loss         | 373      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 427      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7200     |\n|    time_elapsed       | 3049     |\n|    total_timesteps    | 1080000  |\n| train/                |          |\n|    entropy_loss       | -0.0685  |\n|    explained_variance | 0.112    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7199     |\n|    policy_loss        | 0.273    |\n|    value_loss         | 2.6e+04  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 97.8     |\n|    ep_rew_mean        | 553      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7300     |\n|    time_elapsed       | 3092     |\n|    total_timesteps    | 1095000  |\n| train/                |          |\n|    entropy_loss       | -0.0592  |\n|    explained_variance | 0.933    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7299     |\n|    policy_loss        | -0.267   |\n|    value_loss         | 94.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 472      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7400     |\n|    time_elapsed       | 3135     |\n|    total_timesteps    | 1110000  |\n| train/                |          |\n|    entropy_loss       | -0.0734  |\n|    explained_variance | 0.926    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7399     |\n|    policy_loss        | -0.243   |\n|    value_loss         | 85.6     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 785      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7500     |\n|    time_elapsed       | 3177     |\n|    total_timesteps    | 1125000  |\n| train/                |          |\n|    entropy_loss       | -0.0146  |\n|    explained_variance | 0.171    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7499     |\n|    policy_loss        | 0.0457   |\n|    value_loss         | 4.52e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 350      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7600     |\n|    time_elapsed       | 3219     |\n|    total_timesteps    | 1140000  |\n| train/                |          |\n|    entropy_loss       | -0.163   |\n|    explained_variance | 0.939    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7599     |\n|    policy_loss        | -0.449   |\n|    value_loss         | 21.9     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 504      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7700     |\n|    time_elapsed       | 3261     |\n|    total_timesteps    | 1155000  |\n| train/                |          |\n|    entropy_loss       | -0.0973  |\n|    explained_variance | 0.0408   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7699     |\n|    policy_loss        | -0.291   |\n|    value_loss         | 4.61e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 541      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7800     |\n|    time_elapsed       | 3304     |\n|    total_timesteps    | 1170000  |\n| train/                |          |\n|    entropy_loss       | -0.0811  |\n|    explained_variance | 0.964    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7799     |\n|    policy_loss        | -0.212   |\n|    value_loss         | 73.7     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 107      |\n|    ep_rew_mean        | 531      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 7900     |\n|    time_elapsed       | 3346     |\n|    total_timesteps    | 1185000  |\n| train/                |          |\n|    entropy_loss       | -0.0637  |\n|    explained_variance | 0.94     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7899     |\n|    policy_loss        | -0.695   |\n|    value_loss         | 103      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 93.7     |\n|    ep_rew_mean        | 217      |\n| time/                 |          |\n|    fps                | 354      |\n|    iterations         | 8000     |\n|    time_elapsed       | 3389     |\n|    total_timesteps    | 1200000  |\n| train/                |          |\n|    entropy_loss       | -0.138   |\n|    explained_variance | 0.887    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7999     |\n|    policy_loss        | -0.176   |\n|    value_loss         | 17.5     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 107      |\n|    ep_rew_mean        | 430      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 8100     |\n|    time_elapsed       | 3432     |\n|    total_timesteps    | 1215000  |\n| train/                |          |\n|    entropy_loss       | -0.109   |\n|    explained_variance | 0.0182   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8099     |\n|    policy_loss        | -0.52    |\n|    value_loss         | 2.07e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 597      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 8200     |\n|    time_elapsed       | 3474     |\n|    total_timesteps    | 1230000  |\n| train/                |          |\n|    entropy_loss       | -0.0458  |\n|    explained_variance | -0.00327 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8199     |\n|    policy_loss        | 0.585    |\n|    value_loss         | 4.19e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 526      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 8300     |\n|    time_elapsed       | 3517     |\n|    total_timesteps    | 1245000  |\n| train/                |          |\n|    entropy_loss       | -0.088   |\n|    explained_variance | 0.0372   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8299     |\n|    policy_loss        | -0.0504  |\n|    value_loss         | 6.75e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 112      |\n|    ep_rew_mean        | 524      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 8400     |\n|    time_elapsed       | 3559     |\n|    total_timesteps    | 1260000  |\n| train/                |          |\n|    entropy_loss       | -0.0656  |\n|    explained_variance | 0.933    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8399     |\n|    policy_loss        | -0.16    |\n|    value_loss         | 26.9     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 525      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 8500     |\n|    time_elapsed       | 3602     |\n|    total_timesteps    | 1275000  |\n| train/                |          |\n|    entropy_loss       | -0.136   |\n|    explained_variance | -0.00831 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8499     |\n|    policy_loss        | 8.75     |\n|    value_loss         | 2.7e+04  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 606      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 8600     |\n|    time_elapsed       | 3644     |\n|    total_timesteps    | 1290000  |\n| train/                |          |\n|    entropy_loss       | -0.0367  |\n|    explained_variance | 0.954    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8599     |\n|    policy_loss        | -0.0397  |\n|    value_loss         | 147      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 484      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 8700     |\n|    time_elapsed       | 3687     |\n|    total_timesteps    | 1305000  |\n| train/                |          |\n|    entropy_loss       | -0.11    |\n|    explained_variance | 0.0645   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8699     |\n|    policy_loss        | 0.0318   |\n|    value_loss         | 3.22e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 448      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 8800     |\n|    time_elapsed       | 3730     |\n|    total_timesteps    | 1320000  |\n| train/                |          |\n|    entropy_loss       | -0.127   |\n|    explained_variance | 0.0017   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8799     |\n|    policy_loss        | 2.88     |\n|    value_loss         | 3.44e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 107      |\n|    ep_rew_mean        | 809      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 8900     |\n|    time_elapsed       | 3772     |\n|    total_timesteps    | 1335000  |\n| train/                |          |\n|    entropy_loss       | -0.053   |\n|    explained_variance | 0.954    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8899     |\n|    policy_loss        | -0.269   |\n|    value_loss         | 57.5     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 322      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9000     |\n|    time_elapsed       | 3815     |\n|    total_timesteps    | 1350000  |\n| train/                |          |\n|    entropy_loss       | -0.059   |\n|    explained_variance | 0.0452   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8999     |\n|    policy_loss        | -0.422   |\n|    value_loss         | 3.98e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 562      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9100     |\n|    time_elapsed       | 3857     |\n|    total_timesteps    | 1365000  |\n| train/                |          |\n|    entropy_loss       | -0.0363  |\n|    explained_variance | 0.929    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9099     |\n|    policy_loss        | -0.13    |\n|    value_loss         | 61.7     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 664      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9200     |\n|    time_elapsed       | 3900     |\n|    total_timesteps    | 1380000  |\n| train/                |          |\n|    entropy_loss       | -0.0664  |\n|    explained_variance | 0.921    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9199     |\n|    policy_loss        | -0.38    |\n|    value_loss         | 135      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 705      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9300     |\n|    time_elapsed       | 3943     |\n|    total_timesteps    | 1395000  |\n| train/                |          |\n|    entropy_loss       | -0.0291  |\n|    explained_variance | 0.219    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9299     |\n|    policy_loss        | -0.315   |\n|    value_loss         | 2.83e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 116      |\n|    ep_rew_mean        | 784      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9400     |\n|    time_elapsed       | 3985     |\n|    total_timesteps    | 1410000  |\n| train/                |          |\n|    entropy_loss       | -0.0105  |\n|    explained_variance | 0.942    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9399     |\n|    policy_loss        | -0.285   |\n|    value_loss         | 280      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 676      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9500     |\n|    time_elapsed       | 4028     |\n|    total_timesteps    | 1425000  |\n| train/                |          |\n|    entropy_loss       | -0.023   |\n|    explained_variance | 0.234    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9499     |\n|    policy_loss        | -0.304   |\n|    value_loss         | 2.53e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 553      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9600     |\n|    time_elapsed       | 4071     |\n|    total_timesteps    | 1440000  |\n| train/                |          |\n|    entropy_loss       | -0.029   |\n|    explained_variance | 0.17     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9599     |\n|    policy_loss        | -0.192   |\n|    value_loss         | 1.37e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 754      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9700     |\n|    time_elapsed       | 4115     |\n|    total_timesteps    | 1455000  |\n| train/                |          |\n|    entropy_loss       | -0.0393  |\n|    explained_variance | 0.0134   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9699     |\n|    policy_loss        | -0.211   |\n|    value_loss         | 1.34e+05 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 861      |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9800     |\n|    time_elapsed       | 4158     |\n|    total_timesteps    | 1470000  |\n| train/                |          |\n|    entropy_loss       | -0.0229  |\n|    explained_variance | 0.918    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9799     |\n|    policy_loss        | 0.0208   |\n|    value_loss         | 656      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 87.3     |\n|    ep_rew_mean        | 11.2     |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 9900     |\n|    time_elapsed       | 4202     |\n|    total_timesteps    | 1485000  |\n| train/                |          |\n|    entropy_loss       | -0.165   |\n|    explained_variance | 0.745    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9899     |\n|    policy_loss        | -0.276   |\n|    value_loss         | 12.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 89.4     |\n|    ep_rew_mean        | 21.9     |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 10000    |\n|    time_elapsed       | 4246     |\n|    total_timesteps    | 1500000  |\n| train/                |          |\n|    entropy_loss       | -0.087   |\n|    explained_variance | 0.933    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9999     |\n|    policy_loss        | 0.0113   |\n|    value_loss         | 5.63     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 85.5     |\n|    ep_rew_mean        | 23.9     |\n| time/                 |          |\n|    fps                | 353      |\n|    iterations         | 10100    |\n|    time_elapsed       | 4290     |\n|    total_timesteps    | 1515000  |\n| train/                |          |\n|    entropy_loss       | -0.0681  |\n|    explained_variance | 0.921    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10099    |\n|    policy_loss        | 0.0394   |\n|    value_loss         | 5.78     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 95.1     |\n|    ep_rew_mean        | 133      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 10200    |\n|    time_elapsed       | 4334     |\n|    total_timesteps    | 1530000  |\n| train/                |          |\n|    entropy_loss       | -0.0638  |\n|    explained_variance | 0.937    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10199    |\n|    policy_loss        | -0.0509  |\n|    value_loss         | 10.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 669      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 10300    |\n|    time_elapsed       | 4377     |\n|    total_timesteps    | 1545000  |\n| train/                |          |\n|    entropy_loss       | -0.0202  |\n|    explained_variance | -0.0132  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10299    |\n|    policy_loss        | -0.585   |\n|    value_loss         | 6.06e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 111      |\n|    ep_rew_mean        | 770      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 10400    |\n|    time_elapsed       | 4420     |\n|    total_timesteps    | 1560000  |\n| train/                |          |\n|    entropy_loss       | -0.0105  |\n|    explained_variance | 0.93     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10399    |\n|    policy_loss        | -0.0263  |\n|    value_loss         | 695      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 629      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 10500    |\n|    time_elapsed       | 4465     |\n|    total_timesteps    | 1575000  |\n| train/                |          |\n|    entropy_loss       | -0.0193  |\n|    explained_variance | 0.227    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10499    |\n|    policy_loss        | -0.0781  |\n|    value_loss         | 3.99e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 535      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 10600    |\n|    time_elapsed       | 4508     |\n|    total_timesteps    | 1590000  |\n| train/                |          |\n|    entropy_loss       | -0.0173  |\n|    explained_variance | 0.284    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10599    |\n|    policy_loss        | 0.0028   |\n|    value_loss         | 4.41e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 657      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 10700    |\n|    time_elapsed       | 4551     |\n|    total_timesteps    | 1605000  |\n| train/                |          |\n|    entropy_loss       | -0.024   |\n|    explained_variance | 0.878    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10699    |\n|    policy_loss        | -0.49    |\n|    value_loss         | 486      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 111      |\n|    ep_rew_mean        | 551      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 10800    |\n|    time_elapsed       | 4595     |\n|    total_timesteps    | 1620000  |\n| train/                |          |\n|    entropy_loss       | -0.0828  |\n|    explained_variance | 0.931    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10799    |\n|    policy_loss        | -0.702   |\n|    value_loss         | 71.8     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 723      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 10900    |\n|    time_elapsed       | 4638     |\n|    total_timesteps    | 1635000  |\n| train/                |          |\n|    entropy_loss       | -0.0367  |\n|    explained_variance | 0.504    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10899    |\n|    policy_loss        | -0.905   |\n|    value_loss         | 6.86e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 599      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 11000    |\n|    time_elapsed       | 4682     |\n|    total_timesteps    | 1650000  |\n| train/                |          |\n|    entropy_loss       | -0.0177  |\n|    explained_variance | 0.154    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10999    |\n|    policy_loss        | -0.481   |\n|    value_loss         | 4.87e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 687      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 11100    |\n|    time_elapsed       | 4726     |\n|    total_timesteps    | 1665000  |\n| train/                |          |\n|    entropy_loss       | -0.0333  |\n|    explained_variance | 0.761    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11099    |\n|    policy_loss        | -0.0623  |\n|    value_loss         | 339      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 651      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 11200    |\n|    time_elapsed       | 4770     |\n|    total_timesteps    | 1680000  |\n| train/                |          |\n|    entropy_loss       | -0.00866 |\n|    explained_variance | 0.852    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11199    |\n|    policy_loss        | 0.0849   |\n|    value_loss         | 879      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 807      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 11300    |\n|    time_elapsed       | 4814     |\n|    total_timesteps    | 1695000  |\n| train/                |          |\n|    entropy_loss       | -0.0281  |\n|    explained_variance | 0.125    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11299    |\n|    policy_loss        | 0.11     |\n|    value_loss         | 2.54e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 236      |\n| time/                 |          |\n|    fps                | 352      |\n|    iterations         | 11400    |\n|    time_elapsed       | 4857     |\n|    total_timesteps    | 1710000  |\n| train/                |          |\n|    entropy_loss       | -0.0791  |\n|    explained_variance | 0.159    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11399    |\n|    policy_loss        | -1.26    |\n|    value_loss         | 2.17e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 553      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 11500    |\n|    time_elapsed       | 4900     |\n|    total_timesteps    | 1725000  |\n| train/                |          |\n|    entropy_loss       | -0.0668  |\n|    explained_variance | 0.113    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11499    |\n|    policy_loss        | -0.379   |\n|    value_loss         | 1.85e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 730      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 11600    |\n|    time_elapsed       | 4945     |\n|    total_timesteps    | 1740000  |\n| train/                |          |\n|    entropy_loss       | -0.03    |\n|    explained_variance | 0.875    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11599    |\n|    policy_loss        | -0.698   |\n|    value_loss         | 826      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 624      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 11700    |\n|    time_elapsed       | 4988     |\n|    total_timesteps    | 1755000  |\n| train/                |          |\n|    entropy_loss       | -0.0357  |\n|    explained_variance | 0.0659   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11699    |\n|    policy_loss        | -0.0365  |\n|    value_loss         | 6.36e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 738      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 11800    |\n|    time_elapsed       | 5033     |\n|    total_timesteps    | 1770000  |\n| train/                |          |\n|    entropy_loss       | -0.0362  |\n|    explained_variance | 0.223    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11799    |\n|    policy_loss        | -0.162   |\n|    value_loss         | 3.73e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 708      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 11900    |\n|    time_elapsed       | 5076     |\n|    total_timesteps    | 1785000  |\n| train/                |          |\n|    entropy_loss       | -0.0302  |\n|    explained_variance | 0.0161   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11899    |\n|    policy_loss        | 0.404    |\n|    value_loss         | 4.1e+04  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 868      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 12000    |\n|    time_elapsed       | 5121     |\n|    total_timesteps    | 1800000  |\n| train/                |          |\n|    entropy_loss       | -0.0271  |\n|    explained_variance | 0.372    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11999    |\n|    policy_loss        | -0.451   |\n|    value_loss         | 2.18e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 77.7     |\n|    ep_rew_mean        | 137      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 12100    |\n|    time_elapsed       | 5166     |\n|    total_timesteps    | 1815000  |\n| train/                |          |\n|    entropy_loss       | -0.0528  |\n|    explained_variance | 0.026    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12099    |\n|    policy_loss        | -0.437   |\n|    value_loss         | 4.83e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 792      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 12200    |\n|    time_elapsed       | 5210     |\n|    total_timesteps    | 1830000  |\n| train/                |          |\n|    entropy_loss       | -0.0217  |\n|    explained_variance | 0.351    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12199    |\n|    policy_loss        | 9.03     |\n|    value_loss         | 3.04e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 95.1     |\n|    ep_rew_mean        | 391      |\n| time/                 |          |\n|    fps                | 351      |\n|    iterations         | 12300    |\n|    time_elapsed       | 5256     |\n|    total_timesteps    | 1845000  |\n| train/                |          |\n|    entropy_loss       | -0.109   |\n|    explained_variance | 0.0526   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12299    |\n|    policy_loss        | 0.356    |\n|    value_loss         | 6.81e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 95.1     |\n|    ep_rew_mean        | 500      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 12400    |\n|    time_elapsed       | 5300     |\n|    total_timesteps    | 1860000  |\n| train/                |          |\n|    entropy_loss       | -0.0282  |\n|    explained_variance | 0.857    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12399    |\n|    policy_loss        | -0.298   |\n|    value_loss         | 1.18e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 97.8     |\n|    ep_rew_mean        | 587      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 12500    |\n|    time_elapsed       | 5345     |\n|    total_timesteps    | 1875000  |\n| train/                |          |\n|    entropy_loss       | -0.0235  |\n|    explained_variance | 0.0205   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12499    |\n|    policy_loss        | 0.248    |\n|    value_loss         | 5.77e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 827      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 12600    |\n|    time_elapsed       | 5389     |\n|    total_timesteps    | 1890000  |\n| train/                |          |\n|    entropy_loss       | -0.036   |\n|    explained_variance | -0.0267  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12599    |\n|    policy_loss        | -0.0812  |\n|    value_loss         | 3.2e+04  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 765      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 12700    |\n|    time_elapsed       | 5434     |\n|    total_timesteps    | 1905000  |\n| train/                |          |\n|    entropy_loss       | -0.0168  |\n|    explained_variance | 0.357    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12699    |\n|    policy_loss        | -0.194   |\n|    value_loss         | 3.16e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 96.8     |\n|    ep_rew_mean        | 245      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 12800    |\n|    time_elapsed       | 5478     |\n|    total_timesteps    | 1920000  |\n| train/                |          |\n|    entropy_loss       | -0.0974  |\n|    explained_variance | 0.745    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12799    |\n|    policy_loss        | 0.0491   |\n|    value_loss         | 38.5     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 99.5     |\n|    ep_rew_mean        | 329      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 12900    |\n|    time_elapsed       | 5521     |\n|    total_timesteps    | 1935000  |\n| train/                |          |\n|    entropy_loss       | -0.0538  |\n|    explained_variance | 0.923    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12899    |\n|    policy_loss        | -0.0154  |\n|    value_loss         | 120      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 603      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 13000    |\n|    time_elapsed       | 5565     |\n|    total_timesteps    | 1950000  |\n| train/                |          |\n|    entropy_loss       | -0.0352  |\n|    explained_variance | 0.867    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12999    |\n|    policy_loss        | -0.544   |\n|    value_loss         | 1.6e+03  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 803      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 13100    |\n|    time_elapsed       | 5609     |\n|    total_timesteps    | 1965000  |\n| train/                |          |\n|    entropy_loss       | -0.0207  |\n|    explained_variance | 0.3      |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13099    |\n|    policy_loss        | -0.578   |\n|    value_loss         | 5.05e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 97.2     |\n|    ep_rew_mean        | 769      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 13200    |\n|    time_elapsed       | 5654     |\n|    total_timesteps    | 1980000  |\n| train/                |          |\n|    entropy_loss       | -0.016   |\n|    explained_variance | 0.912    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13199    |\n|    policy_loss        | -0.479   |\n|    value_loss         | 2.61e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 651      |\n| time/                 |          |\n|    fps                | 350      |\n|    iterations         | 13300    |\n|    time_elapsed       | 5698     |\n|    total_timesteps    | 1995000  |\n| train/                |          |\n|    entropy_loss       | -0.0312  |\n|    explained_variance | 0.176    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13299    |\n|    policy_loss        | -0.11    |\n|    value_loss         | 2.55e+04 |\n------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<H2>Test our model with 1000 steps and record all plays.</H2>","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Test the trained agent\n# using the vecenv\nobs = vec_env.reset()\ntest_steps = 3000\n\nreplay_folder = './replay'\nif os.path.exists(replay_folder):\n    shutil.rmtree(replay_folder)\n\nn_env = obs.shape[0] # Number of environments. A2C will play all envs\nep_id = np.zeros(n_env, int)\nep_steps = np.zeros(n_env, int)\ncum_reward = np.zeros(n_env)\nmax_reward = -1e10\nmax_game_id = 0\nmax_ep_id = 0\nmax_rm_lines = 0\nmax_lifetime = 0\n\nfor step in range(test_steps):\n    action, _ = model.predict(obs, deterministic=True)\n    obs, reward, done, info = vec_env.step(action)\n    \n    if step % 20 == 0:\n        print(f\"Step {step}\")\n        print(\"Action: \", action)\n        print(\"reward=\", reward, \" done=\", done)\n        \n    for eID in range(n_env):\n        cum_reward[eID] += reward[eID]\n        folder = f'{replay_folder}/{eID}/{ep_id[eID]}'\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n        fname = folder + '/' + '{:06d}'.format(ep_steps[eID]) + '.png'\n        cv2.imwrite(fname, obs[eID])\n        #cv2.imshow(\"Image\" + str(eID), obs[eID])\n        #cv2.waitKey(10)\n        ep_steps[eID] += 1\n        \n        if done[eID]:\n            if cum_reward[eID] > max_reward:\n                max_reward = cum_reward[eID]\n                max_game_id = eID\n                max_ep_id = ep_id[eID]\n                max_rm_lines = info[eID]['removed_lines']\n                max_lifetime = info[eID]['lifetime']\n                \n            ep_id[eID] += 1\n            cum_reward[eID] = 0\n            ep_steps[eID] = 0\n\n#cv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2023-12-30T05:36:12.141731Z","iopub.execute_input":"2023-12-30T05:36:12.142037Z","iopub.status.idle":"2023-12-30T05:40:20.246335Z","shell.execute_reply.started":"2023-12-30T05:36:12.142011Z","shell.execute_reply":"2023-12-30T05:40:20.245373Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Step 0\nAction:  [3 3 3 3 0 3 3 3 0 0 3 3 3 3 0 3 3 3 0 3 3 0 0 0 3 0 3 3 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 20\nAction:  [0 4 0 0 4 0 3 0 0 4 0 1 0 0 0 0 0 0 4 1 3 0 4 3 0 0 0 0 0 0]\nreward= [   0.    0.    0.    0. 1066.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.    5.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 40\nAction:  [4 1 3 0 4 0 0 4 0 0 0 0 4 0 4 0 4 0 3 4 1 0 0 0 0 3 0 4 4 0]\nreward= [   5.    0.    0.    0.    0.    0.    0.    5.    0.    0.    0.    0.\n    5.    0.   -5.    0.    5.    0.    0. 1046.    0.    0.    0.    0.\n    0.    0.    0.    0.    5.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 60\nAction:  [4 4 3 0 4 0 4 0 0 4 4 4 4 1 0 4 0 1 0 0 0 4 4 4 1 0 0 0 1 1]\nreward= [  5. -10.   0.   0.  -5.   0.   5.   0.   0.   5.   5.   5.   5.   0.\n   0.   5.   0.   0.   0.   0.   0.   5.   5.   0.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 80\nAction:  [1 4 4 0 0 1 4 3 4 0 0 4 4 0 4 1 0 4 0 1 4 1 1 3 1 4 0 3 0 4]\nreward= [ 0.  5.  5.  0.  0.  0.  5.  0.  5.  0.  0.  0. -5.  0.  5.  0.  0.  0.\n  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.  0.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 100\nAction:  [4 0 0 4 0 3 4 3 0 0 1 0 4 0 0 4 0 4 3 0 0 4 4 3 0 0 0 4 1 0]\nreward= [ 5.  0.  0.  5.  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  5.\n  0.  0.  0.  5. -5.  0.  0.  0.  0.  5.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 120\nAction:  [4 3 0 3 0 4 1 0 0 4 1 3 3 0 4 4 0 4 0 3 1 4 1 4 0 0 0 0 1 0]\nreward= [ 0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.  0. -5.  5.  0.  5.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 140\nAction:  [0 0 3 0 0 4 0 3 4 3 4 0 0 0 0 3 4 0 0 1 1 0 0 4 4 4 3 4 4 1]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   0.   0.\n   0.   0. -10.   0.   0.   0.   0.   0.   0.   0.   5.   5.   0.  -5.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 160\nAction:  [0 0 0 0 1 3 0 3 1 0 4 0 3 0 0 0 0 0 0 1 4 0 0 0 0 4 0 3 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 5. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 180\nAction:  [0 4 1 0 0 4 3 1 0 3 3 4 0 1 0 0 0 3 1 0 0 1 0 4 4 0 0 3 3 3]\nreward= [  0.   0.   0.   0.   0. -10.   0.   0.   0.   0.   0.   5.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   5.   0.   0.   0.\n   0.   0.]  done= [False False False False False  True False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 200\nAction:  [4 4 4 4 3 0 0 4 0 4 0 0 0 4 4 4 0 0 0 1 0 3 0 4 0 0 0 0 3 0]\nreward= [  0.   5.   0.   0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   5.\n   5.   0.   0.   0.   0.   0.  -5.   0.   0. -10.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n  True False False False False False]\nStep 220\nAction:  [0 0 0 0 0 3 4 0 3 0 0 0 4 4 0 0 0 1 1 0 0 0 4 1 1 0 4 0 4 4]\nreward= [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n   -5. 1026.    0.    0.    0.    0.    0.    0.    0.    0.    5.    0.\n    0.    0.    5.    0.    5.   -5.]  done= [False False False False False False  True False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 240\nAction:  [1 0 4 0 0 0 0 3 4 0 1 0 4 0 0 3 1 1 0 0 0 1 4 4 0 0 3 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 5.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 260\nAction:  [0 0 4 0 0 4 4 0 0 1 0 0 0 4 1 3 0 0 4 1 0 0 0 3 4 4 4 4 4 0]\nreward= [0. 0. 5. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0.\n 5. 5. 5. 5. 5. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 280\nAction:  [0 4 0 0 0 0 4 0 0 4 4 4 0 0 4 0 3 1 1 4 0 4 0 0 0 3 1 1 0 3]\nreward= [  0.   0.   0.   0.   0.   0.   5.   0.   0.   5. -10.   5.   0.   0.\n   5.   0.   0.   0.   0.   5.   0.   5.   0.   0.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 300\nAction:  [4 4 4 4 4 4 1 4 4 1 0 4 0 0 0 0 0 0 3 4 4 3 1 4 4 0 0 4 0 0]\nreward= [ 5.  0.  0.  5.  5. -5.  0.  5.  5.  0.  0.  5.  0.  0.  0.  0.  0.  0.\n  0.  0. -5.  0.  0.  5.  5.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 320\nAction:  [3 0 0 0 3 0 0 0 0 0 4 0 0 0 0 3 4 4 1 0 3 0 0 0 1 1 4 3 0 4]\nreward= [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    5.    0.\n    0.    0.    0.    0.    0. 1006.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 340\nAction:  [3 1 1 4 4 3 1 3 0 0 0 4 0 0 4 0 0 4 0 4 0 1 0 0 0 0 3 4 3 0]\nreward= [ 0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  5.  0.  0.  5.  0.  0.  5.\n  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False  True False False False False\n False False False  True False False]\nStep 360\nAction:  [0 3 4 0 1 4 1 0 1 0 0 1 3 0 3 4 4 4 4 0 0 0 0 4 3 4 0 0 0 0]\nreward= [ 0.  0.  5.  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.  5.\n  5.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 380\nAction:  [0 0 0 0 4 0 4 0 0 3 4 0 4 0 1 4 4 4 0 0 0 4 4 4 3 4 4 4 0 0]\nreward= [  0.   0.   0.   0.   0.   0.  -5.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   5.   5.   0.   0.   0.   5.   5. -10.   0.  -5.   5. -10.\n   0.   0.]  done= [False False False False False False False False False False  True False\n False False False False False False False False False False False False\n False  True False False False False]\nStep 400\nAction:  [0 3 1 4 4 0 4 4 3 0 0 3 0 3 1 0 0 0 0 3 0 0 0 4 0 0 3 0 0 0]\nreward= [ 0.  0.  0.  5.  5.  0.  5. -5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 420\nAction:  [0 0 1 0 4 0 1 4 0 0 0 0 0 3 0 3 4 0 0 1 0 1 4 3 3 4 0 4 4 0]\nreward= [0. 0. 0. 0. 5. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 5. 0.\n 0. 0. 0. 5. 5. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 440\nAction:  [0 0 4 0 0 3 0 3 0 4 0 0 4 4 0 0 0 0 1 0 0 0 0 4 0 0 0 4 1 4]\nreward= [ 0.  0. -5.  0.  0.  0.  0.  0.  0.  5.  0.  0.  5.  5.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 460\nAction:  [4 0 0 4 0 0 4 0 4 0 1 0 0 3 1 0 0 4 0 1 0 4 3 0 4 0 0 0 0 0]\nreward= [  0.   0.   0.  -5.   0.   0.  -5.   0.   5.   0.   0.   0.   0.   0.\n   0.   0.   0.   5. -15.   0.   0.   5.   0.   0.   5.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 480\nAction:  [0 4 0 3 3 0 0 0 1 0 0 0 0 0 3 0 4 0 0 0 4 1 3 3 0 0 0 0 0 4]\nreward= [0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 5. 0. 0. 0.\n 0. 0. 0. 0. 0. 5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 500\nAction:  [0 0 4 4 4 3 4 4 0 0 4 0 4 3 3 4 1 1 4 4 0 4 0 0 0 0 4 4 1 0]\nreward= [   0.    0.    0.   -5.   -5.    0.    5.    0.    0.    0.    5.    0.\n    5.    0.    0.    5.    0.    0. 1016.    5.    0.   -5.    0.    0.\n    0.    0.   -5.    5.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 520\nAction:  [0 0 0 0 0 0 0 4 4 4 0 4 0 0 0 4 1 0 0 4 0 0 0 4 3 4 4 0 3 1]\nreward= [-15.   0.   0.   0.   0.   0.   0.  -5.  -5.  -5.   0.   5.   0.   0.\n   0.  -5.   0.   0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False  True False False False False False False False False\n False False False False False False]\nStep 540\nAction:  [1 4 3 1 4 0 0 3 1 4 1 3 0 4 0 0 0 0 4 0 4 0 0 0 0 4 0 0 4 0]\nreward= [ 0.  5.  0.  0.  5.  0.  0.  0.  0.  5.  0.  0.  0.  5.  0.  0.  0.  0.\n  0.  0.  5.  0.  0.  0.  0.  5.  0.  0. -5.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 560\nAction:  [0 0 4 0 4 3 0 4 1 3 0 1 0 0 0 0 0 4 4 0 0 3 3 1 4 4 0 3 0 3]\nreward= [   0.    0. 1006.    0.    5.    0.    0.    5.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    5.   -5.    0.    0.    0.    0.    0.\n    0.    5.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 580\nAction:  [0 0 3 0 0 0 0 4 1 0 0 0 4 3 1 4 1 3 4 0 0 4 4 0 3 0 0 1 1 0]\nreward= [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n   -5.    0.    0. 1026.    0.    0.    0.    0.    0.   -5.    5.    0.\n    0.    0.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 600\nAction:  [0 0 0 0 0 4 0 0 3 1 0 4 4 0 0 0 0 0 4 0 0 3 0 4 0 0 0 0 0 3]\nreward= [  0.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   5.   0.\n   0.   0.   0.   0. -10.   0.   0.   0.   0.   5.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 620\nAction:  [0 0 4 0 4 0 0 0 0 0 4 0 0 0 4 0 0 0 0 4 3 1 4 0 1 3 0 4 0 3]\nreward= [0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 5. 0. 0. 5. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 640\nAction:  [0 0 0 1 4 4 0 1 0 0 4 0 4 1 3 0 4 0 0 4 0 0 4 4 1 4 0 1 0 4]\nreward= [ 0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  5.  0.  5.  0.  0.  0. -5.  0.\n  0.  5.  0.  0.  5. -5.  0. -5.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 660\nAction:  [1 4 0 1 3 0 4 4 0 4 0 0 0 0 1 0 4 1 0 4 0 4 4 0 0 0 0 4 0 3]\nreward= [  0.   5.   0.   0.   0.   0.   5.   5.   0. -10.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   5.   0.   5.   0.   0.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 680\nAction:  [3 4 0 1 0 0 0 0 0 3 4 0 4 4 4 4 4 0 3 0 0 0 0 0 0 4 3 0 4 4]\nreward= [   0.    5.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.   -5.    5.    5.    0.    0.    0.    0.    0.    0.    0.\n    0.   -5.    0.    0.   -5. 1046.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False  True False False False False]\nStep 700\nAction:  [0 4 4 4 0 1 0 1 1 0 0 0 4 4 0 0 3 0 0 4 0 0 0 0 3 3 3 0 0 3]\nreward= [ 0. -5.  5.  5.  0.  0.  0.  0.  0.  0.  0.  0. -5.  5.  0.  0.  0.  0.\n  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 720\nAction:  [3 1 4 4 0 4 4 0 0 0 1 0 0 0 3 0 0 0 1 1 0 3 0 0 0 3 4 0 1 0]\nreward= [   0.    0.    5. 1161.    0.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.   -5.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 740\nAction:  [4 0 3 3 4 0 3 0 0 3 0 4 1 3 0 0 0 0 1 0 3 3 3 0 0 0 0 4 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 5. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 760\nAction:  [4 3 0 0 3 0 0 0 0 0 0 4 4 0 0 1 0 0 0 0 0 0 0 0 4 0 0 0 3 0]\nreward= [ 5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.\n  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 780\nAction:  [1 0 4 0 0 1 1 0 0 4 0 1 1 4 4 3 0 0 0 1 1 0 3 0 0 0 3 1 3 0]\nreward= [0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 800\nAction:  [0 0 0 0 0 4 0 4 3 4 3 4 4 0 0 4 0 4 3 3 4 0 0 0 0 0 0 4 0 0]\nreward= [   0.    0.    0.    0.    0. 1006.    0.    5.    0.    5.    0. 1086.\n    5.    0.    0.    5.    0.    5.    0.    0.    5.    0.    0.    0.\n    0.    0.    0. 1026.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 820\nAction:  [4 0 1 4 1 0 0 4 3 3 1 4 0 4 4 4 1 0 1 0 4 4 0 4 4 3 4 3 0 4]\nreward= [  5.   0.   0.   5.   0.   0.   0. -10.   0.   0.   0.   5.   0.   0.\n   5.   0.   0.   0.   0.   0.   0.   0.   0.   5.   5.   0.   5.   0.\n   0.   0.]  done= [False False False False False False False  True False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 840\nAction:  [0 0 4 0 0 0 4 0 0 0 0 1 3 0 1 0 0 0 4 0 4 4 0 0 0 4 0 0 4 0]\nreward= [  0.   0.   5.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0. -10.   0.   5.   5.   0.   0.   0.  -5.   0.   0.\n   5.   0.]  done= [False False False False False False  True False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 860\nAction:  [0 3 4 4 4 1 1 4 4 3 0 0 0 0 4 3 4 4 0 0 4 4 4 1 0 0 0 0 1 0]\nreward= [   0.    0.   -5.    0.   -5.    0.    0.   -5.    0.    0.    0.    0.\n    0.    0.    5.    0.   -5.   -5.    0.    0.    5. 1076.    0.    0.\n    0.    0.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 880\nAction:  [3 0 1 0 4 4 1 3 0 0 0 0 1 0 0 3 4 4 4 0 0 0 4 4 1 4 3 0 4 0]\nreward= [   0.    0.    0.    0.   -5.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.   -5.    0.    0.    0.    0.    0. 1126.    5.\n    0.    5.    0.    0.    5.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 900\nAction:  [0 3 4 0 0 1 4 0 0 0 4 0 3 4 3 0 1 4 0 3 3 4 0 0 0 4 0 4 0 4]\nreward= [  0.   0.   0.   0.   0.   0.   5.   0.   0.   0.  -5.   0.   0.   5.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -10.   0.   5.\n   0.   5.]  done= [False False  True False False False False False False False  True False\n False False False False False False False False False False False False\n False False False False False False]\nStep 920\nAction:  [0 0 0 4 1 0 0 0 0 1 4 0 1 0 0 0 1 1 1 4 0 4 0 4 0 4 4 4 0 4]\nreward= [  0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   5.   0.   0. -10.\n   0.   0.   0.   0.   0.   5.   0.   5.   0.  -5.   0.   5.  -5. -10.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 940\nAction:  [1 3 4 0 0 0 0 1 1 4 3 1 0 4 0 4 4 0 4 0 3 1 0 1 1 4 4 0 0 0]\nreward= [  0.   0.   5.   0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   5.\n   0.   5.   5.   0.  -5.   0.   0.   0.   0.   0.   0. -10.  -5.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 960\nAction:  [4 1 4 3 0 1 0 0 4 4 3 3 3 3 0 4 4 0 4 0 0 0 4 4 4 0 0 4 4 0]\nreward= [ 5.  0.  5.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0. -5. -5.  0.\n -5.  0.  0.  0. -5.  0.  5.  0.  0.  5.  5.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 980\nAction:  [4 0 0 4 4 0 4 0 0 4 4 4 4 0 4 0 4 4 0 0 1 4 0 0 0 4 4 4 0 0]\nreward= [ 5.  0.  0.  0.  0.  0.  5.  0.  0.  5. -5. -5.  5.  0.  5.  0.  5.  5.\n  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False  True False False False False False False False\n False False False False False False]\nStep 1000\nAction:  [0 3 0 0 0 0 0 1 0 4 0 4 0 0 4 4 3 0 4 0 4 1 0 0 0 0 0 0 1 1]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0. -5.  0.  0.  5.  5.  0.  0.\n  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1020\nAction:  [0 4 4 3 1 3 0 4 4 0 3 4 1 0 0 4 0 0 4 4 1 1 0 0 4 4 0 0 0 0]\nreward= [ 0.  5.  5.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.\n  0.  5.  0.  0.  0.  0. -5.  5.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False  True False False False False False\n False False False False False False]\nStep 1040\nAction:  [0 4 0 0 3 0 4 0 0 0 0 3 4 1 0 0 0 4 0 3 4 1 0 0 0 0 0 4 3 4]\nreward= [ 0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  5.\n  0.  0. -5.  0.  0.  0.  0.  0.  0.  5.  0.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1060\nAction:  [4 0 4 0 1 3 4 4 1 0 4 0 0 4 3 0 3 1 0 4 1 1 4 4 1 4 0 0 3 3]\nreward= [   5.    0.    0.    0.    0.    0. 1026.    0.    0.    0.    0.    0.\n    0.    5.    0.    0.    0.    0.    0.    0.    0.    0.   -5.   -5.\n    0.    5.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1080\nAction:  [3 0 1 4 4 0 0 0 4 3 0 0 0 0 4 4 0 4 0 0 0 0 0 0 1 4 0 4 1 4]\nreward= [  0.   0.   0.  -5.   5.   0.   0.   0.  -5.   0.   0.   0.   0.   0.\n   5. -10.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   5.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1100\nAction:  [4 0 0 0 0 0 0 0 3 0 0 4 0 4 4 0 0 0 0 1 0 0 0 0 4 4 0 3 1 1]\nreward= [-5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  5.  5.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  5.  5.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1120\nAction:  [0 0 4 1 4 4 4 4 0 1 0 0 0 3 0 1 0 4 0 0 1 4 0 1 0 1 4 0 3 0]\nreward= [ 0.  0.  5.  0.  0.  5.  0. -5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.\n  0.  0.  0.  5.  0.  0.  0.  0.  5.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1140\nAction:  [4 1 4 0 4 1 4 0 0 4 0 1 0 1 0 0 0 4 0 0 0 4 1 0 0 4 4 4 0 4]\nreward= [ 5.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  5.  0.  0.  0. -5.  0.  5.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1160\nAction:  [0 0 0 0 0 0 4 3 0 0 1 0 0 0 0 1 0 0 4 3 0 0 0 3 0 0 0 0 1 0]\nreward= [  0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   0. -10.   0.   0.\n   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1180\nAction:  [1 4 0 0 4 3 4 3 1 0 4 3 0 1 3 0 1 0 4 0 4 0 0 3 0 4 0 0 0 0]\nreward= [   0.    5.    0.  -10.  -10.    0.    5.    0.    0.    0.   -5.    0.\n    0.    0.    0.    0.    0.    0. 1026.    0.    0.    0.   -5.    0.\n    0.    5.    0.    0.    0.  -15.]  done= [False False False False False False False False False False  True False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1200\nAction:  [4 0 0 0 0 4 0 0 0 0 0 0 3 0 0 4 1 0 0 0 4 3 4 0 4 0 1 1 4 4]\nreward= [5. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 5. 0.\n 5. 0. 0. 0. 5. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1220\nAction:  [1 1 1 4 0 0 0 1 4 3 0 1 4 0 0 4 4 4 4 1 1 3 1 0 0 1 4 0 4 0]\nreward= [ 0.  0.  0.  5.  0.  0.  0.  0.  5.  0.  0.  0.  5.  0.  0.  5.  5.  5.\n -5.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False  True False False False False False\n False False False False False False]\nStep 1240\nAction:  [0 0 0 3 4 0 4 0 0 0 1 0 4 0 0 0 0 0 4 0 0 0 0 0 1 0 1 0 4 0]\nreward= [ 0.  0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  5. -5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1260\nAction:  [0 4 4 4 4 0 4 0 0 0 0 0 0 0 0 0 3 0 3 3 0 1 0 1 0 1 0 0 1 1]\nreward= [ 0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1280\nAction:  [4 0 0 0 0 0 4 3 0 0 4 3 1 0 0 0 0 4 0 0 0 4 0 3 4 4 1 0 4 0]\nreward= [ 5.  0.  0.  0.  0.  0.  5.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.  5.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1300\nAction:  [3 0 1 0 4 1 0 0 4 4 0 0 0 0 0 0 4 0 4 0 4 0 4 4 4 4 3 0 4 0]\nreward= [ 0.  0.  0.  0. -5.  0.  0.  0. -5.  5.  0.  0.  0.  0.  0.  0.  0.  0.\n  5.  0.  0.  0.  5.  5.  5.  5.  0.  0.  5.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1320\nAction:  [0 1 4 0 3 4 0 0 0 0 0 1 4 4 0 0 0 1 4 4 4 4 4 0 0 4 0 4 0 4]\nreward= [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n   -5.    0.    0.    0.    0.    0.    5.    5.    5.   -5.    5.    0.\n    0.   -5.    0.  -10.    0. 1026.]  done= [False False False False False  True False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1340\nAction:  [0 4 1 1 4 0 0 0 0 4 3 1 1 0 4 3 0 1 0 1 1 4 4 0 4 1 1 1 0 0]\nreward= [ 0. -5.  0.  0.  5.  0.  0.  0.  0.  5.  0.  0.  0.  0.  5.  0.  0.  0.\n  0.  0.  0.  5.  5.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n  True False False False False False]\nStep 1360\nAction:  [3 0 4 0 4 4 0 0 1 0 4 4 0 0 0 0 0 3 4 0 4 0 3 1 4 3 0 0 0 0]\nreward= [ 0.  0.  0.  0.  5.  5.  0.  0.  0.  0. -5.  5.  0.  0.  0.  0.  0.  0.\n  5.  0.  5.  0.  0.  0. -5.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1380\nAction:  [4 4 0 1 0 0 0 1 0 0 4 4 0 4 0 4 4 4 1 0 0 0 0 0 0 0 0 0 1 0]\nreward= [ -5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   0.   0.\n   0.   5.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0. -10.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1400\nAction:  [0 0 4 0 0 3 0 0 0 1 4 0 0 0 0 0 4 3 3 0 4 0 4 4 0 0 0 4 0 0]\nreward= [ 0.  0. -5.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  5.  0.\n  0.  0.  5.  0.  0.  5.  0.  0.  0.  5.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1420\nAction:  [0 0 0 1 0 0 0 4 3 3 4 0 0 0 0 4 4 4 3 3 0 4 4 4 4 0 0 1 4 4]\nreward= [  0.   0.   0.   0.   0.   0.   0.   5.   0.   0. -10.   0.   0.   0.\n   0.   5.  -5.   5.   0.   0.   0.   5.   5.  -5.   0. -10.   0.   0.\n   5.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1440\nAction:  [0 4 4 1 4 4 0 3 0 0 4 4 0 4 3 0 0 0 4 4 4 1 0 0 0 1 4 4 0 4]\nreward= [  0. -10.   5.   0.  -5.   5.   0.   0.   0.   0.  -5.   0.   0.   5.\n   0.   0.   0.   0.   5.   0.  -5.   0.   0.   0.   0.   0.   5.  -5.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1460\nAction:  [0 0 4 0 0 0 0 0 1 4 4 0 3 4 0 4 4 0 4 3 1 3 4 4 0 0 3 4 0 4]\nreward= [   0.    0.    0.    0.    0.    0.    0.    0.    0.    5.    5.    0.\n    0. 1006.    0.    5.    5.    0.    5.    0.    0.    0.    5.    5.\n    0.    0.    0.   -5.    0.    5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1480\nAction:  [0 3 4 0 3 0 0 4 4 0 4 0 0 0 0 4 0 0 4 4 0 4 0 3 0 1 0 4 0 4]\nreward= [   0.    0.    5.    0.    0.    0.    0.    5.    5.    0.    5.    0.\n    0.    0.    0.   -5.    0.    0.    5.    0.    0. 1006.    0.    0.\n    0.    0.    0.    5.    0.    5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False  True False False]\nStep 1500\nAction:  [4 3 0 0 4 4 0 4 0 1 0 4 1 0 0 4 1 0 0 0 4 1 0 0 0 4 1 0 0 0]\nreward= [5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0.\n 0. 5. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1520\nAction:  [0 3 1 4 4 4 4 4 0 0 1 4 0 0 4 3 0 0 1 0 0 0 0 0 0 0 3 0 0 0]\nreward= [  0.   0.   0.   0.   5.   0.   0.   5.   0.   0.   0.  -5.   0.   0.\n -10.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False  True False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1540\nAction:  [0 4 0 1 4 0 0 4 3 0 0 0 0 0 0 4 4 0 4 0 0 0 3 3 0 4 0 4 0 4]\nreward= [0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 5. 0. 5. 0. 0. 0. 0. 0.\n 0. 5. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1560\nAction:  [0 3 3 0 0 1 0 0 3 0 0 0 4 3 4 4 0 0 0 4 4 1 4 0 4 0 4 4 4 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  5.  0.  0.\n  0.  5.  0.  0.  0.  0.  5.  0. -5. -5. -5.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1580\nAction:  [0 4 0 3 3 4 0 4 4 4 0 3 4 4 0 0 3 0 0 0 0 0 0 4 1 0 1 0 0 4]\nreward= [ 0.  5.  0.  0.  0.  5.  0.  5.  5.  5.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1600\nAction:  [0 3 0 0 0 3 0 4 0 4 0 0 4 0 0 4 0 4 0 0 1 0 0 4 0 0 0 0 4 1]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  5.  0.  0.  5.  0. -5.\n  0.  0.  0.  0.  0.  5.  0.  0.  0.  0. -5.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1620\nAction:  [0 1 1 3 0 4 0 4 0 0 0 4 4 3 0 0 0 3 0 0 0 0 3 0 0 3 4 0 3 0]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5. -10.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.\n   0.   0.]  done= [False False False False False False False False False False False  True\n False False False False False False False False False False False False\n False False False False False False]\nStep 1640\nAction:  [1 0 0 4 0 1 1 1 4 4 0 4 3 4 4 4 4 0 3 4 1 0 3 0 0 4 1 0 0 0]\nreward= [  0.   0.   0.   5.   0.   0.   0.   0.   5.   0.   0.   0.   0.   5.\n  -5. -10.  -5.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1660\nAction:  [0 4 0 4 1 4 3 3 4 0 0 0 0 4 0 0 0 4 0 0 0 4 4 4 0 4 4 0 3 0]\nreward= [   0.    0.    0.   -5.    0.   -5.    0.    0.    5.    0.    0.    0.\n    0.    5.    0.    0.    0.    0.    0.    0.    0.    5.    5.    5.\n    0. 1086.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1680\nAction:  [4 0 0 0 0 0 0 0 4 1 4 0 4 0 0 0 0 1 0 0 4 4 3 1 4 4 0 1 3 3]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -10.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   5.   5.   0.   0.  -5.   5.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1700\nAction:  [4 4 4 4 0 3 3 0 0 0 0 0 0 3 3 4 1 3 4 0 4 1 0 0 0 0 1 0 0 0]\nreward= [5. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 5. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1720\nAction:  [4 4 3 0 3 0 4 0 0 0 0 0 0 3 0 3 4 3 1 4 4 3 1 1 0 3 0 4 3 4]\nreward= [  0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.  -5.   0.   0. -10.  -5.   0.   0.   0.   0.   0.   0.  -5.\n   0.   5.]  done= [False False False False False False False False False False False False\n False False False False  True False False False False False False False\n False False False False False False]\nStep 1740\nAction:  [0 4 3 4 4 0 0 0 1 4 0 4 0 0 0 4 4 4 1 3 4 3 0 4 0 0 3 4 0 0]\nreward= [ 0.  0.  0.  0.  5.  0.  0.  0.  0.  5.  0. -5.  0.  0.  0.  0.  5.  5.\n  0.  0.  5.  0.  0.  0.  0.  0.  0. -5.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1760\nAction:  [0 1 0 0 0 4 0 0 0 0 4 0 3 0 1 1 0 0 1 1 0 0 3 0 3 4 3 0 3 0]\nreward= [   0.    0.    0.    0.    0.    5.    0.    0.    0.    0.    5.    0.\n    0.    0.    0.    0.    0.    0.    0.    0.    0.  -10.    0.    0.\n    0. 1006.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1780\nAction:  [0 4 0 0 3 4 0 4 3 4 0 0 3 4 4 0 0 0 3 0 0 4 4 4 0 0 1 1 4 0]\nreward= [  0.   5.   0.   0.   0.   0.   0. -10.   0.   5.   0.   0.   0.   0.\n   5.   0.   0.   0.   0.   0.   0.   5.   5.   5.   0.   0.   0.   0.\n   5.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1800\nAction:  [4 4 0 4 4 4 0 4 4 0 4 4 4 0 3 4 0 3 3 4 0 3 4 0 0 3 4 4 1 0]\nreward= [  0.  -5.   0.   5.   5.   0.   0. -10.   5.   0.  -5.   5.   0.   0.\n   0.   0.   0.   0.   0.   5.   0.   0.  -5.   0.   0.   0.   5.   5.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1820\nAction:  [1 4 3 3 0 0 0 0 4 3 3 0 4 4 4 4 4 0 0 0 0 3 4 4 1 0 4 0 3 4]\nreward= [ 0. -5.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  5.  5.  5.  5. -5.  0.\n  0.  0.  0.  0.  0.  5.  0.  0.  5.  0.  0. -5.]  done= [False  True False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1840\nAction:  [3 3 0 4 0 3 4 4 4 0 0 0 4 1 0 0 4 0 0 1 4 0 4 3 0 4 0 0 0 4]\nreward= [   0.    0.    0.    5.    0.    0.    5.    0.    5.    0.    0.    0.\n    5.    0.    0.    0. 1076.    0.    0.    0.    5.    0.    0.    0.\n    0.    5.    0.    0.    0.    5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1860\nAction:  [3 4 0 4 4 0 0 4 4 1 0 4 0 0 3 4 4 4 1 4 3 3 4 4 0 4 0 0 4 4]\nreward= [ 0.  5.  0.  0.  0.  0.  0.  5.  5.  0.  0.  5.  0.  0.  0. -5.  5.  5.\n  0.  5.  0.  0.  5.  0.  0.  0.  0.  0.  5.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1880\nAction:  [4 0 4 0 0 1 0 1 0 0 4 4 0 0 0 0 4 3 0 0 0 0 4 3 0 0 4 4 4 0]\nreward= [ 5.  0.  5.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  5.  0.  0.  0.  5. -5.  5.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1900\nAction:  [0 0 4 4 3 1 3 1 4 3 0 0 4 4 0 0 0 0 0 0 0 4 0 0 0 0 0 1 1 4]\nreward= [  0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   5.   0.   0. -15.   0.   0.   0.\n   0.   5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1920\nAction:  [0 0 0 3 0 3 0 3 0 4 0 0 4 4 0 3 4 0 0 4 4 1 0 0 0 4 4 4 0 4]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0. -5.  5.  0.  0.  5.  0.\n  0.  5.  5.  0.  0.  0.  0.  5.  5.  0.  0. -5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1940\nAction:  [1 0 0 4 0 0 0 0 0 4 4 4 0 4 0 4 0 4 4 0 4 4 0 4 4 0 3 4 0 4]\nreward= [   0.    0.    0.    5.    0.    0.    0.    0.    0.  -10.    0.    5.\n    0.    5.    0.    5.    0.    5.    5.    0. 1066.    5.    0.   -5.\n    5.    0.    0.    5.    0.    5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 1960\nAction:  [0 1 0 0 4 0 0 3 0 3 3 4 0 3 0 0 0 4 0 1 4 0 0 0 1 0 0 0 4 4]\nreward= [0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 5. 5.]  done= [False False False False False False False False False False False False\n False False False False False  True False False False False False False\n False False False False False False]\nStep 1980\nAction:  [3 0 3 3 1 3 4 4 0 4 0 0 3 1 0 1 4 0 0 4 3 4 0 0 0 0 0 1 4 0]\nreward= [  0.   0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.\n   0.   0.   5.   0.   0.   0.   0. -10.   0.   0.   0.   0.   0.   0.\n  -5.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2000\nAction:  [4 1 1 0 0 4 4 1 0 4 1 0 0 1 0 0 1 0 0 0 1 0 0 0 4 4 4 0 0 4]\nreward= [ -5.   0.   0.   0.   0.   5. -10.   0.   0.   5.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   5.   5.   0.\n   0.   5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2020\nAction:  [4 4 4 3 3 1 1 1 0 0 3 4 0 3 4 4 4 0 0 3 0 0 1 0 4 1 0 0 3 3]\nreward= [0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 5. 5. 0. 0. 0. 0. 0. 0. 0.\n 5. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False  True False False False False False False False False False\n False False False False False False]\nStep 2040\nAction:  [0 3 0 0 0 3 4 0 0 0 1 0 4 0 0 3 3 0 0 0 4 3 3 1 4 4 0 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0.\n 5. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2060\nAction:  [3 4 0 0 0 3 0 4 4 4 0 0 0 0 0 4 0 4 0 0 4 0 4 1 0 4 0 0 0 3]\nreward= [  0.  -5.   0.   0.   0.   0.   0.   5.   5.   5.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0. -15.   0.   5.   0.   0.   5.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2080\nAction:  [4 3 0 0 0 3 4 3 0 4 0 1 0 4 4 0 0 4 0 4 1 3 0 0 4 0 4 0 4 4]\nreward= [ 5.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  0.  5.  5.  0.  0. -5.\n  0.  5.  0.  0.  0.  0. -5.  0.  5.  0. -5.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2100\nAction:  [3 0 3 0 1 1 0 3 4 0 4 4 1 4 0 0 0 4 0 0 4 4 3 3 0 0 1 3 1 0]\nreward= [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  -10.   -5.\n    0.    5.    0.    0.    0. 1036.    0.    0.    0.    5.    0.    0.\n    0.    0.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2120\nAction:  [4 4 3 4 4 1 3 3 4 1 1 4 4 0 4 4 1 1 4 1 0 4 3 3 0 0 0 0 0 0]\nreward= [   5.    5.    0.    5.    5.    0.    0.    0. 1006.    0.    0.   -5.\n    5.    0.  -10.    5.    0.    0.   -5.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False  True False False False False False False False False False\n False False False False False False]\nStep 2140\nAction:  [0 0 0 1 3 3 1 0 0 4 0 0 3 4 0 4 3 1 4 0 1 0 4 0 4 3 0 1 1 4]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0. -10.   0.   0.   0.   5.\n   0.   5.   0.   0.  -5.   0.   0.   0.   5.   0.   0.   0.   0.   0.\n   0.  -5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2160\nAction:  [4 1 1 0 1 0 0 1 0 0 0 4 4 0 1 1 1 0 0 4 3 3 0 0 0 0 0 0 0 0]\nreward= [ 5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5. -5.  0.  0.  0.  0.  0.\n  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2180\nAction:  [4 0 0 0 1 3 4 0 4 1 1 1 0 0 0 0 1 1 0 0 0 0 4 0 0 4 1 0 3 1]\nreward= [  0.   0.   0. -15.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2200\nAction:  [1 0 0 0 0 4 4 0 4 4 0 0 0 1 3 4 0 4 0 0 4 0 4 0 0 0 1 0 1 4]\nreward= [ 0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  5.\n  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.  0.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2220\nAction:  [0 4 0 0 0 0 4 0 0 4 0 0 0 0 4 0 1 4 3 0 0 0 3 1 0 3 0 4 1 3]\nreward= [ 0.  5.  0.  0.  0. -5.  5.  0.  0. -5.  0.  0.  0.  0.  5.  0.  0.  5.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2240\nAction:  [0 0 0 3 0 0 4 1 0 4 0 1 4 0 1 4 4 4 0 4 0 3 3 0 0 0 4 0 4 1]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   0. -10.   0.\n   0.   5.   5.   5.   0. -10.   0.   0.   0.   0.   0.   0.  -5.   0.\n   5.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False  True False False False]\nStep 2260\nAction:  [0 0 1 0 1 1 1 4 0 3 3 4 4 4 0 4 0 4 4 0 0 0 4 1 0 0 0 0 3 4]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0. -5.  0.  0.  0.  5.  0. -5.\n  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2280\nAction:  [0 4 4 4 0 1 4 0 4 3 1 0 0 4 1 1 0 0 0 0 0 1 4 3 4 4 1 0 0 0]\nreward= [  0.   5.   5.  -5.   0.   0.  -5.   0.  -5.   0.   0.   0.   0. -10.\n   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   5.   5.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2300\nAction:  [4 0 4 0 0 0 0 0 4 4 3 0 0 0 0 4 0 0 4 4 0 3 3 3 0 0 0 0 4 4]\nreward= [  -5.    0.    5.    0.    0.    0.    0.    0. 1016.    5.    0.    0.\n    0.    0.    0.    0.    0.    0.    5.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    5.   -5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2320\nAction:  [0 4 0 0 0 3 1 1 4 4 4 0 4 4 4 0 4 4 4 0 3 4 1 0 4 0 0 4 3 0]\nreward= [ 0.  5.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  5. -5.  5.  0.  5.  5.\n -5.  0.  0.  5.  0.  0.  5.  0.  0.  5.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False  True False False False False False False False\n False False False False False False]\nStep 2340\nAction:  [4 0 3 1 0 4 0 3 3 4 0 0 0 0 0 0 1 0 3 0 0 0 3 4 4 4 0 0 3 3]\nreward= [  5. -10.   0.   0.   0.   5.   0.   0.   0.   5.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   5. -10.  -5.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2360\nAction:  [4 0 0 4 1 1 4 0 0 0 1 0 0 0 0 0 1 0 3 3 3 0 3 4 4 1 0 0 0 0]\nreward= [-10.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   5.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2380\nAction:  [4 4 3 4 3 4 3 4 0 4 3 0 0 1 0 4 0 1 4 0 0 0 3 0 0 0 0 0 0 0]\nreward= [   5. 1006.    0. 1006.    0.   -5.    0.   -5.    0.    5.    0.    0.\n    0.    0.    0.    5.    0.    0.    5.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.]  done= [False False False False False False False  True False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2400\nAction:  [0 4 1 0 0 0 1 0 0 0 0 0 3 0 4 0 4 3 4 0 1 4 4 0 0 1 4 4 4 4]\nreward= [  0.  -5.   0.   0.   0. -10.   0.   0.   0.   0.   0.   0.   0.   0.\n   5.   0.   5.   0.  -5.   0.   0.   5.  -5.   0.   0.   0.   5.   5.\n -10.   5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2420\nAction:  [0 4 1 1 0 0 4 0 4 4 0 1 4 4 0 0 4 0 1 4 4 0 4 0 0 4 0 0 3 4]\nreward= [  0.   5.   0.   0.   0.   0.   5.   0.   5.  -5. -10.   0.   5.   0.\n   0. -10.   5.   0.   0.   5.   0.   0.  -5.   0.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False  True False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2440\nAction:  [0 0 0 0 4 1 3 1 0 0 1 0 0 0 0 4 4 0 0 0 1 4 4 0 1 1 0 0 0 4]\nreward= [  0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   5.   0.   0.   0.   0.   5.   5. -10.   0.   0.   0.   0.\n   0.   5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2460\nAction:  [0 0 0 4 0 0 3 0 0 3 0 0 4 1 4 4 0 4 4 3 4 4 4 3 4 1 4 0 1 4]\nreward= [  0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.\n   5.   5.   0.   5.   0.   0.   5.   0. -10.   0.   0.   0.   5.   0.\n   0.   5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2480\nAction:  [0 1 3 0 4 1 0 4 0 4 4 3 4 0 0 4 4 0 3 4 0 4 1 0 4 4 0 0 0 0]\nreward= [   0.    0.    0.    0.  -10.    0.    0.   -5.    0.    0.    5.    0.\n    5.    0.    0. 1026.   -5.    0.    0.    5.    0.    5.    0.    0.\n    0.    0.    0.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2500\nAction:  [4 0 0 0 4 0 4 0 1 0 1 4 0 0 4 0 0 1 0 0 0 1 0 3 4 4 3 4 1 4]\nreward= [ 5.  0.  0.  0. -5.  0.  5.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  5.  5.  0.  0.  0.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2520\nAction:  [0 0 0 3 4 0 0 0 0 4 0 0 0 3 4 3 0 0 4 0 0 0 4 0 4 4 1 4 0 4]\nreward= [ 0.  0.  0.  0. -5.  0.  0.  0.  0.  5.  0.  0.  0.  0.  5.  0.  0.  0.\n  5.  0.  0.  0.  5.  0.  5. -5.  0.  5.  0.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2540\nAction:  [3 0 0 4 0 0 0 4 0 0 4 4 1 4 1 4 0 3 4 0 0 4 0 4 4 0 0 0 3 4]\nreward= [  0.   0.   0.   0.   0.   0.   0.  -5.   0.   0.   5.   5.   0.   5.\n   0.   5.   0.   0. -10.   0.   0.  -5.   0.  -5.   0.   0.   0.   0.\n   0.   5.]  done= [False False False False False False False False False False False False\n False False False False False False  True False False False False  True\n False False False False False False]\nStep 2560\nAction:  [0 3 0 0 0 0 4 1 0 4 1 0 3 4 3 1 0 1 0 0 4 0 3 0 0 0 3 0 0 3]\nreward= [ 0.  0.  0.  0.  0.  0.  5.  0.  0. -5.  0.  0.  0.  5.  0.  0.  0.  0.\n  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2580\nAction:  [0 3 3 0 0 0 0 3 0 0 0 0 0 0 3 4 0 3 0 3 4 4 4 0 0 1 0 1 3 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 5. 0. 5. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2600\nAction:  [0 3 0 0 4 0 0 3 3 0 0 3 3 0 1 4 0 0 4 4 0 0 4 4 0 4 1 0 0 4]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.\n  5. -5.  0.  0.  5.  5.  0.  5.  0.  0.  0.  5.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2620\nAction:  [3 0 3 0 0 4 0 0 4 4 4 0 0 0 0 0 4 0 3 4 0 1 0 0 0 0 4 4 0 1]\nreward= [  0.   0.   0.   0.   0.   5.   0.   0.   0.   5.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0. -10.   5.   0.\n   0.   0.]  done= [False False False False False False False False  True False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2640\nAction:  [4 0 4 4 3 1 3 0 4 0 3 0 1 0 4 0 0 0 3 0 4 3 4 0 3 0 1 4 1 0]\nreward= [   5.    0.    5. 1006.    0.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.    0.    0.    5.    0.    5.    0.\n    0.    0.    0.    5.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2660\nAction:  [4 0 0 1 0 4 0 0 4 4 4 0 0 3 4 0 4 0 0 4 0 0 4 1 4 0 3 0 4 0]\nreward= [-5.  0.  0.  0.  0.  0.  0.  0.  5.  5.  5.  0.  0.  0.  5.  0.  0.  0.\n  0.  5.  0.  0.  5.  0.  5.  0.  0.  0.  5.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False  True False False False\n False False False False False False]\nStep 2680\nAction:  [4 0 1 0 1 0 1 4 0 0 0 0 1 4 4 4 0 4 0 0 0 0 0 0 0 0 1 4 3 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  5. -5.  5.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2700\nAction:  [0 1 1 0 0 4 1 0 0 0 0 1 0 3 0 4 4 4 4 3 4 0 0 4 0 1 4 0 4 0]\nreward= [   0.    0.    0.    0.    0.    5.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.  -10.    5.    5. 1006.    0.    5.    0.    0.    5.\n    0.    0.    5.    0.    0.    0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2720\nAction:  [3 0 0 0 3 0 4 0 4 4 0 0 0 4 0 0 4 0 4 1 4 4 0 4 4 0 0 0 4 0]\nreward= [   0.    0.    0.    0.    0.    0.    5.    0.    5.   -5.    0.    0.\n    0.    0.    0.    0.    0.    0.    5.    0.    0.  -10.    0.    5.\n 1006.    0.    0.    0.    5.    0.]  done= [False False False False False False False False False False False False\n False False False False  True False False False False False False False\n False False False False False False]\nStep 2740\nAction:  [0 0 3 4 3 1 4 0 0 0 1 4 0 3 0 0 4 0 0 0 0 4 4 4 0 1 0 0 0 4]\nreward= [  0.   0.   0.   5.   0.   0.   5.   0.   0.   0.   0.  -5.   0.   0.\n   0.   0. -10.   0.   0.   0.   0.   5.  -5.  -5.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False  True\n False False False False False False False False False  True False False\n False False False False False False]\nStep 2760\nAction:  [0 0 0 0 1 0 4 0 0 0 0 4 0 3 0 0 0 4 1 3 0 0 0 0 0 1 0 0 0 3]\nreward= [0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2780\nAction:  [0 0 0 4 0 0 0 0 0 0 0 0 0 4 4 4 0 0 3 0 4 4 4 0 1 4 0 0 0 3]\nreward= [ 0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  5.  0.  0.\n  0.  0.  5. -5.  0.  0.  0. -5.  0.  0.  0.  0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2800\nAction:  [3 0 3 3 0 3 4 1 4 4 0 0 0 4 1 0 1 0 0 4 1 4 0 3 0 3 0 0 4 0]\nreward= [  0.   0.   0.   0.   0.   0.   5.   0.   5.  -5.   0.   0.   0. -10.\n   0.   0.   0.   0.   0.  -5.   0.   5.   0.   0.   0.   0.   0.   0.\n   5.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2820\nAction:  [4 3 4 3 0 0 0 1 0 0 4 0 3 1 4 4 0 4 0 4 0 3 0 0 4 0 4 0 0 0]\nreward= [5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2840\nAction:  [0 0 4 4 0 0 0 1 4 4 0 4 4 0 4 0 0 0 4 0 0 4 0 3 0 4 3 0 0 4]\nreward= [  0.   0.   5.   0.   0.   0.   0.   0.   0.   5.   0.   5.   5.   0.\n   5.   0.   0.   0. -10.   0.   0.   5.   0.   0.   0.   5.   0.   0.\n   0.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2860\nAction:  [4 4 4 3 3 0 1 4 3 4 0 1 0 0 3 0 1 4 3 1 0 1 1 4 4 0 1 3 3 1]\nreward= [  0.   5.   5.   0.   0.   0.   0. -10.   0.   5.   0.   0.   0.   0.\n   0.   0.   0.   5.   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.\n   0.   0.]  done= [False False False False False False False  True False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2880\nAction:  [3 0 4 1 0 0 0 0 0 0 0 0 3 0 4 4 0 4 3 0 0 0 0 0 0 0 0 4 4 3]\nreward= [0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 5. 0. 5. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2900\nAction:  [0 1 1 0 0 0 4 0 0 4 3 3 1 3 0 1 0 4 1 1 1 1 4 0 3 0 0 3 4 0]\nreward= [0. 0. 0. 0. 0. 0. 5. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 5. 0.\n 0. 0. 0. 0. 5. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2920\nAction:  [1 0 0 0 0 4 3 0 0 0 4 3 0 0 4 0 3 4 4 1 0 0 0 4 3 3 0 1 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 5.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False  True False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2940\nAction:  [3 0 4 0 0 0 3 0 1 0 0 0 4 4 3 0 4 1 0 1 0 0 4 3 4 1 4 0 3 3]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 5. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2960\nAction:  [0 0 1 4 0 0 1 0 4 4 1 0 3 0 4 0 4 0 3 3 0 4 0 4 0 0 0 3 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 2980\nAction:  [0 4 0 0 1 0 0 4 0 0 0 0 4 0 0 0 1 0 4 1 0 4 3 4 1 0 0 0 4 0]\nreward= [  0.   5.   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n -10.   0.]  done= [False False False False False False False False False False False False\n False False False False False False False False False False False  True\n False False False False  True False]\n","output_type":"stream"}]},{"cell_type":"code","source":"best_replay_path = replay_folder + '/' + str(max_game_id) + '/' + str(max_ep_id)\n\nprint(\"After playing 30 envs each for \", test_steps, \" steps:\")\nprint(\" Max reward=\", max_reward, \" Best video: \" + best_replay_path)\nprint(\" Removed lines=\", max_rm_lines, \" lifetime=\", max_lifetime)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T05:40:20.249526Z","iopub.execute_input":"2023-12-30T05:40:20.249815Z","iopub.status.idle":"2023-12-30T05:40:20.256202Z","shell.execute_reply.started":"2023-12-30T05:40:20.249792Z","shell.execute_reply":"2023-12-30T05:40:20.255262Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"After playing 30 envs each for  3000  steps:\n Max reward= 5340.0  Best video: ./replay/3/12\n Removed lines= 5  lifetime= 150\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<H2>Make a gif image to visualize the best play</H2>","metadata":{}},{"cell_type":"code","source":"import glob\nimport imageio\n\nfilenames = sorted(glob.glob(best_replay_path + '/*.png'))\n\nimages = []\nfor filename in filenames:\n    images.append(imageio.imread(filename))\nimageio.mimsave('replay.gif', images, loop=0)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T05:40:20.257431Z","iopub.execute_input":"2023-12-30T05:40:20.257720Z","iopub.status.idle":"2023-12-30T05:40:20.765598Z","shell.execute_reply.started":"2023-12-30T05:40:20.257685Z","shell.execute_reply":"2023-12-30T05:40:20.764548Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_138/3508690983.py:8: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  images.append(imageio.imread(filename))\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/working/replay.gif')","metadata":{"execution":{"iopub.status.busy":"2023-12-30T05:40:20.766851Z","iopub.execute_input":"2023-12-30T05:40:20.767157Z","iopub.status.idle":"2023-12-30T05:40:20.776853Z","shell.execute_reply.started":"2023-12-30T05:40:20.767129Z","shell.execute_reply":"2023-12-30T05:40:20.775997Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"image/gif":"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","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}]},{"cell_type":"code","source":"model.save('your_studentID_a2c_30env_2M.zip')","metadata":{"execution":{"iopub.status.busy":"2023-12-30T05:40:20.777903Z","iopub.execute_input":"2023-12-30T05:40:20.778219Z","iopub.status.idle":"2023-12-30T05:40:20.960244Z","shell.execute_reply.started":"2023-12-30T05:40:20.778193Z","shell.execute_reply":"2023-12-30T05:40:20.959205Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"with open('submission.csv', 'w') as fs:\n    fs.write('Id,Predicted\\n')\n    fs.write(f'game_score,{max_reward}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-12-30T05:40:20.961627Z","iopub.execute_input":"2023-12-30T05:40:20.961941Z","iopub.status.idle":"2023-12-30T05:40:20.967258Z","shell.execute_reply.started":"2023-12-30T05:40:20.961914Z","shell.execute_reply":"2023-12-30T05:40:20.966240Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Upload your results to Kaggle\nfrom IPython.display import FileLink\nFileLink('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-30T05:40:20.968582Z","iopub.execute_input":"2023-12-30T05:40:20.969260Z","iopub.status.idle":"2023-12-30T05:40:20.980059Z","shell.execute_reply.started":"2023-12-30T05:40:20.969218Z","shell.execute_reply":"2023-12-30T05:40:20.978961Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]}]}