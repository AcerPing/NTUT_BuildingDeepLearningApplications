{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20156.274358,
   "end_time": "2023-06-05T16:10:17.87114",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-05T10:34:21.596782",
   "version": "2.4.0"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 65534,
     "databundleVersionId": 7325705,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30587,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import socket\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.242932,
     "end_time": "2023-06-05T10:34:34.982099",
     "exception": false,
     "start_time": "2023-06-05T10:34:34.739167",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:29:43.808768Z",
     "iopub.execute_input": "2023-12-27T05:29:43.809650Z",
     "iopub.status.idle": "2023-12-27T05:29:43.851296Z",
     "shell.execute_reply.started": "2023-12-27T05:29:43.809615Z",
     "shell.execute_reply": "2023-12-27T05:29:43.850510Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Stable Baseline3 version >= 2.0.0a5\n",
    "#### Note some SB3 versions are not compatible with Gymnasium interface."
   ],
   "metadata": {
    "papermill": {
     "duration": 0.008761,
     "end_time": "2023-06-05T10:34:34.999658",
     "exception": false,
     "start_time": "2023-06-05T10:34:34.990897",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install \"stable-baselines3[extra] >= 2.0.0a5\""
   ],
   "metadata": {
    "papermill": {
     "duration": 36.440551,
     "end_time": "2023-06-05T10:35:11.448804",
     "exception": false,
     "start_time": "2023-06-05T10:34:35.008253",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:29:43.852966Z",
     "iopub.execute_input": "2023-12-27T05:29:43.853278Z",
     "iopub.status.idle": "2023-12-27T05:29:59.070364Z",
     "shell.execute_reply.started": "2023-12-27T05:29:43.853252Z",
     "shell.execute_reply": "2023-12-27T05:29:59.069210Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting stable-baselines3[extra]>=2.0.0a5\n  Obtaining dependency information for stable-baselines3[extra]>=2.0.0a5 from https://files.pythonhosted.org/packages/1e/43/d4b83e644c7e42d90d76a1987fb98a2ab286a2b5593350210ca8efcc378e/stable_baselines3-2.2.1-py3-none-any.whl.metadata\n  Using cached stable_baselines3-2.2.1-py3-none-any.whl.metadata (5.0 kB)\nCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra]>=2.0.0a5)\n  Obtaining dependency information for gymnasium<0.30,>=0.28.1 from https://files.pythonhosted.org/packages/a8/4d/3cbfd81ed84db450dbe73a89afcd8bc405273918415649ac6683356afe92/gymnasium-0.29.1-py3-none-any.whl.metadata\n  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (1.24.3)\nRequirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (2.0.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (2.2.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (2.0.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (3.7.3)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (4.8.1.78)\nCollecting pygame (from stable-baselines3[extra]>=2.0.0a5)\n  Obtaining dependency information for pygame from https://files.pythonhosted.org/packages/c8/c7/0d77e0e327bf09c12f445f92f5bad0b447375d7b836c5bac5255ead8436f/pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Using cached pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: tensorboard>=2.9.1 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (2.13.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (5.9.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (4.66.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (13.5.2)\nCollecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra]>=2.0.0a5)\n  Obtaining dependency information for shimmy[atari]~=1.3.0 from https://files.pythonhosted.org/packages/dc/f9/07ef16463db14ac1b30f149c379760f5cacf3fc677b295d29a92f3127914/Shimmy-1.3.0-py3-none-any.whl.metadata\n  Using cached Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5) (10.1.0)\nCollecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra]>=2.0.0a5)\n  Using cached AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (2.31.0)\nCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5)\n  Using cached AutoROM.accept_rom_license-0.6.1-py3-none-any.whl\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5) (4.5.0)\nCollecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5)\n  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\nCollecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra]>=2.0.0a5)\n  Using cached ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.51.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (3.4.4)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (68.1.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (3.0.1)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (0.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (3.1.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a5) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]>=2.0.0a5) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]>=2.0.0a5) (2023.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5) (2.16.1)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]>=2.0.0a5) (5.13.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (4.9)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (1.3.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5) (0.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]>=2.0.0a5) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5) (3.2.2)\nUsing cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\nUsing cached pygame-2.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\nUsing cached stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\nUsing cached Shimmy-1.3.0-py3-none-any.whl (37 kB)\nInstalling collected packages: farama-notifications, pygame, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable-baselines3\n  Attempting uninstall: gymnasium\n    Found existing installation: Gymnasium 0.26.3\n    Uninstalling Gymnasium-0.26.3:\n      Successfully uninstalled Gymnasium-0.26.3\nSuccessfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 pygame-2.5.2 shimmy-1.3.0 stable-baselines3-2.2.1\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the Java Tetris Server using subprocess"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.013389,
     "end_time": "2023-06-05T10:35:11.475698",
     "exception": false,
     "start_time": "2023-06-05T10:35:11.462309",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# You can download latest server from AIoTLab website. Currently (2023/12/5) is v0.6\n",
    "# !wget http://www.aiotlab.org/teaching/oop/tetris/TetrisTCPserver_v0.6.jar\n",
    "!cp /kaggle/input/112-1-ntut-dl-app-hw4/TetrisTCPserver_v0.6.jar ."
   ],
   "metadata": {
    "papermill": {
     "duration": 2.908889,
     "end_time": "2023-06-05T10:35:14.398262",
     "exception": false,
     "start_time": "2023-06-05T10:35:11.489373",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:29:59.072239Z",
     "iopub.execute_input": "2023-12-27T05:29:59.072563Z",
     "iopub.status.idle": "2023-12-27T05:30:00.057374Z",
     "shell.execute_reply.started": "2023-12-27T05:29:59.072535Z",
     "shell.execute_reply": "2023-12-27T05:30:00.056303Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"java\",\"-jar\",\"TetrisTCPserver_v0.6.jar\"])"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.033297,
     "end_time": "2023-06-05T10:35:14.448829",
     "exception": false,
     "start_time": "2023-06-05T10:35:14.415532",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:30:00.058810Z",
     "iopub.execute_input": "2023-12-27T05:30:00.059145Z",
     "iopub.status.idle": "2023-12-27T05:30:00.068603Z",
     "shell.execute_reply.started": "2023-12-27T05:30:00.059116Z",
     "shell.execute_reply": "2023-12-27T05:30:00.067504Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<Popen: returncode: None args: ['java', '-jar', 'TetrisTCPserver_v0.6.jar']>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create our own Tetris Test environment by inheriting Gym class"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.01539,
     "end_time": "2023-06-05T10:35:14.481793",
     "exception": false,
     "start_time": "2023-06-05T10:35:14.466403",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ],
   "metadata": {
    "papermill": {
     "duration": 1.097632,
     "end_time": "2023-06-05T10:35:15.595159",
     "exception": false,
     "start_time": "2023-06-05T10:35:14.497527",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:30:00.072162Z",
     "iopub.execute_input": "2023-12-27T05:30:00.072792Z",
     "iopub.status.idle": "2023-12-27T05:30:00.796503Z",
     "shell.execute_reply.started": "2023-12-27T05:30:00.072757Z",
     "shell.execute_reply": "2023-12-27T05:30:00.795730Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "Tetris TCP server is listening at 10612\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class TetrisEnv(gym.Env):\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 20}\n",
    "    \n",
    "    '''\n",
    "        The supported actions are\n",
    "        0: move -1\n",
    "        1: move 1\n",
    "        2: rotate 0 // counter-clockwise\n",
    "        3: rotate 1 // clockwise\n",
    "        4: drop down\n",
    "    '''\n",
    "    N_DISCRETE_ACTIONS = 5\n",
    "    \n",
    "    IMG_HEIGHT = 200\n",
    "    IMG_WIDTH = 100\n",
    "    IMG_CHANNELS = 3\n",
    "    \n",
    "\n",
    "    def __init__(self, host_ip=\"127.0.0.1\", host_port=10612):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.action_space = spaces.Discrete(self.N_DISCRETE_ACTIONS)\n",
    "        # Example for using image as input (channel-first; channel-last also works):\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "                                            shape=(self.IMG_HEIGHT, self.IMG_WIDTH, self.IMG_CHANNELS), dtype=np.uint8)\n",
    "        self.server_ip = host_ip\n",
    "        self.server_port = host_port\n",
    "            \n",
    "        self.client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.client_sock.connect((self.server_ip, self.server_port))\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.client_sock.sendall(b\"move -1\\n\")\n",
    "        elif action == 1:\n",
    "            self.client_sock.sendall(b\"move 1\\n\")\n",
    "        elif action == 2:\n",
    "            self.client_sock.sendall(b\"rotate 0\\n\")\n",
    "        elif action == 3:\n",
    "            self.client_sock.sendall(b\"rotate 1\\n\")\n",
    "        elif action == 4:\n",
    "            self.client_sock.sendall(b\"drop\\n\")\n",
    "            \n",
    "        terminated, lines, height, holes, observation = self.get_tetris_server_response(self.client_sock)\n",
    "        self.observation = observation\n",
    "        \n",
    "        reward = 0\n",
    "        if action == 4: # Drop reward\n",
    "            reward += 5\n",
    "            \n",
    "        # Negative height reward\n",
    "        if height > self.height:\n",
    "            reward -= (height - self.height)*5\n",
    "        \n",
    "        # Positive hole reduction reward\n",
    "        if holes < self.holes:\n",
    "            reward += (self.holes - holes)*20\n",
    "        \n",
    "        if lines > self.lines_removed:\n",
    "            reward = reward + (lines - self.lines_removed)*1000\n",
    "            self.lines_removed = lines\n",
    "            \n",
    "        self.reward = self.reward + reward\n",
    "        self.holes = holes\n",
    "        self.height = height\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        return (observation, reward, terminated, truncated, info)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.client_sock.sendall(b\"start\\n\")\n",
    "        terminated, lines, height, holes, observation = self.get_tetris_server_response(self.client_sock)\n",
    "        self.observation = observation\n",
    "        self.reward = 0\n",
    "        self.lines_removed = 0\n",
    "        self.holes = 0\n",
    "        self.height = 0\n",
    "        info = {}\n",
    "        return observation, info\n",
    "\n",
    "    def render(self):\n",
    "        ''''''\n",
    "        #if self.render_mode == \"console\":\n",
    "        #    print('Total reward ' + str(self.reward))\n",
    "        '''\n",
    "        if self.render_mode == \"human\":\n",
    "            cv2.imshow(\"Image\", self.observation)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        '''\n",
    "\n",
    "    def close(self):\n",
    "        self.client_sock.close()\n",
    "        \n",
    "    def get_tetris_server_response(self, sock):\n",
    "        is_game_over = (sock.recv(1) == b'\\x01')\n",
    "        removed_lines = int.from_bytes(sock.recv(4), 'big')\n",
    "        height = int.from_bytes(sock.recv(4), 'big')\n",
    "        holes = int.from_bytes(sock.recv(4), 'big')\n",
    "        img_size = int.from_bytes(sock.recv(4), 'big')\n",
    "        img_png = sock.recv(img_size)\n",
    "\n",
    "        nparr = np.frombuffer(img_png, np.uint8)\n",
    "        np_image = cv2.imdecode(nparr, -1)\n",
    "\n",
    "        return is_game_over, removed_lines, height, holes, np_image"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.040859,
     "end_time": "2023-06-05T10:35:15.650775",
     "exception": false,
     "start_time": "2023-06-05T10:35:15.609916",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:30:00.797615Z",
     "iopub.execute_input": "2023-12-27T05:30:00.797870Z",
     "iopub.status.idle": "2023-12-27T05:30:00.817597Z",
     "shell.execute_reply.started": "2023-12-27T05:30:00.797848Z",
     "shell.execute_reply": "2023-12-27T05:30:00.816568Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use SB3 env_checker to check our environment"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.0144,
     "end_time": "2023-06-05T10:35:15.679875",
     "exception": false,
     "start_time": "2023-06-05T10:35:15.665475",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = TetrisEnv()\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "# No response may be caused by mismatched action state definition and implementation\n",
    "check_env(env)"
   ],
   "metadata": {
    "papermill": {
     "duration": 14.361725,
     "end_time": "2023-06-05T10:35:30.0564",
     "exception": false,
     "start_time": "2023-06-05T10:35:15.694675",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:30:00.819214Z",
     "iopub.execute_input": "2023-12-27T05:30:00.819800Z",
     "iopub.status.idle": "2023-12-27T05:30:18.033014Z",
     "shell.execute_reply.started": "2023-12-27T05:30:00.819765Z",
     "shell.execute_reply": "2023-12-27T05:30:18.032098Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Client has joined the game\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Randomly test the environment"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.014807,
     "end_time": "2023-06-05T10:35:30.086147",
     "exception": false,
     "start_time": "2023-06-05T10:35:30.07134",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "obs, info = env.reset()\n",
    "n_steps = 20\n",
    "for _ in range(n_steps):\n",
    "    # Random action\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    env.render() # We render nothing now\n",
    "    \n",
    "    if terminated:\n",
    "        break"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.852294,
     "end_time": "2023-06-05T10:35:30.953195",
     "exception": false,
     "start_time": "2023-06-05T10:35:30.100901",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:30:18.034252Z",
     "iopub.execute_input": "2023-12-27T05:30:18.034806Z",
     "iopub.status.idle": "2023-12-27T05:30:18.900719Z",
     "shell.execute_reply.started": "2023-12-27T05:30:18.034778Z",
     "shell.execute_reply": "2023-12-27T05:30:18.899887Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the final screen\n",
    "%matplotlib inline \n",
    "plt.imshow(obs)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.329119,
     "end_time": "2023-06-05T10:35:31.297874",
     "exception": false,
     "start_time": "2023-06-05T10:35:30.968755",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:30:18.901837Z",
     "iopub.execute_input": "2023-12-27T05:30:18.902136Z",
     "iopub.status.idle": "2023-12-27T05:30:19.191594Z",
     "shell.execute_reply.started": "2023-12-27T05:30:18.902110Z",
     "shell.execute_reply": "2023-12-27T05:30:19.190663Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f5226444790>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.025065,
     "end_time": "2023-06-05T10:35:31.338187",
     "exception": false,
     "start_time": "2023-06-05T10:35:31.313122",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:30:19.192862Z",
     "iopub.execute_input": "2023-12-27T05:30:19.193457Z",
     "iopub.status.idle": "2023-12-27T05:30:19.198402Z",
     "shell.execute_reply.started": "2023-12-27T05:30:19.193421Z",
     "shell.execute_reply": "2023-12-27T05:30:19.197514Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create an environment with 30 client threads"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.014836,
     "end_time": "2023-06-05T10:35:31.368819",
     "exception": false,
     "start_time": "2023-06-05T10:35:31.353983",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's try A2C by creating 30 environments\n",
    "vec_env = make_vec_env(TetrisEnv, n_envs=30)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.066246,
     "end_time": "2023-06-05T10:35:31.450626",
     "exception": false,
     "start_time": "2023-06-05T10:35:31.38438",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:30:19.199762Z",
     "iopub.execute_input": "2023-12-27T05:30:19.200393Z",
     "iopub.status.idle": "2023-12-27T05:30:19.274336Z",
     "shell.execute_reply.started": "2023-12-27T05:30:19.200357Z",
     "shell.execute_reply": "2023-12-27T05:30:19.273304Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "Client has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\nClient has joined the game\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We choose A2C with CNN policy, and train 3,000,000 steps"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.015242,
     "end_time": "2023-06-05T10:35:31.485434",
     "exception": false,
     "start_time": "2023-06-05T10:35:31.470192",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the agent\n",
    "model = A2C(\"CnnPolicy\", vec_env, verbose=1).learn(3000000)"
   ],
   "metadata": {
    "papermill": {
     "duration": 19960.126622,
     "end_time": "2023-06-05T16:08:11.627714",
     "exception": false,
     "start_time": "2023-06-05T10:35:31.501092",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T05:30:19.275642Z",
     "iopub.execute_input": "2023-12-27T05:30:19.276297Z",
     "iopub.status.idle": "2023-12-27T08:00:09.634480Z",
     "shell.execute_reply.started": "2023-12-27T05:30:19.276259Z",
     "shell.execute_reply": "2023-12-27T08:00:09.633249Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "Using cuda device\nWrapping the env in a VecTransposeImage.\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 119      |\n|    ep_rew_mean        | -17.3    |\n| time/                 |          |\n|    fps                | 302      |\n|    iterations         | 100      |\n|    time_elapsed       | 49       |\n|    total_timesteps    | 15000    |\n| train/                |          |\n|    entropy_loss       | -0.189   |\n|    explained_variance | 0.807    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 99       |\n|    policy_loss        | -0.433   |\n|    value_loss         | 28.6     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 92.8     |\n|    ep_rew_mean        | 22.2     |\n| time/                 |          |\n|    fps                | 324      |\n|    iterations         | 200      |\n|    time_elapsed       | 92       |\n|    total_timesteps    | 30000    |\n| train/                |          |\n|    entropy_loss       | -0.0986  |\n|    explained_variance | 0.335    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 199      |\n|    policy_loss        | 0.131    |\n|    value_loss         | 693      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 80.3     |\n|    ep_rew_mean        | 3.7      |\n| time/                 |          |\n|    fps                | 327      |\n|    iterations         | 300      |\n|    time_elapsed       | 137      |\n|    total_timesteps    | 45000    |\n| train/                |          |\n|    entropy_loss       | -0.157   |\n|    explained_variance | 0.0294   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 299      |\n|    policy_loss        | -0.282   |\n|    value_loss         | 250      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 77       |\n|    ep_rew_mean        | -26.9    |\n| time/                 |          |\n|    fps                | 329      |\n|    iterations         | 400      |\n|    time_elapsed       | 182      |\n|    total_timesteps    | 60000    |\n| train/                |          |\n|    entropy_loss       | -1       |\n|    explained_variance | 0.62     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 399      |\n|    policy_loss        | -0.862   |\n|    value_loss         | 14.6     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 78.1     |\n|    ep_rew_mean        | -27.1    |\n| time/                 |          |\n|    fps                | 330      |\n|    iterations         | 500      |\n|    time_elapsed       | 227      |\n|    total_timesteps    | 75000    |\n| train/                |          |\n|    entropy_loss       | -0.899   |\n|    explained_variance | 0.761    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 499      |\n|    policy_loss        | 0.41     |\n|    value_loss         | 3.53     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 79.6     |\n|    ep_rew_mean        | 1.2      |\n| time/                 |          |\n|    fps                | 331      |\n|    iterations         | 600      |\n|    time_elapsed       | 271      |\n|    total_timesteps    | 90000    |\n| train/                |          |\n|    entropy_loss       | -0.859   |\n|    explained_variance | 0.193    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 599      |\n|    policy_loss        | -1.12    |\n|    value_loss         | 24.5     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 84.7     |\n|    ep_rew_mean        | -13.4    |\n| time/                 |          |\n|    fps                | 332      |\n|    iterations         | 700      |\n|    time_elapsed       | 316      |\n|    total_timesteps    | 105000   |\n| train/                |          |\n|    entropy_loss       | -0.813   |\n|    explained_variance | 0.321    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 699      |\n|    policy_loss        | 0.356    |\n|    value_loss         | 11.6     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 81.1     |\n|    ep_rew_mean        | -5.2     |\n| time/                 |          |\n|    fps                | 332      |\n|    iterations         | 800      |\n|    time_elapsed       | 360      |\n|    total_timesteps    | 120000   |\n| train/                |          |\n|    entropy_loss       | -0.371   |\n|    explained_variance | 0.118    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 799      |\n|    policy_loss        | -0.106   |\n|    value_loss         | 10       |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 87.7     |\n|    ep_rew_mean        | -5.1     |\n| time/                 |          |\n|    fps                | 333      |\n|    iterations         | 900      |\n|    time_elapsed       | 404      |\n|    total_timesteps    | 135000   |\n| train/                |          |\n|    entropy_loss       | -0.661   |\n|    explained_variance | 0.484    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 899      |\n|    policy_loss        | 0.0486   |\n|    value_loss         | 6.37     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 88.7     |\n|    ep_rew_mean        | 0.05     |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 1000     |\n|    time_elapsed       | 448      |\n|    total_timesteps    | 150000   |\n| train/                |          |\n|    entropy_loss       | -0.532   |\n|    explained_variance | -0.00205 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 999      |\n|    policy_loss        | 14.3     |\n|    value_loss         | 2.62e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 32.1     |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 1100     |\n|    time_elapsed       | 492      |\n|    total_timesteps    | 165000   |\n| train/                |          |\n|    entropy_loss       | -0.466   |\n|    explained_variance | 0.652    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1099     |\n|    policy_loss        | 0.18     |\n|    value_loss         | 15.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 55.3     |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 1200     |\n|    time_elapsed       | 534      |\n|    total_timesteps    | 180000   |\n| train/                |          |\n|    entropy_loss       | -0.201   |\n|    explained_variance | 0.361    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1199     |\n|    policy_loss        | -0.0147  |\n|    value_loss         | 49.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 123      |\n|    ep_rew_mean        | 81.5     |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 1300     |\n|    time_elapsed       | 577      |\n|    total_timesteps    | 195000   |\n| train/                |          |\n|    entropy_loss       | -0.0584  |\n|    explained_variance | 0.681    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1299     |\n|    policy_loss        | 0.000439 |\n|    value_loss         | 269      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 11.3     |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 1400     |\n|    time_elapsed       | 620      |\n|    total_timesteps    | 210000   |\n| train/                |          |\n|    entropy_loss       | -0.341   |\n|    explained_variance | 0.919    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1399     |\n|    policy_loss        | -0.242   |\n|    value_loss         | 3.09     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 107      |\n|    ep_rew_mean        | 64.7     |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 1500     |\n|    time_elapsed       | 663      |\n|    total_timesteps    | 225000   |\n| train/                |          |\n|    entropy_loss       | -0.306   |\n|    explained_variance | 0.846    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1499     |\n|    policy_loss        | -0.197   |\n|    value_loss         | 6.99     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 112      |\n|    ep_rew_mean        | 28       |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 1600     |\n|    time_elapsed       | 706      |\n|    total_timesteps    | 240000   |\n| train/                |          |\n|    entropy_loss       | -0.347   |\n|    explained_variance | 0.893    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1599     |\n|    policy_loss        | -0.0859  |\n|    value_loss         | 4.61     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 64.2     |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 1700     |\n|    time_elapsed       | 749      |\n|    total_timesteps    | 255000   |\n| train/                |          |\n|    entropy_loss       | -0.415   |\n|    explained_variance | 0.802    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1699     |\n|    policy_loss        | -0.451   |\n|    value_loss         | 7.51     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 117      |\n|    ep_rew_mean        | 85.1     |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 1800     |\n|    time_elapsed       | 792      |\n|    total_timesteps    | 270000   |\n| train/                |          |\n|    entropy_loss       | -0.258   |\n|    explained_variance | 0.791    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1799     |\n|    policy_loss        | -0.191   |\n|    value_loss         | 10.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 78.9     |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 1900     |\n|    time_elapsed       | 835      |\n|    total_timesteps    | 285000   |\n| train/                |          |\n|    entropy_loss       | -0.279   |\n|    explained_variance | 0.865    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1899     |\n|    policy_loss        | 0.187    |\n|    value_loss         | 6.82     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 98.3     |\n|    ep_rew_mean        | 219      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 2000     |\n|    time_elapsed       | 878      |\n|    total_timesteps    | 300000   |\n| train/                |          |\n|    entropy_loss       | -0.26    |\n|    explained_variance | 0.828    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 1999     |\n|    policy_loss        | -0.207   |\n|    value_loss         | 5.68     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 227      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 2100     |\n|    time_elapsed       | 922      |\n|    total_timesteps    | 315000   |\n| train/                |          |\n|    entropy_loss       | -0.238   |\n|    explained_variance | 0.946    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2099     |\n|    policy_loss        | -0.0809  |\n|    value_loss         | 3.34     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 304      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 2200     |\n|    time_elapsed       | 965      |\n|    total_timesteps    | 330000   |\n| train/                |          |\n|    entropy_loss       | -0.271   |\n|    explained_variance | 0.00359  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2199     |\n|    policy_loss        | 1.72     |\n|    value_loss         | 6.77e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 227      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 2300     |\n|    time_elapsed       | 1008     |\n|    total_timesteps    | 345000   |\n| train/                |          |\n|    entropy_loss       | -0.216   |\n|    explained_variance | 0.905    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2299     |\n|    policy_loss        | -0.0368  |\n|    value_loss         | 4.96     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 99.9     |\n|    ep_rew_mean        | 279      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 2400     |\n|    time_elapsed       | 1052     |\n|    total_timesteps    | 360000   |\n| train/                |          |\n|    entropy_loss       | -0.244   |\n|    explained_variance | 0.836    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2399     |\n|    policy_loss        | -0.253   |\n|    value_loss         | 5.64     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 176      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 2500     |\n|    time_elapsed       | 1096     |\n|    total_timesteps    | 375000   |\n| train/                |          |\n|    entropy_loss       | -0.208   |\n|    explained_variance | -0.0225  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2499     |\n|    policy_loss        | 9.43     |\n|    value_loss         | 3.24e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 130      |\n|    ep_rew_mean        | 453      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 2600     |\n|    time_elapsed       | 1138     |\n|    total_timesteps    | 390000   |\n| train/                |          |\n|    entropy_loss       | -0.221   |\n|    explained_variance | 0.916    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2599     |\n|    policy_loss        | -0.151   |\n|    value_loss         | 7.49     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 332      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 2700     |\n|    time_elapsed       | 1182     |\n|    total_timesteps    | 405000   |\n| train/                |          |\n|    entropy_loss       | -0.178   |\n|    explained_variance | -0.00912 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2699     |\n|    policy_loss        | 1.54     |\n|    value_loss         | 1.56e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 111      |\n|    ep_rew_mean        | 393      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 2800     |\n|    time_elapsed       | 1226     |\n|    total_timesteps    | 420000   |\n| train/                |          |\n|    entropy_loss       | -0.147   |\n|    explained_variance | 0.929    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2799     |\n|    policy_loss        | -0.2     |\n|    value_loss         | 6.41     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 236      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 2900     |\n|    time_elapsed       | 1269     |\n|    total_timesteps    | 435000   |\n| train/                |          |\n|    entropy_loss       | -0.178   |\n|    explained_variance | 0.971    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2899     |\n|    policy_loss        | -0.0553  |\n|    value_loss         | 4.1      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 380      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 3000     |\n|    time_elapsed       | 1312     |\n|    total_timesteps    | 450000   |\n| train/                |          |\n|    entropy_loss       | -0.16    |\n|    explained_variance | 0.969    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 2999     |\n|    policy_loss        | 0.0142   |\n|    value_loss         | 2.34     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 370      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 3100     |\n|    time_elapsed       | 1356     |\n|    total_timesteps    | 465000   |\n| train/                |          |\n|    entropy_loss       | -0.191   |\n|    explained_variance | 0.95     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3099     |\n|    policy_loss        | -0.151   |\n|    value_loss         | 6.41     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 438      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 3200     |\n|    time_elapsed       | 1399     |\n|    total_timesteps    | 480000   |\n| train/                |          |\n|    entropy_loss       | -0.116   |\n|    explained_variance | -0.00145 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3199     |\n|    policy_loss        | -0.0136  |\n|    value_loss         | 2.43e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 421      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 3300     |\n|    time_elapsed       | 1443     |\n|    total_timesteps    | 495000   |\n| train/                |          |\n|    entropy_loss       | -0.101   |\n|    explained_variance | 0.0222   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3299     |\n|    policy_loss        | -0.0736  |\n|    value_loss         | 1.96e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 350      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 3400     |\n|    time_elapsed       | 1487     |\n|    total_timesteps    | 510000   |\n| train/                |          |\n|    entropy_loss       | -0.134   |\n|    explained_variance | 0.933    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3399     |\n|    policy_loss        | 0.209    |\n|    value_loss         | 6.65     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 117      |\n|    ep_rew_mean        | 363      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 3500     |\n|    time_elapsed       | 1529     |\n|    total_timesteps    | 525000   |\n| train/                |          |\n|    entropy_loss       | -0.195   |\n|    explained_variance | 0.943    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3499     |\n|    policy_loss        | -0.287   |\n|    value_loss         | 6.57     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 115      |\n|    ep_rew_mean        | 494      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 3600     |\n|    time_elapsed       | 1573     |\n|    total_timesteps    | 540000   |\n| train/                |          |\n|    entropy_loss       | -0.127   |\n|    explained_variance | 0.048    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3599     |\n|    policy_loss        | -0.0598  |\n|    value_loss         | 6.67e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 119      |\n|    ep_rew_mean        | 536      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 3700     |\n|    time_elapsed       | 1615     |\n|    total_timesteps    | 555000   |\n| train/                |          |\n|    entropy_loss       | -0.142   |\n|    explained_variance | 0.956    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3699     |\n|    policy_loss        | -0.0284  |\n|    value_loss         | 6.02     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 493      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 3800     |\n|    time_elapsed       | 1658     |\n|    total_timesteps    | 570000   |\n| train/                |          |\n|    entropy_loss       | -0.121   |\n|    explained_variance | 0.904    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3799     |\n|    policy_loss        | -0.199   |\n|    value_loss         | 10.7     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 469      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 3900     |\n|    time_elapsed       | 1702     |\n|    total_timesteps    | 585000   |\n| train/                |          |\n|    entropy_loss       | -0.155   |\n|    explained_variance | 0.958    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3899     |\n|    policy_loss        | -0.135   |\n|    value_loss         | 4.38     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 475      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4000     |\n|    time_elapsed       | 1746     |\n|    total_timesteps    | 600000   |\n| train/                |          |\n|    entropy_loss       | -0.0963  |\n|    explained_variance | 0.979    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 3999     |\n|    policy_loss        | -0.00397 |\n|    value_loss         | 4.86     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 491      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4100     |\n|    time_elapsed       | 1789     |\n|    total_timesteps    | 615000   |\n| train/                |          |\n|    entropy_loss       | -0.127   |\n|    explained_variance | 0.937    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4099     |\n|    policy_loss        | -0.944   |\n|    value_loss         | 16.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 527      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4200     |\n|    time_elapsed       | 1833     |\n|    total_timesteps    | 630000   |\n| train/                |          |\n|    entropy_loss       | -0.0751  |\n|    explained_variance | 0.956    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4199     |\n|    policy_loss        | -0.0122  |\n|    value_loss         | 17.9     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 538      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4300     |\n|    time_elapsed       | 1877     |\n|    total_timesteps    | 645000   |\n| train/                |          |\n|    entropy_loss       | -0.104   |\n|    explained_variance | 0.0324   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4299     |\n|    policy_loss        | 0.195    |\n|    value_loss         | 4.34e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 499      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4400     |\n|    time_elapsed       | 1921     |\n|    total_timesteps    | 660000   |\n| train/                |          |\n|    entropy_loss       | -0.153   |\n|    explained_variance | 0.948    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4399     |\n|    policy_loss        | -0.794   |\n|    value_loss         | 13.8     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 364      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4500     |\n|    time_elapsed       | 1965     |\n|    total_timesteps    | 675000   |\n| train/                |          |\n|    entropy_loss       | -0.167   |\n|    explained_variance | 0.941    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4499     |\n|    policy_loss        | -0.366   |\n|    value_loss         | 10.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 595      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4600     |\n|    time_elapsed       | 2009     |\n|    total_timesteps    | 690000   |\n| train/                |          |\n|    entropy_loss       | -0.0624  |\n|    explained_variance | 0.00506  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4599     |\n|    policy_loss        | 3.94     |\n|    value_loss         | 5.53e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 583      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4700     |\n|    time_elapsed       | 2053     |\n|    total_timesteps    | 705000   |\n| train/                |          |\n|    entropy_loss       | -0.145   |\n|    explained_variance | 0.975    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4699     |\n|    policy_loss        | -0.0517  |\n|    value_loss         | 6.6      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 500      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4800     |\n|    time_elapsed       | 2097     |\n|    total_timesteps    | 720000   |\n| train/                |          |\n|    entropy_loss       | -0.105   |\n|    explained_variance | 0.0345   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4799     |\n|    policy_loss        | -0.166   |\n|    value_loss         | 1.31e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 507      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 4900     |\n|    time_elapsed       | 2140     |\n|    total_timesteps    | 735000   |\n| train/                |          |\n|    entropy_loss       | -0.094   |\n|    explained_variance | 0.956    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4899     |\n|    policy_loss        | -0.1     |\n|    value_loss         | 12.4     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 429      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 5000     |\n|    time_elapsed       | 2185     |\n|    total_timesteps    | 750000   |\n| train/                |          |\n|    entropy_loss       | -0.125   |\n|    explained_variance | -0.00301 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 4999     |\n|    policy_loss        | -0.268   |\n|    value_loss         | 1.5e+04  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 518      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 5100     |\n|    time_elapsed       | 2229     |\n|    total_timesteps    | 765000   |\n| train/                |          |\n|    entropy_loss       | -0.101   |\n|    explained_variance | -0.00578 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5099     |\n|    policy_loss        | -0.488   |\n|    value_loss         | 7.62e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 399      |\n| time/                 |          |\n|    fps                | 343      |\n|    iterations         | 5200     |\n|    time_elapsed       | 2273     |\n|    total_timesteps    | 780000   |\n| train/                |          |\n|    entropy_loss       | -0.151   |\n|    explained_variance | 0.0344   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5199     |\n|    policy_loss        | -0.182   |\n|    value_loss         | 2.29e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 484      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 5300     |\n|    time_elapsed       | 2318     |\n|    total_timesteps    | 795000   |\n| train/                |          |\n|    entropy_loss       | -0.108   |\n|    explained_variance | 0.969    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5299     |\n|    policy_loss        | -0.242   |\n|    value_loss         | 6.27     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 107      |\n|    ep_rew_mean        | 546      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 5400     |\n|    time_elapsed       | 2362     |\n|    total_timesteps    | 810000   |\n| train/                |          |\n|    entropy_loss       | -0.0837  |\n|    explained_variance | 0.959    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5399     |\n|    policy_loss        | -0.148   |\n|    value_loss         | 7.25     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 536      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 5500     |\n|    time_elapsed       | 2406     |\n|    total_timesteps    | 825000   |\n| train/                |          |\n|    entropy_loss       | -0.113   |\n|    explained_variance | 0.941    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5499     |\n|    policy_loss        | -0.131   |\n|    value_loss         | 7.86     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 614      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 5600     |\n|    time_elapsed       | 2450     |\n|    total_timesteps    | 840000   |\n| train/                |          |\n|    entropy_loss       | -0.0464  |\n|    explained_variance | 0.0683   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5599     |\n|    policy_loss        | -0.161   |\n|    value_loss         | 3.15e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 602      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 5700     |\n|    time_elapsed       | 2495     |\n|    total_timesteps    | 855000   |\n| train/                |          |\n|    entropy_loss       | -0.0716  |\n|    explained_variance | 0.965    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5699     |\n|    policy_loss        | -0.104   |\n|    value_loss         | 27.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 507      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 5800     |\n|    time_elapsed       | 2539     |\n|    total_timesteps    | 870000   |\n| train/                |          |\n|    entropy_loss       | -0.098   |\n|    explained_variance | 0.932    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5799     |\n|    policy_loss        | -0.15    |\n|    value_loss         | 7.23     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 565      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 5900     |\n|    time_elapsed       | 2584     |\n|    total_timesteps    | 885000   |\n| train/                |          |\n|    entropy_loss       | -0.104   |\n|    explained_variance | -0.00251 |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5899     |\n|    policy_loss        | 8.95     |\n|    value_loss         | 4.84e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 525      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6000     |\n|    time_elapsed       | 2628     |\n|    total_timesteps    | 900000   |\n| train/                |          |\n|    entropy_loss       | -0.0968  |\n|    explained_variance | 0.957    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 5999     |\n|    policy_loss        | 0.0138   |\n|    value_loss         | 11.4     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 747      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6100     |\n|    time_elapsed       | 2672     |\n|    total_timesteps    | 915000   |\n| train/                |          |\n|    entropy_loss       | -0.101   |\n|    explained_variance | 0.957    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6099     |\n|    policy_loss        | -0.145   |\n|    value_loss         | 20.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 709      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6200     |\n|    time_elapsed       | 2716     |\n|    total_timesteps    | 930000   |\n| train/                |          |\n|    entropy_loss       | -0.0565  |\n|    explained_variance | 0.9      |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6199     |\n|    policy_loss        | -0.206   |\n|    value_loss         | 81       |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 526      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6300     |\n|    time_elapsed       | 2760     |\n|    total_timesteps    | 945000   |\n| train/                |          |\n|    entropy_loss       | -0.106   |\n|    explained_variance | 0.942    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6299     |\n|    policy_loss        | -0.311   |\n|    value_loss         | 20.4     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 115      |\n|    ep_rew_mean        | 931      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6400     |\n|    time_elapsed       | 2804     |\n|    total_timesteps    | 960000   |\n| train/                |          |\n|    entropy_loss       | -0.0486  |\n|    explained_variance | -0.0363  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6399     |\n|    policy_loss        | -0.147   |\n|    value_loss         | 2.6e+04  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 694      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6500     |\n|    time_elapsed       | 2849     |\n|    total_timesteps    | 975000   |\n| train/                |          |\n|    entropy_loss       | -0.0254  |\n|    explained_variance | 0.0128   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6499     |\n|    policy_loss        | -0.0847  |\n|    value_loss         | 3.76e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 115      |\n|    ep_rew_mean        | 731      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6600     |\n|    time_elapsed       | 2893     |\n|    total_timesteps    | 990000   |\n| train/                |          |\n|    entropy_loss       | -0.0395  |\n|    explained_variance | 0.26     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6599     |\n|    policy_loss        | 0.253    |\n|    value_loss         | 1.67e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 544      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6700     |\n|    time_elapsed       | 2937     |\n|    total_timesteps    | 1005000  |\n| train/                |          |\n|    entropy_loss       | -0.0638  |\n|    explained_variance | 0.0276   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6699     |\n|    policy_loss        | -0.581   |\n|    value_loss         | 2.74e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 765      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6800     |\n|    time_elapsed       | 2981     |\n|    total_timesteps    | 1020000  |\n| train/                |          |\n|    entropy_loss       | -0.0179  |\n|    explained_variance | 0.346    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6799     |\n|    policy_loss        | -0.123   |\n|    value_loss         | 2.02e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 115      |\n|    ep_rew_mean        | 587      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 6900     |\n|    time_elapsed       | 3025     |\n|    total_timesteps    | 1035000  |\n| train/                |          |\n|    entropy_loss       | -0.0342  |\n|    explained_variance | 0.752    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6899     |\n|    policy_loss        | -0.34    |\n|    value_loss         | 1.24e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 116      |\n|    ep_rew_mean        | 567      |\n| time/                 |          |\n|    fps                | 342      |\n|    iterations         | 7000     |\n|    time_elapsed       | 3069     |\n|    total_timesteps    | 1050000  |\n| train/                |          |\n|    entropy_loss       | -0.0269  |\n|    explained_variance | -0.0495  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 6999     |\n|    policy_loss        | -0.131   |\n|    value_loss         | 4.39e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 627      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 7100     |\n|    time_elapsed       | 3114     |\n|    total_timesteps    | 1065000  |\n| train/                |          |\n|    entropy_loss       | -0.0613  |\n|    explained_variance | 0.962    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7099     |\n|    policy_loss        | -0.028   |\n|    value_loss         | 39.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 743      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 7200     |\n|    time_elapsed       | 3158     |\n|    total_timesteps    | 1080000  |\n| train/                |          |\n|    entropy_loss       | -0.0614  |\n|    explained_variance | 0.951    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7199     |\n|    policy_loss        | -0.145   |\n|    value_loss         | 102      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 119      |\n|    ep_rew_mean        | 778      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 7300     |\n|    time_elapsed       | 3202     |\n|    total_timesteps    | 1095000  |\n| train/                |          |\n|    entropy_loss       | -0.0345  |\n|    explained_variance | 0.213    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7299     |\n|    policy_loss        | -0.275   |\n|    value_loss         | 3.33e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 434      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 7400     |\n|    time_elapsed       | 3247     |\n|    total_timesteps    | 1110000  |\n| train/                |          |\n|    entropy_loss       | -0.0844  |\n|    explained_variance | 0.0105   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7399     |\n|    policy_loss        | 3.11     |\n|    value_loss         | 3.73e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 115      |\n|    ep_rew_mean        | 584      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 7500     |\n|    time_elapsed       | 3291     |\n|    total_timesteps    | 1125000  |\n| train/                |          |\n|    entropy_loss       | -0.0813  |\n|    explained_variance | 0.0535   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7499     |\n|    policy_loss        | -0.089   |\n|    value_loss         | 7.22e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 112      |\n|    ep_rew_mean        | 639      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 7600     |\n|    time_elapsed       | 3335     |\n|    total_timesteps    | 1140000  |\n| train/                |          |\n|    entropy_loss       | -0.0749  |\n|    explained_variance | 0.106    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7599     |\n|    policy_loss        | -0.674   |\n|    value_loss         | 2.18e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 120      |\n|    ep_rew_mean        | 563      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 7700     |\n|    time_elapsed       | 3379     |\n|    total_timesteps    | 1155000  |\n| train/                |          |\n|    entropy_loss       | -0.0739  |\n|    explained_variance | 0.927    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7699     |\n|    policy_loss        | 0.0629   |\n|    value_loss         | 85.9     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 119      |\n|    ep_rew_mean        | 519      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 7800     |\n|    time_elapsed       | 3424     |\n|    total_timesteps    | 1170000  |\n| train/                |          |\n|    entropy_loss       | -0.111   |\n|    explained_variance | 0.0569   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7799     |\n|    policy_loss        | -0.313   |\n|    value_loss         | 3.91e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 616      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 7900     |\n|    time_elapsed       | 3468     |\n|    total_timesteps    | 1185000  |\n| train/                |          |\n|    entropy_loss       | -0.0614  |\n|    explained_variance | 0.978    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7899     |\n|    policy_loss        | -0.0835  |\n|    value_loss         | 16.9     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 118      |\n|    ep_rew_mean        | 817      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8000     |\n|    time_elapsed       | 3512     |\n|    total_timesteps    | 1200000  |\n| train/                |          |\n|    entropy_loss       | -0.0435  |\n|    explained_variance | 0.955    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 7999     |\n|    policy_loss        | -0.345   |\n|    value_loss         | 477      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 118      |\n|    ep_rew_mean        | 651      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8100     |\n|    time_elapsed       | 3556     |\n|    total_timesteps    | 1215000  |\n| train/                |          |\n|    entropy_loss       | -0.0391  |\n|    explained_variance | 0.208    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8099     |\n|    policy_loss        | -0.337   |\n|    value_loss         | 5.08e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 121      |\n|    ep_rew_mean        | 579      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8200     |\n|    time_elapsed       | 3600     |\n|    total_timesteps    | 1230000  |\n| train/                |          |\n|    entropy_loss       | -0.0404  |\n|    explained_variance | 0.949    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8199     |\n|    policy_loss        | -0.197   |\n|    value_loss         | 316      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 125      |\n|    ep_rew_mean        | 679      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8300     |\n|    time_elapsed       | 3644     |\n|    total_timesteps    | 1245000  |\n| train/                |          |\n|    entropy_loss       | -0.037   |\n|    explained_variance | 0.175    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8299     |\n|    policy_loss        | -0.0561  |\n|    value_loss         | 2.67e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 123      |\n|    ep_rew_mean        | 643      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8400     |\n|    time_elapsed       | 3688     |\n|    total_timesteps    | 1260000  |\n| train/                |          |\n|    entropy_loss       | -0.0416  |\n|    explained_variance | 0.965    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8399     |\n|    policy_loss        | -0.298   |\n|    value_loss         | 315      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 115      |\n|    ep_rew_mean        | 693      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8500     |\n|    time_elapsed       | 3733     |\n|    total_timesteps    | 1275000  |\n| train/                |          |\n|    entropy_loss       | -0.0633  |\n|    explained_variance | 0.222    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8499     |\n|    policy_loss        | 0.0988   |\n|    value_loss         | 2.03e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 452      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8600     |\n|    time_elapsed       | 3778     |\n|    total_timesteps    | 1290000  |\n| train/                |          |\n|    entropy_loss       | -0.132   |\n|    explained_variance | 0.9      |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8599     |\n|    policy_loss        | -0.0766  |\n|    value_loss         | 22.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 651      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8700     |\n|    time_elapsed       | 3822     |\n|    total_timesteps    | 1305000  |\n| train/                |          |\n|    entropy_loss       | -0.0327  |\n|    explained_variance | 0.951    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8699     |\n|    policy_loss        | 0.071    |\n|    value_loss         | 258      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 528      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8800     |\n|    time_elapsed       | 3867     |\n|    total_timesteps    | 1320000  |\n| train/                |          |\n|    entropy_loss       | -0.0457  |\n|    explained_variance | 0.135    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8799     |\n|    policy_loss        | -0.181   |\n|    value_loss         | 2.61e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 492      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 8900     |\n|    time_elapsed       | 3911     |\n|    total_timesteps    | 1335000  |\n| train/                |          |\n|    entropy_loss       | -0.101   |\n|    explained_variance | 0.0233   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8899     |\n|    policy_loss        | -0.0677  |\n|    value_loss         | 2.21e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 435      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 9000     |\n|    time_elapsed       | 3956     |\n|    total_timesteps    | 1350000  |\n| train/                |          |\n|    entropy_loss       | -0.107   |\n|    explained_variance | 0.0339   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 8999     |\n|    policy_loss        | -0.00291 |\n|    value_loss         | 5.58e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 109      |\n|    ep_rew_mean        | 668      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 9100     |\n|    time_elapsed       | 4001     |\n|    total_timesteps    | 1365000  |\n| train/                |          |\n|    entropy_loss       | -0.0198  |\n|    explained_variance | 0.913    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9099     |\n|    policy_loss        | -0.0197  |\n|    value_loss         | 190      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 544      |\n| time/                 |          |\n|    fps                | 341      |\n|    iterations         | 9200     |\n|    time_elapsed       | 4046     |\n|    total_timesteps    | 1380000  |\n| train/                |          |\n|    entropy_loss       | -0.0934  |\n|    explained_variance | 0.038    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9199     |\n|    policy_loss        | -0.409   |\n|    value_loss         | 1.98e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 89.7     |\n|    ep_rew_mean        | 428      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 9300     |\n|    time_elapsed       | 4092     |\n|    total_timesteps    | 1395000  |\n| train/                |          |\n|    entropy_loss       | -0.0941  |\n|    explained_variance | 0.915    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9299     |\n|    policy_loss        | -0.314   |\n|    value_loss         | 31.6     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 96.1     |\n|    ep_rew_mean        | 421      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 9400     |\n|    time_elapsed       | 4137     |\n|    total_timesteps    | 1410000  |\n| train/                |          |\n|    entropy_loss       | -0.104   |\n|    explained_variance | 0.9      |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9399     |\n|    policy_loss        | 0.0804   |\n|    value_loss         | 21       |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 107      |\n|    ep_rew_mean        | 630      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 9500     |\n|    time_elapsed       | 4182     |\n|    total_timesteps    | 1425000  |\n| train/                |          |\n|    entropy_loss       | -0.0536  |\n|    explained_variance | 0.943    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9499     |\n|    policy_loss        | -0.441   |\n|    value_loss         | 95.6     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 616      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 9600     |\n|    time_elapsed       | 4227     |\n|    total_timesteps    | 1440000  |\n| train/                |          |\n|    entropy_loss       | -0.0839  |\n|    explained_variance | -0.0118  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9599     |\n|    policy_loss        | -0.111   |\n|    value_loss         | 2.13e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 752      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 9700     |\n|    time_elapsed       | 4271     |\n|    total_timesteps    | 1455000  |\n| train/                |          |\n|    entropy_loss       | -0.0178  |\n|    explained_variance | 0.0646   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9699     |\n|    policy_loss        | -0.0101  |\n|    value_loss         | 6.28e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 111      |\n|    ep_rew_mean        | 672      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 9800     |\n|    time_elapsed       | 4316     |\n|    total_timesteps    | 1470000  |\n| train/                |          |\n|    entropy_loss       | -0.0559  |\n|    explained_variance | 0.252    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9799     |\n|    policy_loss        | -0.0456  |\n|    value_loss         | 1.93e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 781      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 9900     |\n|    time_elapsed       | 4361     |\n|    total_timesteps    | 1485000  |\n| train/                |          |\n|    entropy_loss       | -0.00892 |\n|    explained_variance | 0.939    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9899     |\n|    policy_loss        | -0.0461  |\n|    value_loss         | 647      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 886      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 10000    |\n|    time_elapsed       | 4406     |\n|    total_timesteps    | 1500000  |\n| train/                |          |\n|    entropy_loss       | -0.0222  |\n|    explained_variance | 0.862    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 9999     |\n|    policy_loss        | -0.304   |\n|    value_loss         | 1.18e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 751      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 10100    |\n|    time_elapsed       | 4451     |\n|    total_timesteps    | 1515000  |\n| train/                |          |\n|    entropy_loss       | -0.021   |\n|    explained_variance | 0.219    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10099    |\n|    policy_loss        | -0.508   |\n|    value_loss         | 3.45e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 633      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 10200    |\n|    time_elapsed       | 4497     |\n|    total_timesteps    | 1530000  |\n| train/                |          |\n|    entropy_loss       | -0.0262  |\n|    explained_variance | 0.516    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10199    |\n|    policy_loss        | -0.479   |\n|    value_loss         | 1.39e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 810      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 10300    |\n|    time_elapsed       | 4542     |\n|    total_timesteps    | 1545000  |\n| train/                |          |\n|    entropy_loss       | -0.0204  |\n|    explained_variance | 0.893    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10299    |\n|    policy_loss        | -1.79    |\n|    value_loss         | 1.22e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 781      |\n| time/                 |          |\n|    fps                | 340      |\n|    iterations         | 10400    |\n|    time_elapsed       | 4587     |\n|    total_timesteps    | 1560000  |\n| train/                |          |\n|    entropy_loss       | -0.0135  |\n|    explained_variance | 0.411    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10399    |\n|    policy_loss        | -0.22    |\n|    value_loss         | 1.33e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 774      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 10500    |\n|    time_elapsed       | 4633     |\n|    total_timesteps    | 1575000  |\n| train/                |          |\n|    entropy_loss       | -0.0117  |\n|    explained_variance | 0.615    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10499    |\n|    policy_loss        | -0.336   |\n|    value_loss         | 1.33e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 94.6     |\n|    ep_rew_mean        | 334      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 10600    |\n|    time_elapsed       | 4679     |\n|    total_timesteps    | 1590000  |\n| train/                |          |\n|    entropy_loss       | -0.05    |\n|    explained_variance | 0.638    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10599    |\n|    policy_loss        | -0.584   |\n|    value_loss         | 94.4     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 80.3     |\n|    ep_rew_mean        | 339      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 10700    |\n|    time_elapsed       | 4725     |\n|    total_timesteps    | 1605000  |\n| train/                |          |\n|    entropy_loss       | -0.083   |\n|    explained_variance | 0.873    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10699    |\n|    policy_loss        | -0.0483  |\n|    value_loss         | 51.7     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 623      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 10800    |\n|    time_elapsed       | 4771     |\n|    total_timesteps    | 1620000  |\n| train/                |          |\n|    entropy_loss       | -0.0226  |\n|    explained_variance | 0.93     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10799    |\n|    policy_loss        | -0.00166 |\n|    value_loss         | 274      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 421      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 10900    |\n|    time_elapsed       | 4815     |\n|    total_timesteps    | 1635000  |\n| train/                |          |\n|    entropy_loss       | -0.0573  |\n|    explained_variance | 0.574    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10899    |\n|    policy_loss        | 0.0157   |\n|    value_loss         | 127      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 107      |\n|    ep_rew_mean        | 592      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11000    |\n|    time_elapsed       | 4860     |\n|    total_timesteps    | 1650000  |\n| train/                |          |\n|    entropy_loss       | -0.077   |\n|    explained_variance | 0.113    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 10999    |\n|    policy_loss        | 0.35     |\n|    value_loss         | 2.36e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 541      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11100    |\n|    time_elapsed       | 4905     |\n|    total_timesteps    | 1665000  |\n| train/                |          |\n|    entropy_loss       | -0.0488  |\n|    explained_variance | 0.898    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11099    |\n|    policy_loss        | -0.0395  |\n|    value_loss         | 179      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 93.4     |\n|    ep_rew_mean        | 450      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11200    |\n|    time_elapsed       | 4950     |\n|    total_timesteps    | 1680000  |\n| train/                |          |\n|    entropy_loss       | -0.0962  |\n|    explained_variance | 0.895    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11199    |\n|    policy_loss        | 0.149    |\n|    value_loss         | 18.1     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 662      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11300    |\n|    time_elapsed       | 4996     |\n|    total_timesteps    | 1695000  |\n| train/                |          |\n|    entropy_loss       | -0.0438  |\n|    explained_variance | 0.243    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11299    |\n|    policy_loss        | -0.545   |\n|    value_loss         | 3e+04    |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 696      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11400    |\n|    time_elapsed       | 5040     |\n|    total_timesteps    | 1710000  |\n| train/                |          |\n|    entropy_loss       | -0.00449 |\n|    explained_variance | 0.857    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11399    |\n|    policy_loss        | -0.727   |\n|    value_loss         | 1.01e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 110      |\n|    ep_rew_mean        | 880      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11500    |\n|    time_elapsed       | 5085     |\n|    total_timesteps    | 1725000  |\n| train/                |          |\n|    entropy_loss       | -0.00906 |\n|    explained_variance | 0.391    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11499    |\n|    policy_loss        | -0.0614  |\n|    value_loss         | 1.82e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 111      |\n|    ep_rew_mean        | 688      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11600    |\n|    time_elapsed       | 5130     |\n|    total_timesteps    | 1740000  |\n| train/                |          |\n|    entropy_loss       | -0.0175  |\n|    explained_variance | 0.928    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11599    |\n|    policy_loss        | -0.00963 |\n|    value_loss         | 486      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 606      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11700    |\n|    time_elapsed       | 5175     |\n|    total_timesteps    | 1755000  |\n| train/                |          |\n|    entropy_loss       | -0.0165  |\n|    explained_variance | 0.353    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11699    |\n|    policy_loss        | 0.0298   |\n|    value_loss         | 3.66e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 602      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11800    |\n|    time_elapsed       | 5220     |\n|    total_timesteps    | 1770000  |\n| train/                |          |\n|    entropy_loss       | -0.0145  |\n|    explained_variance | 0.911    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11799    |\n|    policy_loss        | -0.01    |\n|    value_loss         | 579      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 846      |\n| time/                 |          |\n|    fps                | 339      |\n|    iterations         | 11900    |\n|    time_elapsed       | 5265     |\n|    total_timesteps    | 1785000  |\n| train/                |          |\n|    entropy_loss       | -0.0112  |\n|    explained_variance | 0.915    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11899    |\n|    policy_loss        | -0.281   |\n|    value_loss         | 1.4e+03  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 689      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12000    |\n|    time_elapsed       | 5310     |\n|    total_timesteps    | 1800000  |\n| train/                |          |\n|    entropy_loss       | -0.0218  |\n|    explained_variance | 0.95     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 11999    |\n|    policy_loss        | -0.0617  |\n|    value_loss         | 1.17e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 89.5     |\n|    ep_rew_mean        | 637      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12100    |\n|    time_elapsed       | 5356     |\n|    total_timesteps    | 1815000  |\n| train/                |          |\n|    entropy_loss       | -0.0218  |\n|    explained_variance | 0.173    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12099    |\n|    policy_loss        | 0.0234   |\n|    value_loss         | 5.88e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 85       |\n|    ep_rew_mean        | 537      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12200    |\n|    time_elapsed       | 5402     |\n|    total_timesteps    | 1830000  |\n| train/                |          |\n|    entropy_loss       | -0.0207  |\n|    explained_variance | 0.417    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12199    |\n|    policy_loss        | -0.206   |\n|    value_loss         | 1.12e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 98.8     |\n|    ep_rew_mean        | 594      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12300    |\n|    time_elapsed       | 5448     |\n|    total_timesteps    | 1845000  |\n| train/                |          |\n|    entropy_loss       | -0.0168  |\n|    explained_variance | 0.892    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12299    |\n|    policy_loss        | 0.142    |\n|    value_loss         | 400      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 95.3     |\n|    ep_rew_mean        | 636      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12400    |\n|    time_elapsed       | 5494     |\n|    total_timesteps    | 1860000  |\n| train/                |          |\n|    entropy_loss       | -0.0241  |\n|    explained_variance | 0.228    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12399    |\n|    policy_loss        | -0.142   |\n|    value_loss         | 4.19e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 689      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12500    |\n|    time_elapsed       | 5539     |\n|    total_timesteps    | 1875000  |\n| train/                |          |\n|    entropy_loss       | -0.0198  |\n|    explained_variance | 0.448    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12499    |\n|    policy_loss        | -0.323   |\n|    value_loss         | 3.21e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 497      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12600    |\n|    time_elapsed       | 5585     |\n|    total_timesteps    | 1890000  |\n| train/                |          |\n|    entropy_loss       | -0.0246  |\n|    explained_variance | 0.772    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12599    |\n|    policy_loss        | -0.0694  |\n|    value_loss         | 134      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 99.5     |\n|    ep_rew_mean        | 480      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12700    |\n|    time_elapsed       | 5631     |\n|    total_timesteps    | 1905000  |\n| train/                |          |\n|    entropy_loss       | -0.0407  |\n|    explained_variance | 0.769    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12699    |\n|    policy_loss        | -0.444   |\n|    value_loss         | 1.36e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 625      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12800    |\n|    time_elapsed       | 5677     |\n|    total_timesteps    | 1920000  |\n| train/                |          |\n|    entropy_loss       | -0.0283  |\n|    explained_variance | 0.165    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12799    |\n|    policy_loss        | -0.121   |\n|    value_loss         | 1.98e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 85.8     |\n|    ep_rew_mean        | 453      |\n| time/                 |          |\n|    fps                | 338      |\n|    iterations         | 12900    |\n|    time_elapsed       | 5723     |\n|    total_timesteps    | 1935000  |\n| train/                |          |\n|    entropy_loss       | -0.0368  |\n|    explained_variance | 0.328    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12899    |\n|    policy_loss        | -0.124   |\n|    value_loss         | 2.72e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 873      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13000    |\n|    time_elapsed       | 5769     |\n|    total_timesteps    | 1950000  |\n| train/                |          |\n|    entropy_loss       | -0.0148  |\n|    explained_variance | 0.832    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 12999    |\n|    policy_loss        | -0.0391  |\n|    value_loss         | 1.12e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 97.6     |\n|    ep_rew_mean        | 641      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13100    |\n|    time_elapsed       | 5815     |\n|    total_timesteps    | 1965000  |\n| train/                |          |\n|    entropy_loss       | -0.00419 |\n|    explained_variance | 0.745    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13099    |\n|    policy_loss        | 0.0702   |\n|    value_loss         | 2.67e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 96.8     |\n|    ep_rew_mean        | 551      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13200    |\n|    time_elapsed       | 5861     |\n|    total_timesteps    | 1980000  |\n| train/                |          |\n|    entropy_loss       | -0.0182  |\n|    explained_variance | 0.239    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13199    |\n|    policy_loss        | 0.0369   |\n|    value_loss         | 4.53e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 861      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13300    |\n|    time_elapsed       | 5906     |\n|    total_timesteps    | 1995000  |\n| train/                |          |\n|    entropy_loss       | -0.0142  |\n|    explained_variance | 0.39     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13299    |\n|    policy_loss        | -0.21    |\n|    value_loss         | 3.6e+04  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 658      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13400    |\n|    time_elapsed       | 5951     |\n|    total_timesteps    | 2010000  |\n| train/                |          |\n|    entropy_loss       | -0.0204  |\n|    explained_variance | 0.429    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13399    |\n|    policy_loss        | -0.784   |\n|    value_loss         | 5.51e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 107      |\n|    ep_rew_mean        | 661      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13500    |\n|    time_elapsed       | 5997     |\n|    total_timesteps    | 2025000  |\n| train/                |          |\n|    entropy_loss       | -0.0225  |\n|    explained_variance | 0.907    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13499    |\n|    policy_loss        | -0.0594  |\n|    value_loss         | 902      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 767      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13600    |\n|    time_elapsed       | 6043     |\n|    total_timesteps    | 2040000  |\n| train/                |          |\n|    entropy_loss       | -0.017   |\n|    explained_variance | 0.928    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13599    |\n|    policy_loss        | -0.133   |\n|    value_loss         | 755      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 715      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13700    |\n|    time_elapsed       | 6088     |\n|    total_timesteps    | 2055000  |\n| train/                |          |\n|    entropy_loss       | -0.00813 |\n|    explained_variance | 0.368    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13699    |\n|    policy_loss        | -0.109   |\n|    value_loss         | 1.83e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 108      |\n|    ep_rew_mean        | 818      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13800    |\n|    time_elapsed       | 6134     |\n|    total_timesteps    | 2070000  |\n| train/                |          |\n|    entropy_loss       | -0.0108  |\n|    explained_variance | 0.738    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13799    |\n|    policy_loss        | 1.83     |\n|    value_loss         | 4.65e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 103      |\n|    ep_rew_mean        | 559      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 13900    |\n|    time_elapsed       | 6180     |\n|    total_timesteps    | 2085000  |\n| train/                |          |\n|    entropy_loss       | -0.0132  |\n|    explained_variance | 0.219    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13899    |\n|    policy_loss        | -0.183   |\n|    value_loss         | 3.09e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 93.3     |\n|    ep_rew_mean        | 637      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 14000    |\n|    time_elapsed       | 6226     |\n|    total_timesteps    | 2100000  |\n| train/                |          |\n|    entropy_loss       | -0.00919 |\n|    explained_variance | 0.642    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 13999    |\n|    policy_loss        | -0.571   |\n|    value_loss         | 2.01e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 745      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 14100    |\n|    time_elapsed       | 6272     |\n|    total_timesteps    | 2115000  |\n| train/                |          |\n|    entropy_loss       | -0.0227  |\n|    explained_variance | 0.877    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14099    |\n|    policy_loss        | -0.138   |\n|    value_loss         | 908      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 104      |\n|    ep_rew_mean        | 847      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 14200    |\n|    time_elapsed       | 6318     |\n|    total_timesteps    | 2130000  |\n| train/                |          |\n|    entropy_loss       | -0.0102  |\n|    explained_variance | 0.845    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14199    |\n|    policy_loss        | -0.224   |\n|    value_loss         | 2.25e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 788      |\n| time/                 |          |\n|    fps                | 337      |\n|    iterations         | 14300    |\n|    time_elapsed       | 6364     |\n|    total_timesteps    | 2145000  |\n| train/                |          |\n|    entropy_loss       | -0.00614 |\n|    explained_variance | 0.0253   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14299    |\n|    policy_loss        | 0.0998   |\n|    value_loss         | 3.21e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 87.1     |\n|    ep_rew_mean        | 423      |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 14400    |\n|    time_elapsed       | 6412     |\n|    total_timesteps    | 2160000  |\n| train/                |          |\n|    entropy_loss       | -0.00756 |\n|    explained_variance | 0.473    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14399    |\n|    policy_loss        | -0.0057  |\n|    value_loss         | 3.49e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 92.6     |\n|    ep_rew_mean        | 586      |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 14500    |\n|    time_elapsed       | 6460     |\n|    total_timesteps    | 2175000  |\n| train/                |          |\n|    entropy_loss       | -0.019   |\n|    explained_variance | 0.409    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14499    |\n|    policy_loss        | -0.115   |\n|    value_loss         | 3.14e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 90.9     |\n|    ep_rew_mean        | 624      |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 14600    |\n|    time_elapsed       | 6508     |\n|    total_timesteps    | 2190000  |\n| train/                |          |\n|    entropy_loss       | -0.0148  |\n|    explained_variance | 0.932    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14599    |\n|    policy_loss        | 0.0445   |\n|    value_loss         | 1.08e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 77.5     |\n|    ep_rew_mean        | 279      |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 14700    |\n|    time_elapsed       | 6555     |\n|    total_timesteps    | 2205000  |\n| train/                |          |\n|    entropy_loss       | -0.019   |\n|    explained_variance | 0.737    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14699    |\n|    policy_loss        | 0.0307   |\n|    value_loss         | 909      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 77       |\n|    ep_rew_mean        | 110      |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 14800    |\n|    time_elapsed       | 6603     |\n|    total_timesteps    | 2220000  |\n| train/                |          |\n|    entropy_loss       | -0.0695  |\n|    explained_variance | 0.442    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14799    |\n|    policy_loss        | -0.0497  |\n|    value_loss         | 45.7     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 323      |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 14900    |\n|    time_elapsed       | 6649     |\n|    total_timesteps    | 2235000  |\n| train/                |          |\n|    entropy_loss       | -0.0211  |\n|    explained_variance | 0.243    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14899    |\n|    policy_loss        | -0.257   |\n|    value_loss         | 1.93e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 105      |\n|    ep_rew_mean        | 455      |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 15000    |\n|    time_elapsed       | 6695     |\n|    total_timesteps    | 2250000  |\n| train/                |          |\n|    entropy_loss       | -0.0123  |\n|    explained_variance | 0.645    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 14999    |\n|    policy_loss        | -0.0536  |\n|    value_loss         | 824      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 111      |\n|    ep_rew_mean        | 835      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 15100    |\n|    time_elapsed       | 6741     |\n|    total_timesteps    | 2265000  |\n| train/                |          |\n|    entropy_loss       | -0.0135  |\n|    explained_variance | 0.82     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 15099    |\n|    policy_loss        | -0.11    |\n|    value_loss         | 2.28e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 781      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 15200    |\n|    time_elapsed       | 6786     |\n|    total_timesteps    | 2280000  |\n| train/                |          |\n|    entropy_loss       | -0.0213  |\n|    explained_variance | 0.929    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 15199    |\n|    policy_loss        | -0.283   |\n|    value_loss         | 1.78e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 112      |\n|    ep_rew_mean        | 575      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 15300    |\n|    time_elapsed       | 6832     |\n|    total_timesteps    | 2295000  |\n| train/                |          |\n|    entropy_loss       | -0.0284  |\n|    explained_variance | 0.377    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 15299    |\n|    policy_loss        | 0.0494   |\n|    value_loss         | 3.02e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 115      |\n|    ep_rew_mean        | 613      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 15400    |\n|    time_elapsed       | 6878     |\n|    total_timesteps    | 2310000  |\n| train/                |          |\n|    entropy_loss       | -0.02    |\n|    explained_variance | 0.918    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 15399    |\n|    policy_loss        | -0.344   |\n|    value_loss         | 776      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 118      |\n|    ep_rew_mean        | 758      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 15500    |\n|    time_elapsed       | 6923     |\n|    total_timesteps    | 2325000  |\n| train/                |          |\n|    entropy_loss       | -0.0104  |\n|    explained_variance | 0.417    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 15499    |\n|    policy_loss        | -0.134   |\n|    value_loss         | 4.42e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 117      |\n|    ep_rew_mean        | 801      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 15600    |\n|    time_elapsed       | 6969     |\n|    total_timesteps    | 2340000  |\n| train/                |          |\n|    entropy_loss       | -0.00892 |\n|    explained_variance | 0.423    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 15599    |\n|    policy_loss        | -0.0236  |\n|    value_loss         | 1.68e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 734      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 15700    |\n|    time_elapsed       | 7015     |\n|    total_timesteps    | 2355000  |\n| train/                |          |\n|    entropy_loss       | -0.0107  |\n|    explained_variance | 0.824    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 15699    |\n|    policy_loss        | -0.177   |\n|    value_loss         | 5.52e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 118      |\n|    ep_rew_mean        | 662      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 15800    |\n|    time_elapsed       | 7060     |\n|    total_timesteps    | 2370000  |\n| train/                |          |\n|    entropy_loss       | -0.0104  |\n|    explained_variance | 0.501    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 15799    |\n|    policy_loss        | -0.192   |\n|    value_loss         | 643      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 143      |\n|    ep_rew_mean        | 89       |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 15900    |\n|    time_elapsed       | 7103     |\n|    total_timesteps    | 2385000  |\n| train/                |          |\n|    entropy_loss       | -0.0118  |\n|    explained_variance | 0.918    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 15899    |\n|    policy_loss        | 0.0685   |\n|    value_loss         | 174      |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 139       |\n|    ep_rew_mean        | 44.8      |\n| time/                 |           |\n|    fps                | 335       |\n|    iterations         | 16000     |\n|    time_elapsed       | 7147      |\n|    total_timesteps    | 2400000   |\n| train/                |           |\n|    entropy_loss       | -1.11e-05 |\n|    explained_variance | 0.818     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 15999     |\n|    policy_loss        | -5.52e-06 |\n|    value_loss         | 594       |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 149      |\n|    ep_rew_mean        | 76       |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 16100    |\n|    time_elapsed       | 7191     |\n|    total_timesteps    | 2415000  |\n| train/                |          |\n|    entropy_loss       | -0.0132  |\n|    explained_variance | 0.85     |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 16099    |\n|    policy_loss        | -0.139   |\n|    value_loss         | 345      |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 143       |\n|    ep_rew_mean        | 19.6      |\n| time/                 |           |\n|    fps                | 335       |\n|    iterations         | 16200     |\n|    time_elapsed       | 7235      |\n|    total_timesteps    | 2430000   |\n| train/                |           |\n|    entropy_loss       | -9.71e-08 |\n|    explained_variance | 0.91      |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 16199     |\n|    policy_loss        | -0        |\n|    value_loss         | 332       |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 152      |\n|    ep_rew_mean        | 47.5     |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 16300    |\n|    time_elapsed       | 7278     |\n|    total_timesteps    | 2445000  |\n| train/                |          |\n|    entropy_loss       | -0.00454 |\n|    explained_variance | 0.905    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 16299    |\n|    policy_loss        | 0.024    |\n|    value_loss         | 130      |\n------------------------------------\n-------------------------------------\n| rollout/              |           |\n|    ep_len_mean        | 157       |\n|    ep_rew_mean        | 58.2      |\n| time/                 |           |\n|    fps                | 336       |\n|    iterations         | 16500     |\n|    time_elapsed       | 7365      |\n|    total_timesteps    | 2475000   |\n| train/                |           |\n|    entropy_loss       | -4.88e-06 |\n|    explained_variance | 0.88      |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 16499     |\n|    policy_loss        | 7.77e-07  |\n|    value_loss         | 341       |\n-------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 147      |\n|    ep_rew_mean        | 66.5     |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 16600    |\n|    time_elapsed       | 7408     |\n|    total_timesteps    | 2490000  |\n| train/                |          |\n|    entropy_loss       | -0.00411 |\n|    explained_variance | 0.884    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 16599    |\n|    policy_loss        | 0.0397   |\n|    value_loss         | 135      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 117      |\n|    ep_rew_mean        | 218      |\n| time/                 |          |\n|    fps                | 336      |\n|    iterations         | 16700    |\n|    time_elapsed       | 7454     |\n|    total_timesteps    | 2505000  |\n| train/                |          |\n|    entropy_loss       | -0.0882  |\n|    explained_variance | 0.0113   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 16699    |\n|    policy_loss        | 12.2     |\n|    value_loss         | 3.99e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 78.3     |\n|    ep_rew_mean        | 183      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 16800    |\n|    time_elapsed       | 7501     |\n|    total_timesteps    | 2520000  |\n| train/                |          |\n|    entropy_loss       | -0.025   |\n|    explained_variance | 0.00915  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 16799    |\n|    policy_loss        | 0.691    |\n|    value_loss         | 3.66e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 98.8     |\n|    ep_rew_mean        | 238      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 16900    |\n|    time_elapsed       | 7547     |\n|    total_timesteps    | 2535000  |\n| train/                |          |\n|    entropy_loss       | -0.0311  |\n|    explained_variance | 0.387    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 16899    |\n|    policy_loss        | -0.273   |\n|    value_loss         | 351      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 93.2     |\n|    ep_rew_mean        | 445      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17000    |\n|    time_elapsed       | 7593     |\n|    total_timesteps    | 2550000  |\n| train/                |          |\n|    entropy_loss       | -0.0203  |\n|    explained_variance | 0.764    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 16999    |\n|    policy_loss        | -0.0791  |\n|    value_loss         | 56.3     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 93.5     |\n|    ep_rew_mean        | 518      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17100    |\n|    time_elapsed       | 7640     |\n|    total_timesteps    | 2565000  |\n| train/                |          |\n|    entropy_loss       | -0.0452  |\n|    explained_variance | 0.864    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17099    |\n|    policy_loss        | -0.0654  |\n|    value_loss         | 70.2     |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 80.5     |\n|    ep_rew_mean        | 588      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17200    |\n|    time_elapsed       | 7687     |\n|    total_timesteps    | 2580000  |\n| train/                |          |\n|    entropy_loss       | -0.0341  |\n|    explained_variance | 0.904    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17199    |\n|    policy_loss        | -0.0206  |\n|    value_loss         | 387      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 93.8     |\n|    ep_rew_mean        | 705      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17300    |\n|    time_elapsed       | 7735     |\n|    total_timesteps    | 2595000  |\n| train/                |          |\n|    entropy_loss       | -0.00756 |\n|    explained_variance | 0.188    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17299    |\n|    policy_loss        | -1.05    |\n|    value_loss         | 3.96e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 106      |\n|    ep_rew_mean        | 862      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17400    |\n|    time_elapsed       | 7781     |\n|    total_timesteps    | 2610000  |\n| train/                |          |\n|    entropy_loss       | -0.0178  |\n|    explained_variance | 0.354    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17399    |\n|    policy_loss        | -0.0412  |\n|    value_loss         | 3.39e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 99.7     |\n|    ep_rew_mean        | 645      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17500    |\n|    time_elapsed       | 7828     |\n|    total_timesteps    | 2625000  |\n| train/                |          |\n|    entropy_loss       | -0.0289  |\n|    explained_variance | 0.844    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17499    |\n|    policy_loss        | -0.0526  |\n|    value_loss         | 1.95e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 833      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17600    |\n|    time_elapsed       | 7874     |\n|    total_timesteps    | 2640000  |\n| train/                |          |\n|    entropy_loss       | -0.01    |\n|    explained_variance | 0.328    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17599    |\n|    policy_loss        | -0.113   |\n|    value_loss         | 3.67e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 809      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17700    |\n|    time_elapsed       | 7920     |\n|    total_timesteps    | 2655000  |\n| train/                |          |\n|    entropy_loss       | -0.0196  |\n|    explained_variance | 0.504    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17699    |\n|    policy_loss        | -0.426   |\n|    value_loss         | 3.3e+04  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 919      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17800    |\n|    time_elapsed       | 7967     |\n|    total_timesteps    | 2670000  |\n| train/                |          |\n|    entropy_loss       | -0.0149  |\n|    explained_variance | 0.734    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17799    |\n|    policy_loss        | 0.0735   |\n|    value_loss         | 7.61e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 733      |\n| time/                 |          |\n|    fps                | 335      |\n|    iterations         | 17900    |\n|    time_elapsed       | 8013     |\n|    total_timesteps    | 2685000  |\n| train/                |          |\n|    entropy_loss       | -0.0187  |\n|    explained_variance | 0.838    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17899    |\n|    policy_loss        | -2.75    |\n|    value_loss         | 2.57e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 98.3     |\n|    ep_rew_mean        | 524      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18000    |\n|    time_elapsed       | 8059     |\n|    total_timesteps    | 2700000  |\n| train/                |          |\n|    entropy_loss       | -0.0143  |\n|    explained_variance | 0.796    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 17999    |\n|    policy_loss        | 0.0747   |\n|    value_loss         | 1.67e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 86.5     |\n|    ep_rew_mean        | 644      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18100    |\n|    time_elapsed       | 8107     |\n|    total_timesteps    | 2715000  |\n| train/                |          |\n|    entropy_loss       | -0.0234  |\n|    explained_variance | 0.461    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18099    |\n|    policy_loss        | -0.2     |\n|    value_loss         | 2.95e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 99       |\n|    ep_rew_mean        | 804      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18200    |\n|    time_elapsed       | 8153     |\n|    total_timesteps    | 2730000  |\n| train/                |          |\n|    entropy_loss       | -0.0116  |\n|    explained_variance | 0.481    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18199    |\n|    policy_loss        | -0.222   |\n|    value_loss         | 3.62e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 102      |\n|    ep_rew_mean        | 869      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18300    |\n|    time_elapsed       | 8200     |\n|    total_timesteps    | 2745000  |\n| train/                |          |\n|    entropy_loss       | -0.0202  |\n|    explained_variance | 0.664    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18299    |\n|    policy_loss        | 3.77     |\n|    value_loss         | 2.99e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 95.2     |\n|    ep_rew_mean        | 607      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18400    |\n|    time_elapsed       | 8246     |\n|    total_timesteps    | 2760000  |\n| train/                |          |\n|    entropy_loss       | -0.00744 |\n|    explained_variance | 0.821    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18399    |\n|    policy_loss        | -0.064   |\n|    value_loss         | 2.36e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 1.07e+03 |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18500    |\n|    time_elapsed       | 8292     |\n|    total_timesteps    | 2775000  |\n| train/                |          |\n|    entropy_loss       | -0.00289 |\n|    explained_variance | 0.653    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18499    |\n|    policy_loss        | -0.0232  |\n|    value_loss         | 4.36e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 101      |\n|    ep_rew_mean        | 825      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18600    |\n|    time_elapsed       | 8339     |\n|    total_timesteps    | 2790000  |\n| train/                |          |\n|    entropy_loss       | -0.0157  |\n|    explained_variance | -0.0762  |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18599    |\n|    policy_loss        | -3.43    |\n|    value_loss         | 1.26e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 85.6     |\n|    ep_rew_mean        | 233      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18700    |\n|    time_elapsed       | 8386     |\n|    total_timesteps    | 2805000  |\n| train/                |          |\n|    entropy_loss       | -0.019   |\n|    explained_variance | 0.893    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18699    |\n|    policy_loss        | -0.0216  |\n|    value_loss         | 1.11e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 93.2     |\n|    ep_rew_mean        | 740      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18800    |\n|    time_elapsed       | 8432     |\n|    total_timesteps    | 2820000  |\n| train/                |          |\n|    entropy_loss       | -0.0266  |\n|    explained_variance | 0.294    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18799    |\n|    policy_loss        | -0.325   |\n|    value_loss         | 4.14e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 92.9     |\n|    ep_rew_mean        | 464      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 18900    |\n|    time_elapsed       | 8478     |\n|    total_timesteps    | 2835000  |\n| train/                |          |\n|    entropy_loss       | -0.0073  |\n|    explained_variance | 0.384    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18899    |\n|    policy_loss        | -0.133   |\n|    value_loss         | 770      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 75.6     |\n|    ep_rew_mean        | 407      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 19000    |\n|    time_elapsed       | 8526     |\n|    total_timesteps    | 2850000  |\n| train/                |          |\n|    entropy_loss       | -0.033   |\n|    explained_variance | 0.0559   |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 18999    |\n|    policy_loss        | -0.326   |\n|    value_loss         | 5.43e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 95.8     |\n|    ep_rew_mean        | 682      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 19100    |\n|    time_elapsed       | 8572     |\n|    total_timesteps    | 2865000  |\n| train/                |          |\n|    entropy_loss       | -0.00963 |\n|    explained_variance | 0.804    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19099    |\n|    policy_loss        | 0.101    |\n|    value_loss         | 2.48e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 688      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 19200    |\n|    time_elapsed       | 8619     |\n|    total_timesteps    | 2880000  |\n| train/                |          |\n|    entropy_loss       | -0.0116  |\n|    explained_variance | 0.699    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19199    |\n|    policy_loss        | 0.0101   |\n|    value_loss         | 4.14e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 730      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 19300    |\n|    time_elapsed       | 8664     |\n|    total_timesteps    | 2895000  |\n| train/                |          |\n|    entropy_loss       | -0.011   |\n|    explained_variance | 0.876    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19299    |\n|    policy_loss        | -0.0604  |\n|    value_loss         | 3.43e+03 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 451      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 19400    |\n|    time_elapsed       | 8710     |\n|    total_timesteps    | 2910000  |\n| train/                |          |\n|    entropy_loss       | -0.0193  |\n|    explained_variance | 0.147    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19399    |\n|    policy_loss        | -0.444   |\n|    value_loss         | 3.7e+04  |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 113      |\n|    ep_rew_mean        | 392      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 19500    |\n|    time_elapsed       | 8756     |\n|    total_timesteps    | 2925000  |\n| train/                |          |\n|    entropy_loss       | -0.0161  |\n|    explained_variance | 0.573    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19499    |\n|    policy_loss        | 0.00247  |\n|    value_loss         | 403      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 100      |\n|    ep_rew_mean        | 594      |\n| time/                 |          |\n|    fps                | 334      |\n|    iterations         | 19600    |\n|    time_elapsed       | 8801     |\n|    total_timesteps    | 2940000  |\n| train/                |          |\n|    entropy_loss       | -0.0227  |\n|    explained_variance | 0.882    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19599    |\n|    policy_loss        | 0.081    |\n|    value_loss         | 369      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 96.2     |\n|    ep_rew_mean        | 350      |\n| time/                 |          |\n|    fps                | 333      |\n|    iterations         | 19700    |\n|    time_elapsed       | 8847     |\n|    total_timesteps    | 2955000  |\n| train/                |          |\n|    entropy_loss       | -0.00163 |\n|    explained_variance | 0.731    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19699    |\n|    policy_loss        | -0.00851 |\n|    value_loss         | 423      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 127      |\n|    ep_rew_mean        | 385      |\n| time/                 |          |\n|    fps                | 333      |\n|    iterations         | 19800    |\n|    time_elapsed       | 8892     |\n|    total_timesteps    | 2970000  |\n| train/                |          |\n|    entropy_loss       | -0.0116  |\n|    explained_variance | 0.148    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19799    |\n|    policy_loss        | -0.19    |\n|    value_loss         | 7.99e+04 |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 99.8     |\n|    ep_rew_mean        | 327      |\n| time/                 |          |\n|    fps                | 333      |\n|    iterations         | 19900    |\n|    time_elapsed       | 8938     |\n|    total_timesteps    | 2985000  |\n| train/                |          |\n|    entropy_loss       | -0.00192 |\n|    explained_variance | 0.556    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19899    |\n|    policy_loss        | -0.0211  |\n|    value_loss         | 920      |\n------------------------------------\n------------------------------------\n| rollout/              |          |\n|    ep_len_mean        | 114      |\n|    ep_rew_mean        | 434      |\n| time/                 |          |\n|    fps                | 333      |\n|    iterations         | 20000    |\n|    time_elapsed       | 8984     |\n|    total_timesteps    | 3000000  |\n| train/                |          |\n|    entropy_loss       | -0.0221  |\n|    explained_variance | 0.735    |\n|    learning_rate      | 0.0007   |\n|    n_updates          | 19999    |\n|    policy_loss        | -0.272   |\n|    value_loss         | 5.4e+03  |\n------------------------------------\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test our model with 1000 steps and record all plays."
   ],
   "metadata": {
    "papermill": {
     "duration": 0.036758,
     "end_time": "2023-06-05T16:08:11.70343",
     "exception": false,
     "start_time": "2023-06-05T16:08:11.666672",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Test the trained agent\n",
    "# using the vecenv\n",
    "obs = vec_env.reset()\n",
    "n_steps = 1000\n",
    "\n",
    "replay_folder = './replay'\n",
    "if os.path.exists(replay_folder):\n",
    "    shutil.rmtree(replay_folder)\n",
    "\n",
    "n_env = obs.shape[0] # Number of environments. A2C will play all envs\n",
    "ep_id = np.zeros(n_env, int)\n",
    "ep_steps = np.zeros(n_env, int)\n",
    "cum_reward = np.zeros(n_env)\n",
    "max_reward = -1e10\n",
    "max_game_id = 0\n",
    "max_ep_id = 0\n",
    "\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}\")\n",
    "        print(\"Action: \", action)\n",
    "        print(\"reward=\", reward, \"done=\", done)\n",
    "        \n",
    "    for eID in range(n_env):\n",
    "        cum_reward[eID] += reward[eID]\n",
    "        folder = f'{replay_folder}/{eID}/{ep_id[eID]}'\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        fname = folder + '/' + '{:06d}'.format(ep_steps[eID]) + '.png'\n",
    "        cv2.imwrite(fname, obs[eID])\n",
    "        #cv2.imshow(\"Image\" + str(eID), obs[eID])\n",
    "        #cv2.waitKey(10)\n",
    "        ep_steps[eID] += 1\n",
    "        \n",
    "        if done[eID]:\n",
    "            if cum_reward[eID] > max_reward:\n",
    "                max_reward = cum_reward[eID]\n",
    "                max_game_id = eID\n",
    "                max_ep_id = ep_id[eID]\n",
    "                \n",
    "            ep_id[eID] += 1\n",
    "            cum_reward[eID] = 0\n",
    "            ep_steps[eID] = 0\n",
    "\n",
    "#cv2.destroyAllWindows()"
   ],
   "metadata": {
    "papermill": {
     "duration": 121.450789,
     "end_time": "2023-06-05T16:10:13.191134",
     "exception": false,
     "start_time": "2023-06-05T16:08:11.740345",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T08:00:09.636396Z",
     "iopub.execute_input": "2023-12-27T08:00:09.636771Z",
     "iopub.status.idle": "2023-12-27T08:01:31.323916Z",
     "shell.execute_reply.started": "2023-12-27T08:00:09.636735Z",
     "shell.execute_reply": "2023-12-27T08:01:31.323100Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": "Step 0\nAction:  [2 0 0 2 0 0 2 2 2 2 2 2 2 0 2 2 2 0 2 0 0 2 0 2 0 0 0 2 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 10\nAction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 4 4 0 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0.\n 5. 5. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 20\nAction:  [1 0 2 1 4 1 1 0 2 1 0 0 0 4 0 4 0 0 0 0 0 0 2 1 0 0 1 0 0 4]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.\n   0. -10.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 30\nAction:  [2 0 2 1 4 0 4 1 0 4 0 0 1 0 0 4 4 0 0 1 0 4 0 0 2 1 0 0 0 0]\nreward= [0. 0. 0. 0. 5. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 5. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 40\nAction:  [2 1 0 0 0 0 0 4 0 0 4 4 4 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 5. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 50\nAction:  [0 0 4 0 0 2 1 0 4 0 0 4 0 4 0 1 0 0 4 0 0 4 4 0 0 4 0 0 0 0]\nreward= [  0.   0.   5.   0.   0.   0.   0.   0. -10.   0.   0.   5.   0.   5.\n   0.   0.   0.   0. -10.   0.   0.   0.  -5.   0.   0.   5.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 60\nAction:  [0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 4 2 0 4 0 0 0 0 0 0 2 0 0 1]\nreward= [  0.   0.   0.   0. -10.   0.   0.  -5.   0.   0.   0.   0.   0.   0.\n   0.   0.  -5.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 70\nAction:  [0 0 0 0 0 0 4 0 0 4 0 0 2 4 4 4 0 4 0 0 0 4 0 0 0 0 4 4 0 0]\nreward= [ 0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  5.  5.  0.  5.\n  0.  0.  0.  0.  0.  0.  0.  0.  5. -5.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 80\nAction:  [0 1 0 0 0 4 0 0 0 1 0 0 4 4 0 0 0 4 0 0 0 0 0 0 0 0 4 0 0 2]\nreward= [  0.   0.   0.   0.   0.   5.   0.   0. -15.   0. -15.   0.   0.   5.\n   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.  -5.   0.\n -10.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False  True False False False]\nStep 90\nAction:  [0 0 0 0 0 0 0 0 0 4 0 0 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nreward= [  0.   0.   0.   0.   0. -10. -10.   0.   0.   5.   0.   0.  -5.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0. -15.  20.   0.   0. -10.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 100\nAction:  [0 1 0 0 0 0 4 0 0 0 0 0 0 0 0 4 1 0 0 0 0 2 0 0 4 0 1 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n  True False False False False False]\nStep 110\nAction:  [0 0 4 0 2 0 0 4 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 4 0 0 0]\nreward= [ 0.  0.  5.  0.  0. 80.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 120\nAction:  [0 4 2 1 0 0 0 0 0 0 0 0 0 4 0 2 4 4 1 0 0 0 0 0 0 0 0 0 1 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 5. 5. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 130\nAction:  [0 0 4 4 0 0 2 0 4 0 0 0 0 2 0 0 0 0 0 0 0 2 4 0 1 0 0 0 0 4]\nreward= [  0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0. -15.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False  True\n False False False False False False]\nStep 140\nAction:  [0 0 0 0 0 0 4 0 0 0 0 0 0 0 2 0 0 2 4 0 0 4 0 0 0 0 4 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0.\n 0. 0. 5. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 150\nAction:  [0 2 0 0 0 4 0 0 4 0 0 0 0 0 4 0 0 1 0 0 0 0 0 0 0 0 4 0 4 4]\nreward= [   0.    0.    0.    0.    0.   -5.    0.    0. 1045.    0.    0.    0.\n    0.    0.    5.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 160\nAction:  [4 0 4 0 0 4 0 0 1 0 0 0 0 0 0 2 0 4 0 0 4 0 0 0 2 0 2 0 4 0]\nreward= [ 5.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.\n  0.  0. -5.  0.  0.  0.  0.  0.  0.  0. -5.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 170\nAction:  [4 1 0 0 0 2 2 0 2 0 0 4 0 0 0 0 0 0 0 1 0 2 0 1 0 0 0 0 0 4]\nreward= [ -5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0. -10.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.  -5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 180\nAction:  [4 4 0 0 0 4 4 0 0 2 0 0 0 0 0 0 0 0 0 0 4 0 0 0 4 0 4 0 0 0]\nreward= [ -5.   5.   0.   0.   0.   5.   5.   0.   0.   0.   0.   0.   0.   0.\n   0. -10.   0.   0.   0.   0.   5.   0.   0.   0.   5.   0.   5.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 190\nAction:  [0 0 0 0 0 1 0 0 1 2 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -10.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5. -10.   0.\n   0.   0.] done= [False False False False  True False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 200\nAction:  [0 0 0 0 0 4 0 0 4 4 0 0 2 2 0 0 0 0 2 4 0 0 0 4 0 0 0 0 0 0]\nreward= [ 80.   0.   0.   0.   0.   0.   0.   0.  -5. -10.   0.  20.   0.   0.\n   0.   0.   0.   0.   0.  -5.   0.   0.   0.   5.   0.   0.  80.   0.\n   0.  80.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 210\nAction:  [0 0 0 0 0 0 2 0 0 0 0 0 2 2 0 0 0 0 4 0 4 0 0 0 4 0 2 0 4 0]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  80.   0.   0.   0.\n   0.  80.   0.   0.  -5.   0. -15.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False  True False]\nStep 220\nAction:  [0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 2 0 0 0 4 0 0 0 0 0 4 0 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.\n  0. -5.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 230\nAction:  [2 0 4 4 4 4 0 0 0 0 0 0 0 4 0 0 0 4 0 0 4 0 0 0 0 0 2 0 4 4]\nreward= [   0.    0.    0.    0.    5. 1005.    0.    0.    0.    0.    0.    0.\n    0.    5.    0.    0.    0.    0.    0.    0.  -10.    0.    0.  -10.\n    0.    0.    0.    0.    0.  -10.] done= [False False False  True False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 240\nAction:  [4 0 0 0 0 0 0 0 1 0 2 0 4 4 0 4 4 0 0 0 4 0 4 0 4 1 0 0 0 0]\nreward= [ -5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.\n   0. -10.   5.   0.   0.   0.   5.   0.   5.   0.  -5.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 250\nAction:  [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 260\nAction:  [4 0 4 0 0 0 0 0 0 0 0 4 0 0 0 0 1 2 0 0 4 0 0 0 0 0 0 2 0 0]\nreward= [5. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 270\nAction:  [4 0 0 0 0 4 0 2 4 4 0 0 0 0 0 0 0 0 0 4 4 0 0 0 0 0 4 0 0 0]\nreward= [  0.   0.   0.   0.   0.  -5.   0.   0.  -5.   5.   0.   0.   0. -15.\n   0.   0.   0.   0. -10. -10.   5. -15. -15.   0.   0.   0.   5.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 280\nAction:  [0 0 2 0 0 0 0 0 1 0 0 0 0 0 4 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 5. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 290\nAction:  [0 0 1 4 4 4 0 0 0 0 0 0 4 0 0 0 0 0 4 4 0 0 0 0 0 0 0 4 0 0]\nreward= [   0.    0.    0.    5.    5.    5.  -15.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.  -10.    5.    0.    0.   -5.    0.\n    0.    0.    0. 1025.    0.    0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 300\nAction:  [2 0 4 0 0 4 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]\nreward= [  0.   0. -10.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 310\nAction:  [4 0 0 0 0 0 2 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 1 0 0 4 0 0]\nreward= [1005.    0.    0.    0.    0.    0.    0.    0.    0.  -15.    0.    0.\n    0.   40.    5.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    5.    0.   -5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False  True False\n False False False False False False]\nStep 320\nAction:  [0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 1 0 0 0 0 0 2 4 2 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 5. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 330\nAction:  [0 4 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 4 0 0 0 0 4 0 0]\nreward= [0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0.\n 0. 0. 0. 5. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 340\nAction:  [0 0 2 0 0 4 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 4 0 0 0 1 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 350\nAction:  [0 4 4 0 0 0 0 1 2 0 0 0 0 1 0 4 0 0 0 4 0 0 4 2 0 4 0 4 0 4]\nreward= [   0.    0.  -10.    0.    0.    0.    0.    0.    0.    0.   20.    0.\n    0.    0.    0.    5.    0.    0.    0.    5.    0.    0. 1005.    0.\n    0.    0.    0.  -10.    0.    5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 360\nAction:  [0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 4 4 4 0 4 2 0]\nreward= [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.  -10.  -10.    0.    0.    0.   -5.\n 1005.   -5.    0.    0.    0.    0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 370\nAction:  [0 4 4 0 0 0 2 0 0 0 0 4 0 4 2 0 0 0 0 0 0 0 0 0 0 4 1 0 4 0]\nreward= [   0.  -10.    0.    0.    0.    0.    0.    0.    0.    0.    0.    5.\n    0.    5.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    0. 1305.    0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 380\nAction:  [2 0 0 0 0 0 0 0 0 1 2 0 0 0 0 2 0 2 0 4 4 0 0 0 0 0 0 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False  True False False False False\n False False False False False False]\nStep 390\nAction:  [2 0 0 0 2 0 0 0 0 4 4 0 0 0 4 0 0 1 4 0 2 0 0 0 0 0 0 0 2 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0. -5.  0.  0.  0.\n -5.  0.  0.  0.  0.  0. 80.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 400\nAction:  [1 0 0 4 0 0 0 0 0 0 0 4 2 2 4 0 0 0 1 4 0 0 0 2 0 2 0 0 0 1]\nreward= [ 0.  0.  0.  5.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.\n  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 410\nAction:  [0 0 0 4 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 4]\nreward= [  0.   0.   0.   5.   0.   0.   0.   0. -10.   0.   0.   0.   0.   0.\n   0.  -5.   0.   0.   0.   0.   0.   0.   0.  -5.   0.   0.   0.   0.\n   0. -10.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 420\nAction:  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 4 2 0 1 1 0 0 0 0 4 0 0 0 4]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0.\n 0. 5. 0. 0. 0. 5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 430\nAction:  [2 4 0 0 4 0 0 0 4 0 4 0 0 0 0 4 0 0 0 4 0 0 0 0 0 0 0 0 0 0]\nreward= [  0.   5.   0.   0.  -5.   0.   0.  -5.   5.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   5.   0.   0.   0.   0. -10.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False  True False False False False False False False False\n False False False False False False]\nStep 440\nAction:  [4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 1 4 0 0 0 0 0 0 1 4 0 0 0]\nreward= [ 5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.\n  0.  0.  0. 20.  0.  0.  0.  0.  5.  0.  0. -5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 450\nAction:  [4 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 4 0 0 0]\nreward= [-5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  5.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 460\nAction:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 4 0 2 0 0 2 0 1]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 470\nAction:  [1 4 0 0 4 0 4 2 4 0 1 0 0 0 0 2 0 4 0 0 0 0 0 4 0 0 0 4 4 0]\nreward= [   0.    5.    0.    0.    5.    0.   -5.    0.    5.    0.    0.    0.\n    0.    0.    0.    0.    0.   -5.    0.    0.    0.    0.    0. 1065.\n    0.    0.    0.    0.   -5.    0.] done= [False False False False False False False False False False False False\n False False False False False  True False False False False False False\n False False False False False False]\nStep 480\nAction:  [0 0 0 0 4 0 0 4 0 0 4 0 0 0 0 0 4 0 0 0 4 0 0 0 1 0 0 0 1 2]\nreward= [ 0.  0.  0.  0. -5.  0.  0. -5.  0.  0. -5.  0.  0.  0.  0.  0. -5.  0.\n  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 490\nAction:  [4 2 0 0 0 0 2 0 0 0 0 4 0 0 4 4 0 0 0 0 0 4 0 0 0 4 0 1 4 1]\nreward= [ 5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  5.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  5.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 500\nAction:  [4 2 0 1 4 4 0 0 0 0 0 1 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nreward= [ -5.   0.   0.   0.   5.   5.   0.   0.   0.   0.   0.   0.   0.   0.\n   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -10.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 510\nAction:  [0 1 0 0 0 2 0 4 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 4 4]\nreward= [  0.   0.   0.   0.   0.   0.   0.   5.   0.   0. -15.   0.   0.   0.\n   0. -15.   0.   0.   5.   0.   0.   0.   0.   0. -10.   0.  -5.   0.\n  -5.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 520\nAction:  [4 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 1 0 0 0 0 4 0 0 0 0]\nreward= [-10.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0. -10.   0.   0.   0.   0. -10.   0.   0.   5.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 530\nAction:  [0 0 0 2 0 1 0 0 0 1 0 4 0 0 4 0 4 0 0 0 0 0 0 0 0 0 0 0 2 2]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 540\nAction:  [0 0 0 0 4 0 0 0 0 4 0 1 0 0 0 0 0 0 0 4 0 0 0 0 0 0 4 0 0 0]\nreward= [  0. -10.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 550\nAction:  [0 0 4 0 0 0 4 0 0 0 0 1 0 4 4 0 4 0 0 4 4 0 0 0 4 2 4 0 0 0]\nreward= [ 0.  0. -5.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.  5.  5.  0.  5.  0.\n  0. -5. -5.  0.  0.  0. -5.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False  True False  True False False False\n False False False False False False]\nStep 560\nAction:  [0 0 4 0 0 0 2 0 0 4 0 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 4 1 4 0]\nreward= [ 0.  0.  5. -5.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  5.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 570\nAction:  [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 4 4 0 4 0 4 4 4 0 2 0 0 2]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  5.  0.  5. -5. -5.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 580\nAction:  [4 0 0 0 0 0 4 0 0 1 0 0 0 0 0 0 0 0 2 1 4 0 2 0 0 0 1 0 0 0]\nreward= [-5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 590\nAction:  [0 0 0 0 0 0 4 1 0 0 0 0 4 0 0 0 0 0 0 1 4 0 0 0 0 0 0 1 0 0]\nreward= [  0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   5.   0.\n -10.   0.   0.   0.   0.   0.   5.   0.   0.   0.   0.   0. -10.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 600\nAction:  [1 0 0 0 0 0 0 4 0 4 4 0 0 0 0 4 0 0 4 0 0 0 0 0 4 0 0 0 0 2]\nreward= [  0.   0.   0.   0.   0.   0.   0.   5.   0.   0. -10.   0.   0.   0.\n  -5.   5.   0.   0.   5.   0.   0.   0.   0.   0.   5.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 610\nAction:  [4 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 0 2 4 0 4 0 4 4 0 0]\nreward= [ -5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0. -10.   0.   0.   0.   0.  -5.   0.   5.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 620\nAction:  [0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 40.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 630\nAction:  [0 4 0 4 0 0 0 0 0 0 0 2 0 2 0 0 4 0 1 4 0 2 0 0 2 0 4 0 0 4]\nreward= [  0.   0.   0.   5.   0.   0.   0.   0.   0. -10.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5. -10.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False  True False False False False False False False\n False False False False False False]\nStep 640\nAction:  [0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 1 0 0 1 2 0 0 4 2 0 0 0]\nreward= [  0.   0.   0.   0.  80.   0.   0.  -5.   0.   0.  80.   5.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. -10.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 650\nAction:  [0 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 4 0 2 0 0 0 0 0 0 0 2 0 0 4]\nreward= [ 0.  0.  5. -5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.] done= [False False False False  True False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 660\nAction:  [0 0 0 0 0 4 0 0 0 0 4 0 0 4 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 670\nAction:  [0 0 0 0 1 0 2 0 0 0 0 0 4 4 1 0 4 0 0 0 0 4 0 0 0 0 0 0 0 0]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.\n   0.   0.   5.   0.   0.   0.   0. -10.   0.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 680\nAction:  [2 0 0 0 0 0 0 0 0 0 4 0 0 0 1 4 0 0 0 2 0 0 0 0 0 0 0 4 0 0]\nreward= [ 0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  5.  0.  0.  0.  0.  5.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 690\nAction:  [0 0 4 0 4 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nreward= [ 0.  0.  5.  0.  0.  0.  5. -5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  0. -5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 700\nAction:  [4 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0]\nreward= [-10.   0.   0.  -5.   0.  80.   0.   0.   0.   0.   0.   5.   0.   0.\n -10.   0.   5.   0.   0. -10.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n  True False False False False False False False False False False False\n False False False False False False]\nStep 710\nAction:  [4 2 0 0 2 0 4 0 4 4 0 4 4 1 0 0 0 0 4 4 0 0 0 4 0 1 0 0 1 0]\nreward= [  5.   0.   0.   0.   0.   0.  -5.   0.   5.  -5. -10.   0.   5.   0.\n   0.   0.   0.   0.   5.   5.   0.   0.   0. -10.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 720\nAction:  [0 0 0 0 0 0 0 4 0 0 0 0 4 2 0 0 0 0 0 0 0 0 0 2 0 0 4 0 2 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0. -5. -5.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 730\nAction:  [2 4 0 0 0 2 0 0 0 0 0 4 0 0 4 0 4 4 2 0 4 0 0 0 0 0 0 0 0 0]\nreward= [  0.   5.   0.   0.   0.   0.   0.   0.   0.   0.  80.   0.   0. -15.\n   5.   0.   5.  -5.   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 740\nAction:  [1 0 0 0 0 4 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 4 0 0 4 4 0 4 4]\nreward= [ 0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  5.  0.  0.  5.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 750\nAction:  [0 0 0 0 2 2 0 4 0 0 0 0 2 0 0 0 0 1 0 0 0 4 4 2 0 0 0 0 0 4]\nreward= [   0.    0.    0.    0.    0.    0.    0.  -10.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    5.    0.\n    0.    0.    0.   -5.    0. 1005.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 760\nAction:  [0 0 2 0 0 0 0 4 4 0 0 2 0 4 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0. -5.  0. 80.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 770\nAction:  [2 0 4 0 4 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 4 2 4 0 0 0 0 0 0 0]\nreward= [  0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0. -15.  -5.   0.   5.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False  True False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 780\nAction:  [0 0 0 0 0 2 0 4 0 1 0 4 0 0 0 0 0 4 0 0 0 2 4 0 4 0 0 2 0 1]\nreward= [  0.   0.   0.   0.   0.   0.   0.   5.   0.   0.   0.   5.  80.   0.\n   0.   0.   0. -10.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 790\nAction:  [0 0 0 0 4 0 4 2 4 0 2 0 0 4 0 0 0 4 0 0 4 2 0 4 0 0 4 0 0 4]\nreward= [-15.   0.   0.   0.  -5.   0.   5.   0.   5.   0.   0.   0.   0.   5.\n   0.   0.   0.   5.  -5.   0.   5.   0.   0.  -5.   0.   0.   5.   0.\n  -5.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 800\nAction:  [0 2 0 0 0 0 0 0 4 0 4 0 0 0 0 0 0 4 0 2 0 1 0 0 0 0 0 0 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 5. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 810\nAction:  [0 0 0 2 0 0 0 1 1 0 0 0 4 1 0 1 4 0 0 0 0 0 0 1 0 0 0 2 0 0]\nreward= [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 820\nAction:  [4 0 0 0 0 0 0 0 0 0 0 4 1 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0]\nreward= [  5.   0.   0.   0.   0.   0.   0.   0.   0. -15.   0.   5.   0.   0.\n   0.   0.   5.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.  -5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 830\nAction:  [0 4 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 4 0]\nreward= [ 0.  0.  0.  0.  0.  0. 20.  0.  0.  0.  0.  0.  5.  0. -5.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 840\nAction:  [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 1 0 0 0 0 4 2]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  5.  0.  0.  0.  0.  0.  0.  0. -5.  0.] done= [False False False False False False False False  True False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 850\nAction:  [0 0 0 0 0 0 0 0 0 0 2 0 0 4 0 0 4 0 0 0 0 0 4 0 0 4 0 0 0 0]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.\n -15.   0.   5.   0.   0.   0.   0.   0.   5.   0.   0.   5.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 860\nAction:  [0 4 0 0 0 0 0 0 4 4 4 4 0 0 2 0 0 0 0 4 4 0 4 4 0 4 0 0 0 4]\nreward= [  0.   5.   0. -15.  -5.   0.   0.   0.   5.   5.   0.   5.   0.   0.\n   0.   0.   0.   0.   0.   5.   5.   0.   0. -10.   0.   0.   0.   0.\n   0.   5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 870\nAction:  [0 4 0 0 0 0 0 1 1 4 0 0 0 0 0 0 0 0 4 0 0 0 4 0 0 0 4 4 0 4]\nreward= [  0. -10.   0.   0.   0.   0.   0.   0.   0.  -5.   0.   0.   0.   0.\n   0.   0.   0.   0.   0. -10.   0.   0.  -5.   0.   0.   0.   5.  -5.\n   0.   5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 880\nAction:  [0 0 0 4 0 0 0 4 2 0 2 4 0 0 0 0 0 0 0 4 0 2 0 0 0 0 2 4 4 0]\nreward= [ -5.   0.  80.  -5.   0.   0.   0.   5.   0.   0.   0.   5.   0.   0.\n   0.   0.   0.   0.   0. -10.   0.   0.   0.   0.   0.   0.   0.  -5.\n   5.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 890\nAction:  [0 0 0 0 0 2 0 4 4 0 0 4 0 4 0 0 1 0 0 0 1 0 0 0 0 0 0 4 0 4]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0. -5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 900\nAction:  [0 0 0 1 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 4 0 0 4 0 0 0 2 0]\nreward= [  40.    0.    0.    0.    0.    0.   -5.    0.    0.    0.    0.    0.\n    0.    5.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n 1025.    0.    0.    0.    0.    0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 910\nAction:  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 4 0 0 0 0 0 1 0 0]\nreward= [ 0. 40.  0.  0.  0.  0.  0.  0.  0.  0. -5.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 920\nAction:  [4 0 0 4 0 0 0 2 0 0 0 2 0 0 0 1 0 0 4 4 0 1 0 0 0 4 0 0 0 0]\nreward= [  5.   0.   0.   5.   0.  -5.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   5.   0.   0.   0.   0.   0.   0.   5.   0.   0.\n   0. -10.] done= [ True False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 930\nAction:  [0 0 0 4 0 0 0 4 0 1 2 0 1 2 0 4 0 0 0 0 0 1 0 0 0 0 0 0 2 0]\nreward= [ 0.  0.  0. -5.  0. 80.  0.  5.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 940\nAction:  [4 2 0 0 0 0 0 0 0 4 0 1 0 0 0 0 0 2 0 0 0 0 0 4 0 0 0 0 0 0]\nreward= [-10.   0.   0.   0.  -5.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5.   0.   0.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 950\nAction:  [4 0 4 4 0 0 0 0 0 4 0 0 4 0 0 4 0 1 0 0 0 0 0 2 2 0 0 0 0 4]\nreward= [  5.   0.  -5.   5.   0.   0.   0.   0.  -5.   5.   0.   0.   0.   0.\n   0. -10.   0.   0.   0.  20.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.  -5.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 960\nAction:  [0 0 0 0 0 0 4 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0]\nreward= [   0.    0.    0.   -5.    0.    0. 1005.    0.    0.    0.    0.    0.\n    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n    0.    0.    0.    5.    0.    0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 970\nAction:  [0 0 0 0 0 2 0 0 0 0 4 0 0 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nreward= [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   5.   0.   0. -10.\n   0.   0.   0.   0.  -5.   0.   0.   0.   0.   0.   0. -15.   0.   0.\n   0.   0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False]\nStep 980\nAction:  [0 0 0 0 4 0 2 0 0 4 4 0 0 4 0 0 0 0 0 0 2 0 0 0 4 0 0 1 0 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  5.  0.  0.  0.  5.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0. -5.  0.  0.  0. -5.  0.] done= [False False False False False False False False False False False False\n False False False False False False False False False False False False\n  True False False False False False]\nStep 990\nAction:  [0 0 0 0 0 1 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 4 0 0]\nreward= [ 0.  0.  0.  0.  0.  0.  0. -5.  0. -5.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0. -5.  0. -5.  0.  0.] done= [False False False False False False False False  True False False False\n False False False False False False False False False False False False\n False False False False False False]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Goal reached!\", \" Max reward=\", max_reward)\n",
    "best_replay_path = f'{replay_folder}/{max_game_id}/{max_ep_id}'"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.073784,
     "end_time": "2023-06-05T16:10:13.321571",
     "exception": false,
     "start_time": "2023-06-05T16:10:13.247787",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T08:01:31.328543Z",
     "iopub.execute_input": "2023-12-27T08:01:31.328906Z",
     "iopub.status.idle": "2023-12-27T08:01:31.334239Z",
     "shell.execute_reply.started": "2023-12-27T08:01:31.328880Z",
     "shell.execute_reply": "2023-12-27T08:01:31.333173Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "Goal reached!  Max reward= 3355.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make a gif image to visualize the best play"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.046509,
     "end_time": "2023-06-05T16:10:13.419713",
     "exception": false,
     "start_time": "2023-06-05T16:10:13.373204",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "import imageio\n",
    "\n",
    "filenames = sorted(glob.glob(best_replay_path + '/*.png'))\n",
    "\n",
    "images = []\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('replay.gif', images, loop=0)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.917406,
     "end_time": "2023-06-05T16:10:14.494372",
     "exception": false,
     "start_time": "2023-06-05T16:10:13.576966",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T08:01:31.335466Z",
     "iopub.execute_input": "2023-12-27T08:01:31.335727Z",
     "iopub.status.idle": "2023-12-27T08:01:31.923442Z",
     "shell.execute_reply.started": "2023-12-27T08:01:31.335704Z",
     "shell.execute_reply": "2023-12-27T08:01:31.922580Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_141/3508690983.py:8: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  images.append(imageio.imread(filename))\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='/kaggle/working/replay.gif')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.065458,
     "end_time": "2023-06-05T16:10:14.608246",
     "exception": false,
     "start_time": "2023-06-05T16:10:14.542788",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T08:01:31.924636Z",
     "iopub.execute_input": "2023-12-27T08:01:31.924924Z",
     "iopub.status.idle": "2023-12-27T08:01:31.932746Z",
     "shell.execute_reply.started": "2023-12-27T08:01:31.924899Z",
     "shell.execute_reply": "2023-12-27T08:01:31.931708Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": [
    {
     "execution_count": 16,
     "output_type": "execute_result",
     "data": {
      "image/gif": "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",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.save('your_studentID_a2c_30env_3M.zip')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.269024,
     "end_time": "2023-06-05T16:10:14.928052",
     "exception": false,
     "start_time": "2023-06-05T16:10:14.659028",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T08:01:31.933805Z",
     "iopub.execute_input": "2023-12-27T08:01:31.934102Z",
     "iopub.status.idle": "2023-12-27T08:01:32.096576Z",
     "shell.execute_reply.started": "2023-12-27T08:01:31.934073Z",
     "shell.execute_reply": "2023-12-27T08:01:32.095138Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('tetris_best_score.csv', 'w') as fs:\n",
    "    fs.write('Id,Predicted\\n')\n",
    "    fs.write(f'game_score,{max_reward}\\n')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.059126,
     "end_time": "2023-06-05T16:10:15.037477",
     "exception": false,
     "start_time": "2023-06-05T16:10:14.978351",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T08:01:32.098580Z",
     "iopub.execute_input": "2023-12-27T08:01:32.099023Z",
     "iopub.status.idle": "2023-12-27T08:01:32.105347Z",
     "shell.execute_reply.started": "2023-12-27T08:01:32.098982Z",
     "shell.execute_reply": "2023-12-27T08:01:32.104104Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Upload your results to Kaggle\n",
    "from IPython.display import FileLink\n",
    "FileLink('tetris_best_score.csv')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.059854,
     "end_time": "2023-06-05T16:10:15.145755",
     "exception": false,
     "start_time": "2023-06-05T16:10:15.085901",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2023-12-27T08:01:32.106620Z",
     "iopub.execute_input": "2023-12-27T08:01:32.106890Z",
     "iopub.status.idle": "2023-12-27T08:01:32.120541Z",
     "shell.execute_reply.started": "2023-12-27T08:01:32.106865Z",
     "shell.execute_reply": "2023-12-27T08:01:32.119602Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "execution_count": 19,
     "output_type": "execute_result",
     "data": {
      "text/plain": "/kaggle/working/tetris_best_score.csv",
      "text/html": "<a href='tetris_best_score.csv' target='_blank'>tetris_best_score.csv</a><br>"
     },
     "metadata": {}
    }
   ]
  }
 ]
}
